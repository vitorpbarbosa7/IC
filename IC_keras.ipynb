{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "###Depois descomentar aqui\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura  e visualização da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_excel('ic.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>Acidez</th>\n",
       "      <th>Aw</th>\n",
       "      <th>Umidade</th>\n",
       "      <th>ST</th>\n",
       "      <th>Brix</th>\n",
       "      <th>Cor - L*</th>\n",
       "      <th>Cor - a*</th>\n",
       "      <th>Cor - b*</th>\n",
       "      <th>sig</th>\n",
       "      <th>w1_915</th>\n",
       "      <th>w1_2450</th>\n",
       "      <th>w2_915</th>\n",
       "      <th>w2_2450</th>\n",
       "      <th>e1_915</th>\n",
       "      <th>e1_2450</th>\n",
       "      <th>e2_915</th>\n",
       "      <th>e2_2450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>0.632628</td>\n",
       "      <td>0.954</td>\n",
       "      <td>90.070932</td>\n",
       "      <td>9.929068</td>\n",
       "      <td>12.78</td>\n",
       "      <td>43.140000</td>\n",
       "      <td>-1.420000</td>\n",
       "      <td>13.830000</td>\n",
       "      <td>7.783333</td>\n",
       "      <td>0.963829</td>\n",
       "      <td>0.937601</td>\n",
       "      <td>2.731786</td>\n",
       "      <td>1.486015</td>\n",
       "      <td>68.45892</td>\n",
       "      <td>66.25260</td>\n",
       "      <td>17.74668</td>\n",
       "      <td>12.17080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "      <td>0.162593</td>\n",
       "      <td>0.984</td>\n",
       "      <td>86.651121</td>\n",
       "      <td>33.093300</td>\n",
       "      <td>6.58</td>\n",
       "      <td>35.506667</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.423333</td>\n",
       "      <td>2.304333</td>\n",
       "      <td>1.006533</td>\n",
       "      <td>1.170938</td>\n",
       "      <td>1.752721</td>\n",
       "      <td>1.217547</td>\n",
       "      <td>68.65147</td>\n",
       "      <td>67.82910</td>\n",
       "      <td>7.67124</td>\n",
       "      <td>7.05225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>30</td>\n",
       "      <td>0.114207</td>\n",
       "      <td>0.971</td>\n",
       "      <td>92.341183</td>\n",
       "      <td>7.658817</td>\n",
       "      <td>30.23</td>\n",
       "      <td>52.560000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>1.375330</td>\n",
       "      <td>0.970853</td>\n",
       "      <td>0.961769</td>\n",
       "      <td>1.293129</td>\n",
       "      <td>1.181774</td>\n",
       "      <td>74.53736</td>\n",
       "      <td>72.97530</td>\n",
       "      <td>6.15682</td>\n",
       "      <td>10.04180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.162593</td>\n",
       "      <td>0.984</td>\n",
       "      <td>86.651121</td>\n",
       "      <td>33.093300</td>\n",
       "      <td>6.58</td>\n",
       "      <td>35.506667</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.423333</td>\n",
       "      <td>2.006667</td>\n",
       "      <td>1.003972</td>\n",
       "      <td>1.128183</td>\n",
       "      <td>1.638379</td>\n",
       "      <td>1.211513</td>\n",
       "      <td>71.22074</td>\n",
       "      <td>70.25665</td>\n",
       "      <td>7.35131</td>\n",
       "      <td>7.94745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.602444</td>\n",
       "      <td>0.948</td>\n",
       "      <td>92.991176</td>\n",
       "      <td>7.008824</td>\n",
       "      <td>29.63</td>\n",
       "      <td>39.690000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>1.514104</td>\n",
       "      <td>0.968711</td>\n",
       "      <td>0.942145</td>\n",
       "      <td>1.223456</td>\n",
       "      <td>1.056823</td>\n",
       "      <td>82.39297</td>\n",
       "      <td>77.82830</td>\n",
       "      <td>8.74140</td>\n",
       "      <td>17.48670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>60</td>\n",
       "      <td>0.640485</td>\n",
       "      <td>0.953</td>\n",
       "      <td>89.983175</td>\n",
       "      <td>10.016825</td>\n",
       "      <td>12.88</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>8.785722</td>\n",
       "      <td>0.970822</td>\n",
       "      <td>0.944616</td>\n",
       "      <td>2.944587</td>\n",
       "      <td>1.562727</td>\n",
       "      <td>65.63748</td>\n",
       "      <td>63.74590</td>\n",
       "      <td>19.41012</td>\n",
       "      <td>11.75590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>40</td>\n",
       "      <td>0.108326</td>\n",
       "      <td>0.975</td>\n",
       "      <td>92.304239</td>\n",
       "      <td>7.695761</td>\n",
       "      <td>30.31</td>\n",
       "      <td>52.560000</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>2.491002</td>\n",
       "      <td>0.978240</td>\n",
       "      <td>0.969785</td>\n",
       "      <td>1.367705</td>\n",
       "      <td>1.179421</td>\n",
       "      <td>71.93080</td>\n",
       "      <td>70.79350</td>\n",
       "      <td>5.87906</td>\n",
       "      <td>8.32610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>70</td>\n",
       "      <td>0.084709</td>\n",
       "      <td>0.956</td>\n",
       "      <td>88.790259</td>\n",
       "      <td>19.422500</td>\n",
       "      <td>11.56</td>\n",
       "      <td>32.710000</td>\n",
       "      <td>-0.752783</td>\n",
       "      <td>-0.377145</td>\n",
       "      <td>10.298214</td>\n",
       "      <td>1.000613</td>\n",
       "      <td>1.001767</td>\n",
       "      <td>3.665153</td>\n",
       "      <td>1.851654</td>\n",
       "      <td>65.08532</td>\n",
       "      <td>63.23270</td>\n",
       "      <td>26.62322</td>\n",
       "      <td>14.04640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.557757</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.347034</td>\n",
       "      <td>8.652966</td>\n",
       "      <td>8.68</td>\n",
       "      <td>32.599107</td>\n",
       "      <td>-0.601188</td>\n",
       "      <td>5.858409</td>\n",
       "      <td>0.525285</td>\n",
       "      <td>0.967562</td>\n",
       "      <td>0.935186</td>\n",
       "      <td>1.306365</td>\n",
       "      <td>1.096791</td>\n",
       "      <td>82.67530</td>\n",
       "      <td>77.47620</td>\n",
       "      <td>9.14469</td>\n",
       "      <td>18.52640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>0.062703</td>\n",
       "      <td>0.957</td>\n",
       "      <td>91.567047</td>\n",
       "      <td>27.606100</td>\n",
       "      <td>7.88</td>\n",
       "      <td>65.930000</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>0.555275</td>\n",
       "      <td>0.964130</td>\n",
       "      <td>0.943401</td>\n",
       "      <td>1.250194</td>\n",
       "      <td>1.075298</td>\n",
       "      <td>80.68455</td>\n",
       "      <td>76.56200</td>\n",
       "      <td>7.90547</td>\n",
       "      <td>16.03940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>40</td>\n",
       "      <td>0.632628</td>\n",
       "      <td>0.954</td>\n",
       "      <td>90.070932</td>\n",
       "      <td>9.929068</td>\n",
       "      <td>12.78</td>\n",
       "      <td>43.140000</td>\n",
       "      <td>-1.420000</td>\n",
       "      <td>13.830000</td>\n",
       "      <td>6.697333</td>\n",
       "      <td>0.957689</td>\n",
       "      <td>0.929799</td>\n",
       "      <td>2.454128</td>\n",
       "      <td>1.396377</td>\n",
       "      <td>70.64875</td>\n",
       "      <td>68.11160</td>\n",
       "      <td>16.48436</td>\n",
       "      <td>12.92465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>0.080595</td>\n",
       "      <td>0.993</td>\n",
       "      <td>88.777440</td>\n",
       "      <td>28.001800</td>\n",
       "      <td>11.76</td>\n",
       "      <td>32.700000</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.423333</td>\n",
       "      <td>6.710000</td>\n",
       "      <td>0.976758</td>\n",
       "      <td>0.982867</td>\n",
       "      <td>2.741720</td>\n",
       "      <td>1.517627</td>\n",
       "      <td>72.15582</td>\n",
       "      <td>69.46900</td>\n",
       "      <td>20.09000</td>\n",
       "      <td>14.86100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0.538279</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.411839</td>\n",
       "      <td>8.588161</td>\n",
       "      <td>8.58</td>\n",
       "      <td>32.593333</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>5.803333</td>\n",
       "      <td>0.967900</td>\n",
       "      <td>0.970409</td>\n",
       "      <td>1.019321</td>\n",
       "      <td>1.349984</td>\n",
       "      <td>1.149563</td>\n",
       "      <td>75.02658</td>\n",
       "      <td>73.18100</td>\n",
       "      <td>6.27182</td>\n",
       "      <td>10.50700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.538279</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.411839</td>\n",
       "      <td>8.588161</td>\n",
       "      <td>8.58</td>\n",
       "      <td>32.593333</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>5.803333</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.967562</td>\n",
       "      <td>0.950677</td>\n",
       "      <td>1.310825</td>\n",
       "      <td>1.112580</td>\n",
       "      <td>81.02892</td>\n",
       "      <td>76.81590</td>\n",
       "      <td>8.31819</td>\n",
       "      <td>16.51545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>0.538279</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.411839</td>\n",
       "      <td>8.588161</td>\n",
       "      <td>8.58</td>\n",
       "      <td>32.593333</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>5.803333</td>\n",
       "      <td>1.757000</td>\n",
       "      <td>0.984473</td>\n",
       "      <td>1.181408</td>\n",
       "      <td>1.475135</td>\n",
       "      <td>1.185201</td>\n",
       "      <td>64.95208</td>\n",
       "      <td>64.15290</td>\n",
       "      <td>5.78612</td>\n",
       "      <td>5.81640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>90</td>\n",
       "      <td>0.602444</td>\n",
       "      <td>0.948</td>\n",
       "      <td>92.991176</td>\n",
       "      <td>7.008824</td>\n",
       "      <td>29.63</td>\n",
       "      <td>39.690000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>5.138429</td>\n",
       "      <td>0.993653</td>\n",
       "      <td>0.986065</td>\n",
       "      <td>1.480824</td>\n",
       "      <td>1.909772</td>\n",
       "      <td>61.09586</td>\n",
       "      <td>60.50790</td>\n",
       "      <td>6.70216</td>\n",
       "      <td>4.99840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>40</td>\n",
       "      <td>0.162593</td>\n",
       "      <td>0.984</td>\n",
       "      <td>86.651121</td>\n",
       "      <td>33.093300</td>\n",
       "      <td>6.58</td>\n",
       "      <td>35.506667</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.423333</td>\n",
       "      <td>1.712000</td>\n",
       "      <td>0.997615</td>\n",
       "      <td>1.087022</td>\n",
       "      <td>1.509441</td>\n",
       "      <td>1.165283</td>\n",
       "      <td>73.54471</td>\n",
       "      <td>72.30225</td>\n",
       "      <td>7.24015</td>\n",
       "      <td>9.02770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>20</td>\n",
       "      <td>1.556349</td>\n",
       "      <td>0.955</td>\n",
       "      <td>89.885415</td>\n",
       "      <td>10.114585</td>\n",
       "      <td>26.38</td>\n",
       "      <td>33.130000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-1.590000</td>\n",
       "      <td>4.082915</td>\n",
       "      <td>0.952938</td>\n",
       "      <td>0.953762</td>\n",
       "      <td>1.436710</td>\n",
       "      <td>0.722052</td>\n",
       "      <td>76.31194</td>\n",
       "      <td>73.47030</td>\n",
       "      <td>9.01610</td>\n",
       "      <td>13.60830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>40</td>\n",
       "      <td>0.164489</td>\n",
       "      <td>0.986</td>\n",
       "      <td>86.687819</td>\n",
       "      <td>32.041800</td>\n",
       "      <td>6.78</td>\n",
       "      <td>35.553925</td>\n",
       "      <td>-0.752783</td>\n",
       "      <td>-0.377145</td>\n",
       "      <td>1.713000</td>\n",
       "      <td>0.997615</td>\n",
       "      <td>1.087022</td>\n",
       "      <td>1.509441</td>\n",
       "      <td>1.165283</td>\n",
       "      <td>73.43670</td>\n",
       "      <td>72.20960</td>\n",
       "      <td>7.24963</td>\n",
       "      <td>8.94750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>90</td>\n",
       "      <td>0.557757</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.347034</td>\n",
       "      <td>8.652966</td>\n",
       "      <td>8.68</td>\n",
       "      <td>32.599107</td>\n",
       "      <td>-0.601188</td>\n",
       "      <td>5.858409</td>\n",
       "      <td>2.167000</td>\n",
       "      <td>0.990577</td>\n",
       "      <td>1.265180</td>\n",
       "      <td>1.430307</td>\n",
       "      <td>1.252586</td>\n",
       "      <td>60.59102</td>\n",
       "      <td>59.78150</td>\n",
       "      <td>6.12536</td>\n",
       "      <td>4.88620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T    Acidez     Aw    Umidade         ST   Brix   Cor - L*   Cor - a*  \\\n",
       "90   50  0.632628  0.954  90.070932   9.929068  12.78  43.140000  -1.420000   \n",
       "52   60  0.162593  0.984  86.651121  33.093300   6.58  35.506667  -0.783333   \n",
       "146  30  0.114207  0.971  92.341183   7.658817  30.23  52.560000  18.750000   \n",
       "50   50  0.162593  0.984  86.651121  33.093300   6.58  35.506667  -0.783333   \n",
       "120   5  0.602444  0.948  92.991176   7.008824  29.63  39.690000  22.800000   \n",
       "93   60  0.640485  0.953  89.983175  10.016825  12.88  42.600000  -1.520000   \n",
       "149  40  0.108326  0.975  92.304239   7.695761  30.31  52.560000  18.770000   \n",
       "35   70  0.084709  0.956  88.790259  19.422500  11.56  32.710000  -0.752783   \n",
       "1     5  0.557757  0.959  91.347034   8.652966   8.68  32.599107  -0.601188   \n",
       "63   10  0.062703  0.957  91.567047  27.606100   7.88  65.930000   3.730000   \n",
       "88   40  0.632628  0.954  90.070932   9.929068  12.78  43.140000  -1.420000   \n",
       "28   40  0.080595  0.993  88.777440  28.001800  11.76  32.700000  -0.783333   \n",
       "6    30  0.538279  0.959  91.411839   8.588161   8.58  32.593333  -0.633333   \n",
       "2    10  0.538279  0.959  91.411839   8.588161   8.58  32.593333  -0.633333   \n",
       "14   70  0.538279  0.959  91.411839   8.588161   8.58  32.593333  -0.633333   \n",
       "138  90  0.602444  0.948  92.991176   7.008824  29.63  39.690000  22.800000   \n",
       "48   40  0.162593  0.984  86.651121  33.093300   6.58  35.506667  -0.783333   \n",
       "105  20  1.556349  0.955  89.885415  10.114585  26.38  33.130000  -0.250000   \n",
       "49   40  0.164489  0.986  86.687819  32.041800   6.78  35.553925  -0.752783   \n",
       "19   90  0.557757  0.959  91.347034   8.652966   8.68  32.599107  -0.601188   \n",
       "\n",
       "      Cor - b*        sig    w1_915   w1_2450    w2_915   w2_2450    e1_915  \\\n",
       "90   13.830000   7.783333  0.963829  0.937601  2.731786  1.486015  68.45892   \n",
       "52   -0.423333   2.304333  1.006533  1.170938  1.752721  1.217547  68.65147   \n",
       "146  15.310000   1.375330  0.970853  0.961769  1.293129  1.181774  74.53736   \n",
       "50   -0.423333   2.006667  1.003972  1.128183  1.638379  1.211513  71.22074   \n",
       "120  16.900000   1.514104  0.968711  0.942145  1.223456  1.056823  82.39297   \n",
       "93   13.030000   8.785722  0.970822  0.944616  2.944587  1.562727  65.63748   \n",
       "149  15.320000   2.491002  0.978240  0.969785  1.367705  1.179421  71.93080   \n",
       "35   -0.377145  10.298214  1.000613  1.001767  3.665153  1.851654  65.08532   \n",
       "1     5.858409   0.525285  0.967562  0.935186  1.306365  1.096791  82.67530   \n",
       "63   32.590000   0.555275  0.964130  0.943401  1.250194  1.075298  80.68455   \n",
       "88   13.830000   6.697333  0.957689  0.929799  2.454128  1.396377  70.64875   \n",
       "28   -0.423333   6.710000  0.976758  0.982867  2.741720  1.517627  72.15582   \n",
       "6     5.803333   0.967900  0.970409  1.019321  1.349984  1.149563  75.02658   \n",
       "2     5.803333   0.607600  0.967562  0.950677  1.310825  1.112580  81.02892   \n",
       "14    5.803333   1.757000  0.984473  1.181408  1.475135  1.185201  64.95208   \n",
       "138  16.900000   5.138429  0.993653  0.986065  1.480824  1.909772  61.09586   \n",
       "48   -0.423333   1.712000  0.997615  1.087022  1.509441  1.165283  73.54471   \n",
       "105  -1.590000   4.082915  0.952938  0.953762  1.436710  0.722052  76.31194   \n",
       "49   -0.377145   1.713000  0.997615  1.087022  1.509441  1.165283  73.43670   \n",
       "19    5.858409   2.167000  0.990577  1.265180  1.430307  1.252586  60.59102   \n",
       "\n",
       "      e1_2450    e2_915   e2_2450  \n",
       "90   66.25260  17.74668  12.17080  \n",
       "52   67.82910   7.67124   7.05225  \n",
       "146  72.97530   6.15682  10.04180  \n",
       "50   70.25665   7.35131   7.94745  \n",
       "120  77.82830   8.74140  17.48670  \n",
       "93   63.74590  19.41012  11.75590  \n",
       "149  70.79350   5.87906   8.32610  \n",
       "35   63.23270  26.62322  14.04640  \n",
       "1    77.47620   9.14469  18.52640  \n",
       "63   76.56200   7.90547  16.03940  \n",
       "88   68.11160  16.48436  12.92465  \n",
       "28   69.46900  20.09000  14.86100  \n",
       "6    73.18100   6.27182  10.50700  \n",
       "2    76.81590   8.31819  16.51545  \n",
       "14   64.15290   5.78612   5.81640  \n",
       "138  60.50790   6.70216   4.99840  \n",
       "48   72.30225   7.24015   9.02770  \n",
       "105  73.47030   9.01610  13.60830  \n",
       "49   72.20960   7.24963   8.94750  \n",
       "19   59.78150   6.12536   4.88620  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação entre variáveis dependentes e independentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = base.iloc[:, 0:10].values\n",
    "\n",
    "#Vamos prever e1_945\n",
    "y = base['e1_915'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(y, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_x = StandardScaler()\n",
    "X = scaler_x.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação entre dados de treinamento e dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X, y,\n",
    "                                                                  test_size = 0.3,\n",
    "                                                                  random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da arquitetura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim = 10, activation = 'softmax'))\n",
    "# Prever todas saídas\n",
    "model.add(Dense(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da taxa de aprendizado (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate =0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da métrica Root Mean Squared Error (RMSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar o backend do tensorflow (eu não sei exatamente porque precisei fazer isso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo do erro, otimizador e métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics= ['mae','mse',rmse])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo de todos parâmetros e arquitetura do modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6)                 66        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 73\n",
      "Trainable params: 73\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9302 - mae: 0.8287 - mse: 0.9302 - rmse: 0.8287 - val_loss: 0.8874 - val_mae: 0.8155 - val_mse: 0.8874 - val_rmse: 0.8155\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.8551 - mae: 0.7958 - mse: 0.8551 - rmse: 0.7958 - val_loss: 0.8244 - val_mae: 0.7872 - val_mse: 0.8244 - val_rmse: 0.7872\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 0s 292us/step - loss: 0.7958 - mae: 0.7669 - mse: 0.7958 - rmse: 0.7669 - val_loss: 0.7762 - val_mae: 0.7660 - val_mse: 0.7762 - val_rmse: 0.7660\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.7493 - mae: 0.7426 - mse: 0.7493 - rmse: 0.7426 - val_loss: 0.7288 - val_mae: 0.7443 - val_mse: 0.7288 - val_rmse: 0.7443\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 0s 314us/step - loss: 0.7097 - mae: 0.7232 - mse: 0.7097 - rmse: 0.7232 - val_loss: 0.6832 - val_mae: 0.7234 - val_mse: 0.6832 - val_rmse: 0.7234\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 0s 303us/step - loss: 0.6699 - mae: 0.6992 - mse: 0.6699 - rmse: 0.6992 - val_loss: 0.6421 - val_mae: 0.7022 - val_mse: 0.6421 - val_rmse: 0.7022\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 0s 347us/step - loss: 0.6292 - mae: 0.6752 - mse: 0.6292 - rmse: 0.6752 - val_loss: 0.6035 - val_mae: 0.6813 - val_mse: 0.6035 - val_rmse: 0.6813\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 0s 337us/step - loss: 0.5885 - mae: 0.6519 - mse: 0.5885 - rmse: 0.6519 - val_loss: 0.5647 - val_mae: 0.6607 - val_mse: 0.5647 - val_rmse: 0.6607\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 0s 303us/step - loss: 0.5592 - mae: 0.6308 - mse: 0.5592 - rmse: 0.6308 - val_loss: 0.5276 - val_mae: 0.6408 - val_mse: 0.5276 - val_rmse: 0.6408\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 0s 314us/step - loss: 0.5182 - mae: 0.6069 - mse: 0.5182 - rmse: 0.6069 - val_loss: 0.4904 - val_mae: 0.6176 - val_mse: 0.4904 - val_rmse: 0.6176\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 0s 291us/step - loss: 0.4827 - mae: 0.5840 - mse: 0.4827 - rmse: 0.5840 - val_loss: 0.4561 - val_mae: 0.5942 - val_mse: 0.4561 - val_rmse: 0.5942\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 0s 303us/step - loss: 0.4513 - mae: 0.5603 - mse: 0.4513 - rmse: 0.5603 - val_loss: 0.4264 - val_mae: 0.5760 - val_mse: 0.4264 - val_rmse: 0.5760\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 0s 303us/step - loss: 0.4210 - mae: 0.5373 - mse: 0.4210 - rmse: 0.5373 - val_loss: 0.3937 - val_mae: 0.5514 - val_mse: 0.3937 - val_rmse: 0.5514\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 0s 280us/step - loss: 0.3906 - mae: 0.5151 - mse: 0.3906 - rmse: 0.5151 - val_loss: 0.3661 - val_mae: 0.5330 - val_mse: 0.3661 - val_rmse: 0.5330\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 0s 291us/step - loss: 0.3593 - mae: 0.4912 - mse: 0.3593 - rmse: 0.4912 - val_loss: 0.3407 - val_mae: 0.5149 - val_mse: 0.3407 - val_rmse: 0.5149\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 0s 291us/step - loss: 0.3357 - mae: 0.4723 - mse: 0.3357 - rmse: 0.4723 - val_loss: 0.3130 - val_mae: 0.4929 - val_mse: 0.3130 - val_rmse: 0.4929\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.3065 - mae: 0.4518 - mse: 0.3065 - rmse: 0.4518 - val_loss: 0.2852 - val_mae: 0.4669 - val_mse: 0.2852 - val_rmse: 0.4669\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.2819 - mae: 0.4331 - mse: 0.2819 - rmse: 0.4331 - val_loss: 0.2677 - val_mae: 0.4536 - val_mse: 0.2677 - val_rmse: 0.4536\n",
      "Epoch 19/200\n",
      "89/89 [==============================] - 0s 280us/step - loss: 0.2574 - mae: 0.4114 - mse: 0.2574 - rmse: 0.4114 - val_loss: 0.2396 - val_mae: 0.4241 - val_mse: 0.2396 - val_rmse: 0.4241\n",
      "Epoch 20/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.2347 - mae: 0.3941 - mse: 0.2347 - rmse: 0.3941 - val_loss: 0.2189 - val_mae: 0.4019 - val_mse: 0.2189 - val_rmse: 0.4019\n",
      "Epoch 21/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.2154 - mae: 0.3786 - mse: 0.2154 - rmse: 0.3786 - val_loss: 0.2001 - val_mae: 0.3815 - val_mse: 0.2001 - val_rmse: 0.3815\n",
      "Epoch 22/200\n",
      "89/89 [==============================] - 0s 280us/step - loss: 0.1956 - mae: 0.3604 - mse: 0.1956 - rmse: 0.3604 - val_loss: 0.1829 - val_mae: 0.3648 - val_mse: 0.1829 - val_rmse: 0.3648\n",
      "Epoch 23/200\n",
      "89/89 [==============================] - 0s 291us/step - loss: 0.1793 - mae: 0.3442 - mse: 0.1793 - rmse: 0.3442 - val_loss: 0.1613 - val_mae: 0.3371 - val_mse: 0.1613 - val_rmse: 0.3371\n",
      "Epoch 24/200\n",
      "89/89 [==============================] - 0s 280us/step - loss: 0.1590 - mae: 0.3212 - mse: 0.1590 - rmse: 0.3212 - val_loss: 0.1476 - val_mae: 0.3214 - val_mse: 0.1476 - val_rmse: 0.3214\n",
      "Epoch 25/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.1435 - mae: 0.3018 - mse: 0.1435 - rmse: 0.3018 - val_loss: 0.1309 - val_mae: 0.2947 - val_mse: 0.1309 - val_rmse: 0.2947\n",
      "Epoch 26/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.1302 - mae: 0.2875 - mse: 0.1302 - rmse: 0.2875 - val_loss: 0.1195 - val_mae: 0.2815 - val_mse: 0.1195 - val_rmse: 0.2815\n",
      "Epoch 27/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.1186 - mae: 0.2730 - mse: 0.1186 - rmse: 0.2730 - val_loss: 0.1099 - val_mae: 0.2713 - val_mse: 0.1099 - val_rmse: 0.2713\n",
      "Epoch 28/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.1059 - mae: 0.2567 - mse: 0.1059 - rmse: 0.2567 - val_loss: 0.0944 - val_mae: 0.2456 - val_mse: 0.0944 - val_rmse: 0.2456\n",
      "Epoch 29/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0939 - mae: 0.2360 - mse: 0.0939 - rmse: 0.2360 - val_loss: 0.0854 - val_mae: 0.2316 - val_mse: 0.0854 - val_rmse: 0.2316\n",
      "Epoch 30/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0854 - mae: 0.2286 - mse: 0.0854 - rmse: 0.2286 - val_loss: 0.0745 - val_mae: 0.2148 - val_mse: 0.0745 - val_rmse: 0.2148\n",
      "Epoch 31/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0763 - mae: 0.2075 - mse: 0.0763 - rmse: 0.2075 - val_loss: 0.0682 - val_mae: 0.2059 - val_mse: 0.0682 - val_rmse: 0.2059\n",
      "Epoch 32/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0701 - mae: 0.1992 - mse: 0.0701 - rmse: 0.1992 - val_loss: 0.0616 - val_mae: 0.1893 - val_mse: 0.0616 - val_rmse: 0.1893\n",
      "Epoch 33/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0613 - mae: 0.1831 - mse: 0.0613 - rmse: 0.1831 - val_loss: 0.0556 - val_mae: 0.1781 - val_mse: 0.0556 - val_rmse: 0.1781\n",
      "Epoch 34/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0558 - mae: 0.1770 - mse: 0.0558 - rmse: 0.1770 - val_loss: 0.0501 - val_mae: 0.1656 - val_mse: 0.0501 - val_rmse: 0.1656\n",
      "Epoch 35/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0511 - mae: 0.1653 - mse: 0.0511 - rmse: 0.1653 - val_loss: 0.0486 - val_mae: 0.1616 - val_mse: 0.0486 - val_rmse: 0.1616\n",
      "Epoch 36/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0466 - mae: 0.1651 - mse: 0.0466 - rmse: 0.1651 - val_loss: 0.0424 - val_mae: 0.1556 - val_mse: 0.0424 - val_rmse: 0.1556\n",
      "Epoch 37/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0450 - mae: 0.1595 - mse: 0.0450 - rmse: 0.1595 - val_loss: 0.0409 - val_mae: 0.1486 - val_mse: 0.0409 - val_rmse: 0.1486\n",
      "Epoch 38/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0410 - mae: 0.1571 - mse: 0.0410 - rmse: 0.1571 - val_loss: 0.0377 - val_mae: 0.1472 - val_mse: 0.0377 - val_rmse: 0.1472\n",
      "Epoch 39/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0387 - mae: 0.1531 - mse: 0.0387 - rmse: 0.1531 - val_loss: 0.0378 - val_mae: 0.1472 - val_mse: 0.0378 - val_rmse: 0.1472\n",
      "Epoch 40/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0366 - mae: 0.1504 - mse: 0.0366 - rmse: 0.1504 - val_loss: 0.0364 - val_mae: 0.1474 - val_mse: 0.0364 - val_rmse: 0.1474\n",
      "Epoch 41/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0356 - mae: 0.1491 - mse: 0.0356 - rmse: 0.1491 - val_loss: 0.0387 - val_mae: 0.1509 - val_mse: 0.0387 - val_rmse: 0.1509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0329 - mae: 0.1443 - mse: 0.0329 - rmse: 0.1443 - val_loss: 0.0376 - val_mae: 0.1488 - val_mse: 0.0376 - val_rmse: 0.1488\n",
      "Epoch 43/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0322 - mae: 0.1418 - mse: 0.0322 - rmse: 0.1418 - val_loss: 0.0378 - val_mae: 0.1495 - val_mse: 0.0378 - val_rmse: 0.1495\n",
      "Epoch 44/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0300 - mae: 0.1405 - mse: 0.0300 - rmse: 0.1405 - val_loss: 0.0344 - val_mae: 0.1413 - val_mse: 0.0344 - val_rmse: 0.1413\n",
      "Epoch 45/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0299 - mae: 0.1383 - mse: 0.0299 - rmse: 0.1383 - val_loss: 0.0406 - val_mae: 0.1579 - val_mse: 0.0406 - val_rmse: 0.1579\n",
      "Epoch 46/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0284 - mae: 0.1324 - mse: 0.0284 - rmse: 0.1324 - val_loss: 0.0359 - val_mae: 0.1463 - val_mse: 0.0359 - val_rmse: 0.1463\n",
      "Epoch 47/200\n",
      "89/89 [==============================] - 0s 291us/step - loss: 0.0275 - mae: 0.1354 - mse: 0.0275 - rmse: 0.1354 - val_loss: 0.0295 - val_mae: 0.1340 - val_mse: 0.0295 - val_rmse: 0.1340\n",
      "Epoch 48/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0272 - mae: 0.1317 - mse: 0.0272 - rmse: 0.1317 - val_loss: 0.0407 - val_mae: 0.1595 - val_mse: 0.0407 - val_rmse: 0.1595\n",
      "Epoch 49/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0267 - mae: 0.1315 - mse: 0.0267 - rmse: 0.1315 - val_loss: 0.0350 - val_mae: 0.1482 - val_mse: 0.0350 - val_rmse: 0.1482\n",
      "Epoch 50/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0258 - mae: 0.1315 - mse: 0.0258 - rmse: 0.1315 - val_loss: 0.0321 - val_mae: 0.1361 - val_mse: 0.0321 - val_rmse: 0.1361\n",
      "Epoch 51/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0248 - mae: 0.1264 - mse: 0.0248 - rmse: 0.1264 - val_loss: 0.0361 - val_mae: 0.1495 - val_mse: 0.0361 - val_rmse: 0.1495\n",
      "Epoch 52/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0280 - mae: 0.1365 - mse: 0.0280 - rmse: 0.1365 - val_loss: 0.0351 - val_mae: 0.1447 - val_mse: 0.0351 - val_rmse: 0.1447\n",
      "Epoch 53/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0238 - mae: 0.1225 - mse: 0.0238 - rmse: 0.1225 - val_loss: 0.0336 - val_mae: 0.1422 - val_mse: 0.0336 - val_rmse: 0.1422\n",
      "Epoch 54/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0243 - mae: 0.1264 - mse: 0.0243 - rmse: 0.1264 - val_loss: 0.0278 - val_mae: 0.1298 - val_mse: 0.0278 - val_rmse: 0.1298\n",
      "Epoch 55/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0234 - mae: 0.1227 - mse: 0.0234 - rmse: 0.1227 - val_loss: 0.0337 - val_mae: 0.1441 - val_mse: 0.0337 - val_rmse: 0.1441\n",
      "Epoch 56/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0226 - mae: 0.1196 - mse: 0.0226 - rmse: 0.1196 - val_loss: 0.0325 - val_mae: 0.1415 - val_mse: 0.0325 - val_rmse: 0.1415\n",
      "Epoch 57/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0220 - mae: 0.1218 - mse: 0.0220 - rmse: 0.1218 - val_loss: 0.0299 - val_mae: 0.1324 - val_mse: 0.0299 - val_rmse: 0.1324\n",
      "Epoch 58/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0249 - mae: 0.1263 - mse: 0.0249 - rmse: 0.1263 - val_loss: 0.0281 - val_mae: 0.1297 - val_mse: 0.0281 - val_rmse: 0.1297\n",
      "Epoch 59/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0211 - mae: 0.1155 - mse: 0.0211 - rmse: 0.1155 - val_loss: 0.0324 - val_mae: 0.1381 - val_mse: 0.0324 - val_rmse: 0.1381\n",
      "Epoch 60/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0218 - mae: 0.1153 - mse: 0.0218 - rmse: 0.1153 - val_loss: 0.0267 - val_mae: 0.1257 - val_mse: 0.0267 - val_rmse: 0.1257\n",
      "Epoch 61/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0207 - mae: 0.1135 - mse: 0.0207 - rmse: 0.1135 - val_loss: 0.0251 - val_mae: 0.1194 - val_mse: 0.0251 - val_rmse: 0.1194\n",
      "Epoch 62/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0201 - mae: 0.1120 - mse: 0.0201 - rmse: 0.1120 - val_loss: 0.0294 - val_mae: 0.1324 - val_mse: 0.0294 - val_rmse: 0.1324\n",
      "Epoch 63/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0222 - mae: 0.1200 - mse: 0.0222 - rmse: 0.1200 - val_loss: 0.0331 - val_mae: 0.1449 - val_mse: 0.0331 - val_rmse: 0.1449\n",
      "Epoch 64/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0206 - mae: 0.1157 - mse: 0.0206 - rmse: 0.1157 - val_loss: 0.0279 - val_mae: 0.1302 - val_mse: 0.0279 - val_rmse: 0.1302\n",
      "Epoch 65/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0191 - mae: 0.1096 - mse: 0.0191 - rmse: 0.1096 - val_loss: 0.0243 - val_mae: 0.1214 - val_mse: 0.0243 - val_rmse: 0.1214\n",
      "Epoch 66/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0214 - mae: 0.1174 - mse: 0.0214 - rmse: 0.1174 - val_loss: 0.0233 - val_mae: 0.1171 - val_mse: 0.0233 - val_rmse: 0.1171\n",
      "Epoch 67/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0216 - mae: 0.1173 - mse: 0.0216 - rmse: 0.1173 - val_loss: 0.0242 - val_mae: 0.1195 - val_mse: 0.0242 - val_rmse: 0.1195\n",
      "Epoch 68/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0206 - mae: 0.1140 - mse: 0.0206 - rmse: 0.1140 - val_loss: 0.0237 - val_mae: 0.1208 - val_mse: 0.0237 - val_rmse: 0.1208\n",
      "Epoch 69/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0196 - mae: 0.1093 - mse: 0.0196 - rmse: 0.1093 - val_loss: 0.0260 - val_mae: 0.1287 - val_mse: 0.0260 - val_rmse: 0.1287\n",
      "Epoch 70/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0176 - mae: 0.1067 - mse: 0.0176 - rmse: 0.1067 - val_loss: 0.0236 - val_mae: 0.1188 - val_mse: 0.0236 - val_rmse: 0.1188\n",
      "Epoch 71/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0188 - mae: 0.1029 - mse: 0.0188 - rmse: 0.1029 - val_loss: 0.0232 - val_mae: 0.1162 - val_mse: 0.0232 - val_rmse: 0.1162\n",
      "Epoch 72/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0178 - mae: 0.1029 - mse: 0.0178 - rmse: 0.1029 - val_loss: 0.0246 - val_mae: 0.1203 - val_mse: 0.0246 - val_rmse: 0.1203\n",
      "Epoch 73/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.0199 - mae: 0.1144 - mse: 0.0199 - rmse: 0.1144 - val_loss: 0.0211 - val_mae: 0.1156 - val_mse: 0.0211 - val_rmse: 0.1156\n",
      "Epoch 74/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0180 - mae: 0.1023 - mse: 0.0180 - rmse: 0.1023 - val_loss: 0.0307 - val_mae: 0.1387 - val_mse: 0.0307 - val_rmse: 0.1387\n",
      "Epoch 75/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0189 - mae: 0.1134 - mse: 0.0189 - rmse: 0.1134 - val_loss: 0.0254 - val_mae: 0.1282 - val_mse: 0.0254 - val_rmse: 0.1282\n",
      "Epoch 76/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0162 - mae: 0.0975 - mse: 0.0162 - rmse: 0.0975 - val_loss: 0.0263 - val_mae: 0.1247 - val_mse: 0.0263 - val_rmse: 0.1247\n",
      "Epoch 77/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0173 - mae: 0.1033 - mse: 0.0173 - rmse: 0.1033 - val_loss: 0.0247 - val_mae: 0.1233 - val_mse: 0.0247 - val_rmse: 0.1233\n",
      "Epoch 78/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0158 - mae: 0.0988 - mse: 0.0158 - rmse: 0.0988 - val_loss: 0.0266 - val_mae: 0.1290 - val_mse: 0.0266 - val_rmse: 0.1290\n",
      "Epoch 79/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0186 - mae: 0.1084 - mse: 0.0186 - rmse: 0.1084 - val_loss: 0.0230 - val_mae: 0.1144 - val_mse: 0.0230 - val_rmse: 0.1144\n",
      "Epoch 80/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0170 - mae: 0.1021 - mse: 0.0170 - rmse: 0.1021 - val_loss: 0.0238 - val_mae: 0.1209 - val_mse: 0.0238 - val_rmse: 0.1209\n",
      "Epoch 81/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0167 - mae: 0.1013 - mse: 0.0167 - rmse: 0.1013 - val_loss: 0.0226 - val_mae: 0.1202 - val_mse: 0.0226 - val_rmse: 0.1202\n",
      "Epoch 82/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0158 - mae: 0.1006 - mse: 0.0158 - rmse: 0.1006 - val_loss: 0.0235 - val_mae: 0.1216 - val_mse: 0.0235 - val_rmse: 0.1216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0172 - mae: 0.0990 - mse: 0.0172 - rmse: 0.0990 - val_loss: 0.0211 - val_mae: 0.1181 - val_mse: 0.0211 - val_rmse: 0.1181\n",
      "Epoch 84/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0156 - mae: 0.0980 - mse: 0.0156 - rmse: 0.0980 - val_loss: 0.0221 - val_mae: 0.1175 - val_mse: 0.0221 - val_rmse: 0.1175\n",
      "Epoch 85/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0154 - mae: 0.0960 - mse: 0.0154 - rmse: 0.0960 - val_loss: 0.0215 - val_mae: 0.1124 - val_mse: 0.0215 - val_rmse: 0.1124\n",
      "Epoch 86/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0156 - mae: 0.0964 - mse: 0.0156 - rmse: 0.0964 - val_loss: 0.0192 - val_mae: 0.1106 - val_mse: 0.0192 - val_rmse: 0.1106\n",
      "Epoch 87/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0156 - mae: 0.0990 - mse: 0.0156 - rmse: 0.0990 - val_loss: 0.0197 - val_mae: 0.1129 - val_mse: 0.0197 - val_rmse: 0.1129\n",
      "Epoch 88/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0149 - mae: 0.0971 - mse: 0.0149 - rmse: 0.0971 - val_loss: 0.0261 - val_mae: 0.1251 - val_mse: 0.0261 - val_rmse: 0.1251\n",
      "Epoch 89/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0177 - mae: 0.1015 - mse: 0.0177 - rmse: 0.1015 - val_loss: 0.0214 - val_mae: 0.1147 - val_mse: 0.0214 - val_rmse: 0.1147\n",
      "Epoch 90/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0144 - mae: 0.0951 - mse: 0.0144 - rmse: 0.0951 - val_loss: 0.0209 - val_mae: 0.1151 - val_mse: 0.0209 - val_rmse: 0.1151\n",
      "Epoch 91/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0148 - mae: 0.0955 - mse: 0.0148 - rmse: 0.0955 - val_loss: 0.0321 - val_mae: 0.1363 - val_mse: 0.0321 - val_rmse: 0.1363\n",
      "Epoch 92/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0155 - mae: 0.0976 - mse: 0.0155 - rmse: 0.0976 - val_loss: 0.0181 - val_mae: 0.1111 - val_mse: 0.0181 - val_rmse: 0.1111\n",
      "Epoch 93/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0164 - mae: 0.0994 - mse: 0.0164 - rmse: 0.0994 - val_loss: 0.0298 - val_mae: 0.1335 - val_mse: 0.0298 - val_rmse: 0.1335\n",
      "Epoch 94/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0147 - mae: 0.0948 - mse: 0.0147 - rmse: 0.0948 - val_loss: 0.0202 - val_mae: 0.1167 - val_mse: 0.0202 - val_rmse: 0.1167\n",
      "Epoch 95/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0140 - mae: 0.0948 - mse: 0.0140 - rmse: 0.0948 - val_loss: 0.0211 - val_mae: 0.1182 - val_mse: 0.0211 - val_rmse: 0.1182\n",
      "Epoch 96/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0152 - mae: 0.0961 - mse: 0.0152 - rmse: 0.0961 - val_loss: 0.0228 - val_mae: 0.1186 - val_mse: 0.0228 - val_rmse: 0.1186\n",
      "Epoch 97/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0140 - mae: 0.0926 - mse: 0.0140 - rmse: 0.0926 - val_loss: 0.0272 - val_mae: 0.1263 - val_mse: 0.0272 - val_rmse: 0.1263\n",
      "Epoch 98/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0159 - mae: 0.0993 - mse: 0.0159 - rmse: 0.0993 - val_loss: 0.0183 - val_mae: 0.1061 - val_mse: 0.0183 - val_rmse: 0.1061\n",
      "Epoch 99/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0131 - mae: 0.0887 - mse: 0.0131 - rmse: 0.0887 - val_loss: 0.0205 - val_mae: 0.1115 - val_mse: 0.0205 - val_rmse: 0.1115\n",
      "Epoch 100/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0128 - mae: 0.0866 - mse: 0.0128 - rmse: 0.0866 - val_loss: 0.0189 - val_mae: 0.1121 - val_mse: 0.0189 - val_rmse: 0.1121\n",
      "Epoch 101/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0147 - mae: 0.0945 - mse: 0.0147 - rmse: 0.0945 - val_loss: 0.0209 - val_mae: 0.1185 - val_mse: 0.0209 - val_rmse: 0.1185\n",
      "Epoch 102/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0134 - mae: 0.0911 - mse: 0.0134 - rmse: 0.0911 - val_loss: 0.0225 - val_mae: 0.1209 - val_mse: 0.0225 - val_rmse: 0.1209\n",
      "Epoch 103/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0158 - mae: 0.0901 - mse: 0.0158 - rmse: 0.0901 - val_loss: 0.0266 - val_mae: 0.1307 - val_mse: 0.0266 - val_rmse: 0.1307\n",
      "Epoch 104/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0128 - mae: 0.0908 - mse: 0.0128 - rmse: 0.0908 - val_loss: 0.0169 - val_mae: 0.1038 - val_mse: 0.0169 - val_rmse: 0.1038\n",
      "Epoch 105/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0126 - mae: 0.0889 - mse: 0.0126 - rmse: 0.0889 - val_loss: 0.0199 - val_mae: 0.1126 - val_mse: 0.0199 - val_rmse: 0.1126\n",
      "Epoch 106/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0166 - mae: 0.0960 - mse: 0.0166 - rmse: 0.0960 - val_loss: 0.0162 - val_mae: 0.1027 - val_mse: 0.0162 - val_rmse: 0.1027\n",
      "Epoch 107/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0116 - mae: 0.0844 - mse: 0.0116 - rmse: 0.0844 - val_loss: 0.0243 - val_mae: 0.1270 - val_mse: 0.0243 - val_rmse: 0.1270\n",
      "Epoch 108/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0122 - mae: 0.0868 - mse: 0.0122 - rmse: 0.0868 - val_loss: 0.0234 - val_mae: 0.1273 - val_mse: 0.0234 - val_rmse: 0.1273\n",
      "Epoch 109/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0124 - mae: 0.0865 - mse: 0.0124 - rmse: 0.0865 - val_loss: 0.0152 - val_mae: 0.1051 - val_mse: 0.0152 - val_rmse: 0.1051\n",
      "Epoch 110/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0117 - mae: 0.0842 - mse: 0.0117 - rmse: 0.0842 - val_loss: 0.0297 - val_mae: 0.1338 - val_mse: 0.0297 - val_rmse: 0.1338\n",
      "Epoch 111/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0161 - mae: 0.0983 - mse: 0.0161 - rmse: 0.0983 - val_loss: 0.0194 - val_mae: 0.1094 - val_mse: 0.0194 - val_rmse: 0.1094\n",
      "Epoch 112/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0110 - mae: 0.0779 - mse: 0.0110 - rmse: 0.0779 - val_loss: 0.0178 - val_mae: 0.1122 - val_mse: 0.0178 - val_rmse: 0.1122\n",
      "Epoch 113/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0127 - mae: 0.0897 - mse: 0.0127 - rmse: 0.0897 - val_loss: 0.0212 - val_mae: 0.1135 - val_mse: 0.0212 - val_rmse: 0.1135\n",
      "Epoch 114/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0116 - mae: 0.0835 - mse: 0.0116 - rmse: 0.0835 - val_loss: 0.0162 - val_mae: 0.1026 - val_mse: 0.0162 - val_rmse: 0.1026\n",
      "Epoch 115/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0150 - mae: 0.0927 - mse: 0.0150 - rmse: 0.0927 - val_loss: 0.0177 - val_mae: 0.1093 - val_mse: 0.0177 - val_rmse: 0.1093\n",
      "Epoch 116/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0130 - mae: 0.0908 - mse: 0.0130 - rmse: 0.0908 - val_loss: 0.0139 - val_mae: 0.1016 - val_mse: 0.0139 - val_rmse: 0.1016\n",
      "Epoch 117/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0128 - mae: 0.0902 - mse: 0.0128 - rmse: 0.0902 - val_loss: 0.0161 - val_mae: 0.1069 - val_mse: 0.0161 - val_rmse: 0.1069\n",
      "Epoch 118/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0130 - mae: 0.0894 - mse: 0.0130 - rmse: 0.0894 - val_loss: 0.0198 - val_mae: 0.1148 - val_mse: 0.0198 - val_rmse: 0.1148\n",
      "Epoch 119/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0128 - mae: 0.0911 - mse: 0.0128 - rmse: 0.0911 - val_loss: 0.0145 - val_mae: 0.0991 - val_mse: 0.0145 - val_rmse: 0.0991\n",
      "Epoch 120/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0150 - mae: 0.0949 - mse: 0.0150 - rmse: 0.0949 - val_loss: 0.0167 - val_mae: 0.1066 - val_mse: 0.0167 - val_rmse: 0.1066\n",
      "Epoch 121/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0127 - mae: 0.0856 - mse: 0.0127 - rmse: 0.0856 - val_loss: 0.0238 - val_mae: 0.1180 - val_mse: 0.0238 - val_rmse: 0.1180\n",
      "Epoch 122/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0133 - mae: 0.0917 - mse: 0.0133 - rmse: 0.0917 - val_loss: 0.0165 - val_mae: 0.0985 - val_mse: 0.0165 - val_rmse: 0.0985\n",
      "Epoch 123/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0117 - mae: 0.0811 - mse: 0.0117 - rmse: 0.0811 - val_loss: 0.0201 - val_mae: 0.1137 - val_mse: 0.0201 - val_rmse: 0.1137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0125 - mae: 0.0874 - mse: 0.0125 - rmse: 0.0874 - val_loss: 0.0204 - val_mae: 0.1098 - val_mse: 0.0204 - val_rmse: 0.1098\n",
      "Epoch 125/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0158 - mae: 0.0999 - mse: 0.0158 - rmse: 0.0999 - val_loss: 0.0167 - val_mae: 0.1006 - val_mse: 0.0167 - val_rmse: 0.1006\n",
      "Epoch 126/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0106 - mae: 0.0785 - mse: 0.0106 - rmse: 0.0785 - val_loss: 0.0230 - val_mae: 0.1199 - val_mse: 0.0230 - val_rmse: 0.1199\n",
      "Epoch 127/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0115 - mae: 0.0836 - mse: 0.0115 - rmse: 0.0836 - val_loss: 0.0166 - val_mae: 0.1059 - val_mse: 0.0166 - val_rmse: 0.1059\n",
      "Epoch 128/200\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.0101 - mae: 0.0748 - mse: 0.0101 - rmse: 0.07 - 0s 224us/step - loss: 0.0111 - mae: 0.0829 - mse: 0.0111 - rmse: 0.0829 - val_loss: 0.0187 - val_mae: 0.1133 - val_mse: 0.0187 - val_rmse: 0.1133\n",
      "Epoch 129/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0125 - mae: 0.0883 - mse: 0.0125 - rmse: 0.0883 - val_loss: 0.0141 - val_mae: 0.0968 - val_mse: 0.0141 - val_rmse: 0.0968\n",
      "Epoch 130/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0116 - mae: 0.0802 - mse: 0.0116 - rmse: 0.0802 - val_loss: 0.0193 - val_mae: 0.1106 - val_mse: 0.0193 - val_rmse: 0.1106\n",
      "Epoch 131/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0101 - mae: 0.0817 - mse: 0.0101 - rmse: 0.0817 - val_loss: 0.0198 - val_mae: 0.1126 - val_mse: 0.0198 - val_rmse: 0.1126\n",
      "Epoch 132/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0110 - mae: 0.0875 - mse: 0.0110 - rmse: 0.0875 - val_loss: 0.0212 - val_mae: 0.1124 - val_mse: 0.0212 - val_rmse: 0.1124\n",
      "Epoch 133/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0131 - mae: 0.0887 - mse: 0.0131 - rmse: 0.0887 - val_loss: 0.0122 - val_mae: 0.0910 - val_mse: 0.0122 - val_rmse: 0.0910\n",
      "Epoch 134/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0122 - mae: 0.0867 - mse: 0.0122 - rmse: 0.0867 - val_loss: 0.0189 - val_mae: 0.1170 - val_mse: 0.0189 - val_rmse: 0.1170\n",
      "Epoch 135/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0159 - mae: 0.1031 - mse: 0.0159 - rmse: 0.1031 - val_loss: 0.0116 - val_mae: 0.0936 - val_mse: 0.0116 - val_rmse: 0.0936\n",
      "Epoch 136/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0132 - mae: 0.0870 - mse: 0.0132 - rmse: 0.0870 - val_loss: 0.0175 - val_mae: 0.1084 - val_mse: 0.0175 - val_rmse: 0.1084\n",
      "Epoch 137/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0114 - mae: 0.0851 - mse: 0.0114 - rmse: 0.0851 - val_loss: 0.0201 - val_mae: 0.1108 - val_mse: 0.0201 - val_rmse: 0.1108\n",
      "Epoch 138/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0118 - mae: 0.0809 - mse: 0.0118 - rmse: 0.0809 - val_loss: 0.0181 - val_mae: 0.1083 - val_mse: 0.0181 - val_rmse: 0.1083\n",
      "Epoch 139/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0112 - mae: 0.0832 - mse: 0.0112 - rmse: 0.0832 - val_loss: 0.0145 - val_mae: 0.0970 - val_mse: 0.0145 - val_rmse: 0.0970\n",
      "Epoch 140/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0147 - mae: 0.0948 - mse: 0.0147 - rmse: 0.0948 - val_loss: 0.0121 - val_mae: 0.0939 - val_mse: 0.0121 - val_rmse: 0.0939\n",
      "Epoch 141/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0107 - mae: 0.0800 - mse: 0.0107 - rmse: 0.0800 - val_loss: 0.0143 - val_mae: 0.0954 - val_mse: 0.0143 - val_rmse: 0.0954\n",
      "Epoch 142/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0099 - mae: 0.0767 - mse: 0.0099 - rmse: 0.0767 - val_loss: 0.0164 - val_mae: 0.1054 - val_mse: 0.0164 - val_rmse: 0.1054\n",
      "Epoch 143/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0117 - mae: 0.0891 - mse: 0.0117 - rmse: 0.0891 - val_loss: 0.0219 - val_mae: 0.1185 - val_mse: 0.0219 - val_rmse: 0.1185\n",
      "Epoch 144/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0114 - mae: 0.0825 - mse: 0.0114 - rmse: 0.0825 - val_loss: 0.0147 - val_mae: 0.0981 - val_mse: 0.0147 - val_rmse: 0.0981\n",
      "Epoch 145/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0110 - mae: 0.0822 - mse: 0.0110 - rmse: 0.0822 - val_loss: 0.0137 - val_mae: 0.0912 - val_mse: 0.0137 - val_rmse: 0.0912\n",
      "Epoch 146/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0121 - mae: 0.0826 - mse: 0.0121 - rmse: 0.0826 - val_loss: 0.0134 - val_mae: 0.0970 - val_mse: 0.0134 - val_rmse: 0.0970\n",
      "Epoch 147/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0115 - mae: 0.0824 - mse: 0.0115 - rmse: 0.0824 - val_loss: 0.0263 - val_mae: 0.1332 - val_mse: 0.0263 - val_rmse: 0.1332\n",
      "Epoch 148/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0134 - mae: 0.0878 - mse: 0.0134 - rmse: 0.0878 - val_loss: 0.0170 - val_mae: 0.1079 - val_mse: 0.0170 - val_rmse: 0.1079\n",
      "Epoch 149/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0105 - mae: 0.0803 - mse: 0.0105 - rmse: 0.0803 - val_loss: 0.0183 - val_mae: 0.1089 - val_mse: 0.0183 - val_rmse: 0.1089\n",
      "Epoch 150/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0097 - mae: 0.0781 - mse: 0.0097 - rmse: 0.0781 - val_loss: 0.0114 - val_mae: 0.0919 - val_mse: 0.0114 - val_rmse: 0.0919\n",
      "Epoch 151/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0100 - mae: 0.0800 - mse: 0.0100 - rmse: 0.0800 - val_loss: 0.0112 - val_mae: 0.0885 - val_mse: 0.0112 - val_rmse: 0.0885\n",
      "Epoch 152/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0102 - mae: 0.0796 - mse: 0.0102 - rmse: 0.0796 - val_loss: 0.0115 - val_mae: 0.0891 - val_mse: 0.0115 - val_rmse: 0.0891\n",
      "Epoch 153/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0182 - mae: 0.1018 - mse: 0.0182 - rmse: 0.1018 - val_loss: 0.0164 - val_mae: 0.1095 - val_mse: 0.0164 - val_rmse: 0.1095\n",
      "Epoch 154/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0110 - mae: 0.0836 - mse: 0.0110 - rmse: 0.0836 - val_loss: 0.0204 - val_mae: 0.1152 - val_mse: 0.0204 - val_rmse: 0.1152\n",
      "Epoch 155/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0106 - mae: 0.0844 - mse: 0.0106 - rmse: 0.0844 - val_loss: 0.0142 - val_mae: 0.0951 - val_mse: 0.0142 - val_rmse: 0.0951\n",
      "Epoch 156/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.0090 - mae: 0.0727 - mse: 0.0090 - rmse: 0.0727 - val_loss: 0.0157 - val_mae: 0.0980 - val_mse: 0.0157 - val_rmse: 0.0980\n",
      "Epoch 157/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.0112 - mae: 0.0818 - mse: 0.0112 - rmse: 0.0818 - val_loss: 0.0137 - val_mae: 0.0946 - val_mse: 0.0137 - val_rmse: 0.0946\n",
      "Epoch 158/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.0101 - mae: 0.0752 - mse: 0.0101 - rmse: 0.0752 - val_loss: 0.0143 - val_mae: 0.0978 - val_mse: 0.0143 - val_rmse: 0.0978\n",
      "Epoch 159/200\n",
      "89/89 [==============================] - 0s 280us/step - loss: 0.0100 - mae: 0.0803 - mse: 0.0100 - rmse: 0.0803 - val_loss: 0.0127 - val_mae: 0.0923 - val_mse: 0.0127 - val_rmse: 0.0923\n",
      "Epoch 160/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0128 - mae: 0.0837 - mse: 0.0128 - rmse: 0.0837 - val_loss: 0.0120 - val_mae: 0.0902 - val_mse: 0.0120 - val_rmse: 0.0902\n",
      "Epoch 161/200\n",
      "89/89 [==============================] - 0s 280us/step - loss: 0.0114 - mae: 0.0877 - mse: 0.0114 - rmse: 0.0877 - val_loss: 0.0144 - val_mae: 0.1011 - val_mse: 0.0144 - val_rmse: 0.1011\n",
      "Epoch 162/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0121 - mae: 0.0905 - mse: 0.0121 - rmse: 0.0905 - val_loss: 0.0179 - val_mae: 0.1105 - val_mse: 0.0179 - val_rmse: 0.1105\n",
      "Epoch 163/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0096 - mae: 0.0794 - mse: 0.0096 - rmse: 0.0794 - val_loss: 0.0118 - val_mae: 0.0894 - val_mse: 0.0118 - val_rmse: 0.0894\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 247us/step - loss: 0.0094 - mae: 0.0743 - mse: 0.0094 - rmse: 0.0743 - val_loss: 0.0167 - val_mae: 0.1059 - val_mse: 0.0167 - val_rmse: 0.1059\n",
      "Epoch 165/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0104 - mae: 0.0800 - mse: 0.0104 - rmse: 0.0800 - val_loss: 0.0128 - val_mae: 0.0946 - val_mse: 0.0128 - val_rmse: 0.0946\n",
      "Epoch 166/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0091 - mae: 0.0764 - mse: 0.0091 - rmse: 0.0764 - val_loss: 0.0158 - val_mae: 0.1009 - val_mse: 0.0158 - val_rmse: 0.1009\n",
      "Epoch 167/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0097 - mae: 0.0831 - mse: 0.0097 - rmse: 0.0831 - val_loss: 0.0120 - val_mae: 0.0945 - val_mse: 0.0120 - val_rmse: 0.0945\n",
      "Epoch 168/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0121 - mae: 0.0848 - mse: 0.0121 - rmse: 0.0848 - val_loss: 0.0108 - val_mae: 0.0868 - val_mse: 0.0108 - val_rmse: 0.0868\n",
      "Epoch 169/200\n",
      "89/89 [==============================] - 0s 291us/step - loss: 0.0110 - mae: 0.0850 - mse: 0.0110 - rmse: 0.0850 - val_loss: 0.0112 - val_mae: 0.0867 - val_mse: 0.0112 - val_rmse: 0.0867\n",
      "Epoch 170/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0081 - mae: 0.0697 - mse: 0.0081 - rmse: 0.0697 - val_loss: 0.0122 - val_mae: 0.0910 - val_mse: 0.0122 - val_rmse: 0.0910\n",
      "Epoch 171/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0137 - mae: 0.0958 - mse: 0.0137 - rmse: 0.0958 - val_loss: 0.0109 - val_mae: 0.0875 - val_mse: 0.0109 - val_rmse: 0.0875\n",
      "Epoch 172/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0114 - mae: 0.0851 - mse: 0.0114 - rmse: 0.0851 - val_loss: 0.0098 - val_mae: 0.0820 - val_mse: 0.0098 - val_rmse: 0.0820\n",
      "Epoch 173/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0087 - mae: 0.0745 - mse: 0.0087 - rmse: 0.0745 - val_loss: 0.0124 - val_mae: 0.0921 - val_mse: 0.0124 - val_rmse: 0.0921\n",
      "Epoch 174/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0109 - mae: 0.0816 - mse: 0.0109 - rmse: 0.0816 - val_loss: 0.0126 - val_mae: 0.0967 - val_mse: 0.0126 - val_rmse: 0.0967\n",
      "Epoch 175/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0099 - mae: 0.0793 - mse: 0.0099 - rmse: 0.0793 - val_loss: 0.0126 - val_mae: 0.0942 - val_mse: 0.0126 - val_rmse: 0.0942\n",
      "Epoch 176/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0098 - mae: 0.0759 - mse: 0.0098 - rmse: 0.0759 - val_loss: 0.0095 - val_mae: 0.0822 - val_mse: 0.0095 - val_rmse: 0.0822\n",
      "Epoch 177/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0093 - mae: 0.0774 - mse: 0.0093 - rmse: 0.0774 - val_loss: 0.0160 - val_mae: 0.1070 - val_mse: 0.0160 - val_rmse: 0.1070\n",
      "Epoch 178/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0097 - mae: 0.0784 - mse: 0.0097 - rmse: 0.0784 - val_loss: 0.0181 - val_mae: 0.1127 - val_mse: 0.0181 - val_rmse: 0.1127\n",
      "Epoch 179/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0104 - mae: 0.0808 - mse: 0.0104 - rmse: 0.0808 - val_loss: 0.0111 - val_mae: 0.0886 - val_mse: 0.0111 - val_rmse: 0.0886\n",
      "Epoch 180/200\n",
      "89/89 [==============================] - 0s 247us/step - loss: 0.0083 - mae: 0.0703 - mse: 0.0083 - rmse: 0.0703 - val_loss: 0.0097 - val_mae: 0.0835 - val_mse: 0.0097 - val_rmse: 0.0835\n",
      "Epoch 181/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0128 - mae: 0.0841 - mse: 0.0128 - rmse: 0.0841 - val_loss: 0.0138 - val_mae: 0.0959 - val_mse: 0.0138 - val_rmse: 0.0959\n",
      "Epoch 182/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0078 - mae: 0.0669 - mse: 0.0078 - rmse: 0.0669 - val_loss: 0.0117 - val_mae: 0.0862 - val_mse: 0.0117 - val_rmse: 0.0862\n",
      "Epoch 183/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0091 - mae: 0.0748 - mse: 0.0091 - rmse: 0.0748 - val_loss: 0.0136 - val_mae: 0.0968 - val_mse: 0.0136 - val_rmse: 0.0968\n",
      "Epoch 184/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0087 - mae: 0.0750 - mse: 0.0087 - rmse: 0.0750 - val_loss: 0.0118 - val_mae: 0.0926 - val_mse: 0.0118 - val_rmse: 0.0926\n",
      "Epoch 185/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0097 - mae: 0.0813 - mse: 0.0097 - rmse: 0.0813 - val_loss: 0.0152 - val_mae: 0.1035 - val_mse: 0.0152 - val_rmse: 0.1035\n",
      "Epoch 186/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0106 - mae: 0.0849 - mse: 0.0106 - rmse: 0.0849 - val_loss: 0.0104 - val_mae: 0.0850 - val_mse: 0.0104 - val_rmse: 0.0850\n",
      "Epoch 187/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0084 - mae: 0.0719 - mse: 0.0084 - rmse: 0.0719 - val_loss: 0.0159 - val_mae: 0.1048 - val_mse: 0.0159 - val_rmse: 0.1048\n",
      "Epoch 188/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0151 - mae: 0.0962 - mse: 0.0151 - rmse: 0.0962 - val_loss: 0.0139 - val_mae: 0.0950 - val_mse: 0.0139 - val_rmse: 0.0950\n",
      "Epoch 189/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0093 - mae: 0.0746 - mse: 0.0093 - rmse: 0.0746 - val_loss: 0.0171 - val_mae: 0.1093 - val_mse: 0.0171 - val_rmse: 0.1093\n",
      "Epoch 190/200\n",
      "89/89 [==============================] - 0s 291us/step - loss: 0.0116 - mae: 0.0871 - mse: 0.0116 - rmse: 0.0871 - val_loss: 0.0124 - val_mae: 0.0918 - val_mse: 0.0124 - val_rmse: 0.0918\n",
      "Epoch 191/200\n",
      "89/89 [==============================] - 0s 258us/step - loss: 0.0091 - mae: 0.0762 - mse: 0.0091 - rmse: 0.0762 - val_loss: 0.0107 - val_mae: 0.0888 - val_mse: 0.0107 - val_rmse: 0.0888\n",
      "Epoch 192/200\n",
      "89/89 [==============================] - 0s 269us/step - loss: 0.0096 - mae: 0.0759 - mse: 0.0096 - rmse: 0.0759 - val_loss: 0.0106 - val_mae: 0.0831 - val_mse: 0.0106 - val_rmse: 0.0831\n",
      "Epoch 193/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0082 - mae: 0.0714 - mse: 0.0082 - rmse: 0.0714 - val_loss: 0.0154 - val_mae: 0.1068 - val_mse: 0.0154 - val_rmse: 0.1068\n",
      "Epoch 194/200\n",
      "89/89 [==============================] - 0s 213us/step - loss: 0.0095 - mae: 0.0750 - mse: 0.0095 - rmse: 0.0750 - val_loss: 0.0106 - val_mae: 0.0917 - val_mse: 0.0106 - val_rmse: 0.0917\n",
      "Epoch 195/200\n",
      "89/89 [==============================] - 0s 235us/step - loss: 0.0079 - mae: 0.0710 - mse: 0.0079 - rmse: 0.0710 - val_loss: 0.0109 - val_mae: 0.0861 - val_mse: 0.0109 - val_rmse: 0.0861\n",
      "Epoch 196/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0116 - mae: 0.0816 - mse: 0.0116 - rmse: 0.0816 - val_loss: 0.0108 - val_mae: 0.0864 - val_mse: 0.0108 - val_rmse: 0.0864\n",
      "Epoch 197/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0084 - mae: 0.0729 - mse: 0.0084 - rmse: 0.0729 - val_loss: 0.0122 - val_mae: 0.0887 - val_mse: 0.0122 - val_rmse: 0.0887\n",
      "Epoch 198/200\n",
      "89/89 [==============================] - 0s 202us/step - loss: 0.0148 - mae: 0.0913 - mse: 0.0148 - rmse: 0.0913 - val_loss: 0.0170 - val_mae: 0.1085 - val_mse: 0.0170 - val_rmse: 0.1085\n",
      "Epoch 199/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0129 - mae: 0.0914 - mse: 0.0129 - rmse: 0.0914 - val_loss: 0.0139 - val_mae: 0.0960 - val_mse: 0.0139 - val_rmse: 0.0960\n",
      "Epoch 200/200\n",
      "89/89 [==============================] - 0s 224us/step - loss: 0.0076 - mae: 0.0704 - mse: 0.0076 - rmse: 0.0704 - val_loss: 0.0091 - val_mae: 0.0802 - val_mse: 0.0091 - val_rmse: 0.0802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24fb7373588>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_treinamento, y_treinamento, \n",
    "          epochs=200,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predictions ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "previsoes = model.predict(X_teste)\n",
    "\n",
    "test_predictions = model.predict(X_teste)\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(scaler_y.inverse_transform(y_teste), scaler_y.inverse_transform(test_predictions))\n",
    "plt.xlabel('True Values ')\n",
    "plt.ylabel('Predictions ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficiente de determinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9848807076775496"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "acuracia_teste = r2_score(scaler_y.inverse_transform(y_teste),\n",
    "                          scaler_y.inverse_transform(test_predictions))\n",
    "\n",
    "previsao_todas = scaler_y.inverse_transform(model.predict(X_teste))\n",
    "y_teste_real = scaler_y.inverse_transform(y_teste)\n",
    "\n",
    "acuracia_teste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
