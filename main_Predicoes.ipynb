{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão da permissividade dielétrica na frequência de 915 MHz a partir das propriedades físico químicas de 8 diferentes sucos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"physical_devices-------------\", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "###Depois descomentar aqui\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura  e visualização da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_excel('ic.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>Acidez</th>\n",
       "      <th>Aw</th>\n",
       "      <th>Umidade</th>\n",
       "      <th>ST</th>\n",
       "      <th>Brix</th>\n",
       "      <th>Cor - L*</th>\n",
       "      <th>Cor - a*</th>\n",
       "      <th>Cor - b*</th>\n",
       "      <th>sig</th>\n",
       "      <th>w1_915</th>\n",
       "      <th>w1_2450</th>\n",
       "      <th>w2_915</th>\n",
       "      <th>w2_2450</th>\n",
       "      <th>e1_915</th>\n",
       "      <th>e1_2450</th>\n",
       "      <th>e2_915</th>\n",
       "      <th>e2_2450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>60</td>\n",
       "      <td>0.602444</td>\n",
       "      <td>0.948</td>\n",
       "      <td>92.991176</td>\n",
       "      <td>7.008824</td>\n",
       "      <td>29.63</td>\n",
       "      <td>39.690000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>3.748461</td>\n",
       "      <td>0.994881</td>\n",
       "      <td>0.986840</td>\n",
       "      <td>1.481341</td>\n",
       "      <td>1.296540</td>\n",
       "      <td>68.01840</td>\n",
       "      <td>67.22015</td>\n",
       "      <td>6.05872</td>\n",
       "      <td>6.66550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>60</td>\n",
       "      <td>0.640485</td>\n",
       "      <td>0.953</td>\n",
       "      <td>89.983175</td>\n",
       "      <td>10.016825</td>\n",
       "      <td>12.88</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>8.785722</td>\n",
       "      <td>0.970822</td>\n",
       "      <td>0.944616</td>\n",
       "      <td>2.944587</td>\n",
       "      <td>1.562727</td>\n",
       "      <td>65.63748</td>\n",
       "      <td>63.74590</td>\n",
       "      <td>19.41012</td>\n",
       "      <td>11.75590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>10</td>\n",
       "      <td>0.632628</td>\n",
       "      <td>0.954</td>\n",
       "      <td>90.070932</td>\n",
       "      <td>9.929068</td>\n",
       "      <td>12.78</td>\n",
       "      <td>43.140000</td>\n",
       "      <td>-1.420000</td>\n",
       "      <td>13.830000</td>\n",
       "      <td>3.909667</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.891836</td>\n",
       "      <td>1.779939</td>\n",
       "      <td>1.158260</td>\n",
       "      <td>77.69662</td>\n",
       "      <td>72.51310</td>\n",
       "      <td>14.71736</td>\n",
       "      <td>18.35840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>0.956</td>\n",
       "      <td>91.539304</td>\n",
       "      <td>32.769800</td>\n",
       "      <td>7.78</td>\n",
       "      <td>65.850000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.956924</td>\n",
       "      <td>0.928028</td>\n",
       "      <td>1.235806</td>\n",
       "      <td>1.054883</td>\n",
       "      <td>81.28427</td>\n",
       "      <td>76.68500</td>\n",
       "      <td>8.46249</td>\n",
       "      <td>17.36690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>0.162593</td>\n",
       "      <td>0.984</td>\n",
       "      <td>86.651121</td>\n",
       "      <td>33.093300</td>\n",
       "      <td>6.58</td>\n",
       "      <td>35.506667</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.423333</td>\n",
       "      <td>1.152333</td>\n",
       "      <td>0.992871</td>\n",
       "      <td>1.011334</td>\n",
       "      <td>1.349007</td>\n",
       "      <td>1.100703</td>\n",
       "      <td>79.36730</td>\n",
       "      <td>76.99470</td>\n",
       "      <td>7.86732</td>\n",
       "      <td>12.93390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>0.080595</td>\n",
       "      <td>0.993</td>\n",
       "      <td>88.777440</td>\n",
       "      <td>28.001800</td>\n",
       "      <td>11.76</td>\n",
       "      <td>32.700000</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.423333</td>\n",
       "      <td>4.430667</td>\n",
       "      <td>0.950584</td>\n",
       "      <td>0.966461</td>\n",
       "      <td>2.149960</td>\n",
       "      <td>1.262102</td>\n",
       "      <td>75.86600</td>\n",
       "      <td>72.13340</td>\n",
       "      <td>17.73146</td>\n",
       "      <td>17.00750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>50</td>\n",
       "      <td>0.640485</td>\n",
       "      <td>0.953</td>\n",
       "      <td>89.983175</td>\n",
       "      <td>10.016825</td>\n",
       "      <td>12.88</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>7.786388</td>\n",
       "      <td>0.963829</td>\n",
       "      <td>0.937601</td>\n",
       "      <td>2.731786</td>\n",
       "      <td>1.486015</td>\n",
       "      <td>67.99030</td>\n",
       "      <td>65.91400</td>\n",
       "      <td>17.99186</td>\n",
       "      <td>12.14730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>80</td>\n",
       "      <td>0.640485</td>\n",
       "      <td>0.953</td>\n",
       "      <td>89.983175</td>\n",
       "      <td>10.016825</td>\n",
       "      <td>12.88</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>10.725722</td>\n",
       "      <td>0.974852</td>\n",
       "      <td>0.952007</td>\n",
       "      <td>3.403815</td>\n",
       "      <td>1.693362</td>\n",
       "      <td>62.16816</td>\n",
       "      <td>60.62020</td>\n",
       "      <td>22.62102</td>\n",
       "      <td>11.70850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>80</td>\n",
       "      <td>1.556349</td>\n",
       "      <td>0.955</td>\n",
       "      <td>89.885415</td>\n",
       "      <td>10.114585</td>\n",
       "      <td>26.38</td>\n",
       "      <td>33.130000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-1.590000</td>\n",
       "      <td>9.152816</td>\n",
       "      <td>0.964807</td>\n",
       "      <td>1.186616</td>\n",
       "      <td>1.850816</td>\n",
       "      <td>2.263748</td>\n",
       "      <td>60.83796</td>\n",
       "      <td>60.10450</td>\n",
       "      <td>9.99038</td>\n",
       "      <td>6.50680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>0.062703</td>\n",
       "      <td>0.957</td>\n",
       "      <td>91.567047</td>\n",
       "      <td>27.606100</td>\n",
       "      <td>7.88</td>\n",
       "      <td>65.930000</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>2.006392</td>\n",
       "      <td>0.983476</td>\n",
       "      <td>1.255498</td>\n",
       "      <td>1.491879</td>\n",
       "      <td>1.182592</td>\n",
       "      <td>59.54088</td>\n",
       "      <td>58.88850</td>\n",
       "      <td>6.00110</td>\n",
       "      <td>4.48030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>80</td>\n",
       "      <td>0.108326</td>\n",
       "      <td>0.975</td>\n",
       "      <td>92.304239</td>\n",
       "      <td>7.695761</td>\n",
       "      <td>30.31</td>\n",
       "      <td>52.560000</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>3.987648</td>\n",
       "      <td>0.996821</td>\n",
       "      <td>0.989718</td>\n",
       "      <td>1.479371</td>\n",
       "      <td>1.377188</td>\n",
       "      <td>62.38296</td>\n",
       "      <td>61.84170</td>\n",
       "      <td>6.39970</td>\n",
       "      <td>5.19170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>0.108326</td>\n",
       "      <td>0.975</td>\n",
       "      <td>92.304239</td>\n",
       "      <td>7.695761</td>\n",
       "      <td>30.31</td>\n",
       "      <td>52.560000</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>1.778788</td>\n",
       "      <td>0.962151</td>\n",
       "      <td>0.951554</td>\n",
       "      <td>1.232743</td>\n",
       "      <td>1.052875</td>\n",
       "      <td>77.12288</td>\n",
       "      <td>74.89130</td>\n",
       "      <td>6.73920</td>\n",
       "      <td>12.19070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>30</td>\n",
       "      <td>0.114207</td>\n",
       "      <td>0.971</td>\n",
       "      <td>92.341183</td>\n",
       "      <td>7.658817</td>\n",
       "      <td>30.23</td>\n",
       "      <td>52.560000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>1.375330</td>\n",
       "      <td>0.970853</td>\n",
       "      <td>0.961769</td>\n",
       "      <td>1.293129</td>\n",
       "      <td>1.181774</td>\n",
       "      <td>74.53736</td>\n",
       "      <td>72.97530</td>\n",
       "      <td>6.15682</td>\n",
       "      <td>10.04180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>0.062703</td>\n",
       "      <td>0.957</td>\n",
       "      <td>91.567047</td>\n",
       "      <td>27.606100</td>\n",
       "      <td>7.88</td>\n",
       "      <td>65.930000</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>1.056911</td>\n",
       "      <td>0.976111</td>\n",
       "      <td>1.049227</td>\n",
       "      <td>1.336363</td>\n",
       "      <td>1.135742</td>\n",
       "      <td>71.79636</td>\n",
       "      <td>70.56290</td>\n",
       "      <td>5.48920</td>\n",
       "      <td>8.23705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>0.538279</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.411839</td>\n",
       "      <td>8.588161</td>\n",
       "      <td>8.58</td>\n",
       "      <td>32.593333</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>5.803333</td>\n",
       "      <td>2.166000</td>\n",
       "      <td>0.990577</td>\n",
       "      <td>1.265180</td>\n",
       "      <td>1.430307</td>\n",
       "      <td>1.252586</td>\n",
       "      <td>60.59102</td>\n",
       "      <td>59.78150</td>\n",
       "      <td>6.12536</td>\n",
       "      <td>4.88620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>0.956</td>\n",
       "      <td>91.539304</td>\n",
       "      <td>32.769800</td>\n",
       "      <td>7.78</td>\n",
       "      <td>65.850000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>1.834000</td>\n",
       "      <td>0.987263</td>\n",
       "      <td>1.214496</td>\n",
       "      <td>1.410336</td>\n",
       "      <td>1.118249</td>\n",
       "      <td>62.65005</td>\n",
       "      <td>62.10010</td>\n",
       "      <td>5.61354</td>\n",
       "      <td>5.00650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>0.956</td>\n",
       "      <td>91.539304</td>\n",
       "      <td>32.769800</td>\n",
       "      <td>7.78</td>\n",
       "      <td>65.850000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>1.253667</td>\n",
       "      <td>0.980635</td>\n",
       "      <td>1.088957</td>\n",
       "      <td>1.374508</td>\n",
       "      <td>1.147763</td>\n",
       "      <td>69.83565</td>\n",
       "      <td>68.85995</td>\n",
       "      <td>5.33509</td>\n",
       "      <td>7.21160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>0.640485</td>\n",
       "      <td>0.953</td>\n",
       "      <td>89.983175</td>\n",
       "      <td>10.016825</td>\n",
       "      <td>12.88</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>9.696000</td>\n",
       "      <td>0.972601</td>\n",
       "      <td>0.948325</td>\n",
       "      <td>3.449075</td>\n",
       "      <td>1.646802</td>\n",
       "      <td>63.34984</td>\n",
       "      <td>61.71600</td>\n",
       "      <td>21.55308</td>\n",
       "      <td>11.68350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>0.164489</td>\n",
       "      <td>0.986</td>\n",
       "      <td>86.687819</td>\n",
       "      <td>32.041800</td>\n",
       "      <td>6.78</td>\n",
       "      <td>35.553925</td>\n",
       "      <td>-0.752783</td>\n",
       "      <td>-0.377145</td>\n",
       "      <td>2.007244</td>\n",
       "      <td>1.003972</td>\n",
       "      <td>1.128183</td>\n",
       "      <td>1.638379</td>\n",
       "      <td>1.211513</td>\n",
       "      <td>70.95329</td>\n",
       "      <td>69.95025</td>\n",
       "      <td>7.40772</td>\n",
       "      <td>7.82880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.557757</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.347034</td>\n",
       "      <td>8.652966</td>\n",
       "      <td>8.68</td>\n",
       "      <td>32.599107</td>\n",
       "      <td>-0.601188</td>\n",
       "      <td>5.858409</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.967562</td>\n",
       "      <td>0.950677</td>\n",
       "      <td>1.310825</td>\n",
       "      <td>1.112580</td>\n",
       "      <td>81.09296</td>\n",
       "      <td>76.85255</td>\n",
       "      <td>8.24799</td>\n",
       "      <td>16.42495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T    Acidez     Aw    Umidade         ST   Brix   Cor - L*   Cor - a*  \\\n",
       "132  60  0.602444  0.948  92.991176   7.008824  29.63  39.690000  22.800000   \n",
       "93   60  0.640485  0.953  89.983175  10.016825  12.88  42.600000  -1.520000   \n",
       "82   10  0.632628  0.954  90.070932   9.929068  12.78  43.140000  -1.420000   \n",
       "60    5  0.056794  0.956  91.539304  32.769800   7.78  65.850000   3.900000   \n",
       "44   20  0.162593  0.984  86.651121  33.093300   6.58  35.506667  -0.783333   \n",
       "24   20  0.080595  0.993  88.777440  28.001800  11.76  32.700000  -0.783333   \n",
       "91   50  0.640485  0.953  89.983175  10.016825  12.88  42.600000  -1.520000   \n",
       "97   80  0.640485  0.953  89.983175  10.016825  12.88  42.600000  -1.520000   \n",
       "117  80  1.556349  0.955  89.885415  10.114585  26.38  33.130000  -0.250000   \n",
       "79   90  0.062703  0.957  91.567047  27.606100   7.88  65.930000   3.730000   \n",
       "157  80  0.108326  0.975  92.304239   7.695761  30.31  52.560000  18.770000   \n",
       "145  20  0.108326  0.975  92.304239   7.695761  30.31  52.560000  18.770000   \n",
       "146  30  0.114207  0.971  92.341183   7.658817  30.23  52.560000  18.750000   \n",
       "69   40  0.062703  0.957  91.567047  27.606100   7.88  65.930000   3.730000   \n",
       "18   90  0.538279  0.959  91.411839   8.588161   8.58  32.593333  -0.633333   \n",
       "76   80  0.056794  0.956  91.539304  32.769800   7.78  65.850000   3.900000   \n",
       "70   50  0.056794  0.956  91.539304  32.769800   7.78  65.850000   3.900000   \n",
       "95   70  0.640485  0.953  89.983175  10.016825  12.88  42.600000  -1.520000   \n",
       "51   50  0.164489  0.986  86.687819  32.041800   6.78  35.553925  -0.752783   \n",
       "3    10  0.557757  0.959  91.347034   8.652966   8.68  32.599107  -0.601188   \n",
       "\n",
       "      Cor - b*        sig    w1_915   w1_2450    w2_915   w2_2450    e1_915  \\\n",
       "132  16.900000   3.748461  0.994881  0.986840  1.481341  1.296540  68.01840   \n",
       "93   13.030000   8.785722  0.970822  0.944616  2.944587  1.562727  65.63748   \n",
       "82   13.830000   3.909667  0.933287  0.891836  1.779939  1.158260  77.69662   \n",
       "60   32.590000   0.484600  0.956924  0.928028  1.235806  1.054883  81.28427   \n",
       "44   -0.423333   1.152333  0.992871  1.011334  1.349007  1.100703  79.36730   \n",
       "24   -0.423333   4.430667  0.950584  0.966461  2.149960  1.262102  75.86600   \n",
       "91   13.030000   7.786388  0.963829  0.937601  2.731786  1.486015  67.99030   \n",
       "97   13.030000  10.725722  0.974852  0.952007  3.403815  1.693362  62.16816   \n",
       "117  -1.590000   9.152816  0.964807  1.186616  1.850816  2.263748  60.83796   \n",
       "79   32.590000   2.006392  0.983476  1.255498  1.491879  1.182592  59.54088   \n",
       "157  15.320000   3.987648  0.996821  0.989718  1.479371  1.377188  62.38296   \n",
       "145  15.320000   1.778788  0.962151  0.951554  1.232743  1.052875  77.12288   \n",
       "146  15.310000   1.375330  0.970853  0.961769  1.293129  1.181774  74.53736   \n",
       "69   32.590000   1.056911  0.976111  1.049227  1.336363  1.135742  71.79636   \n",
       "18    5.803333   2.166000  0.990577  1.265180  1.430307  1.252586  60.59102   \n",
       "76   32.590000   1.834000  0.987263  1.214496  1.410336  1.118249  62.65005   \n",
       "70   32.590000   1.253667  0.980635  1.088957  1.374508  1.147763  69.83565   \n",
       "95   13.030000   9.696000  0.972601  0.948325  3.449075  1.646802  63.34984   \n",
       "51   -0.377145   2.007244  1.003972  1.128183  1.638379  1.211513  70.95329   \n",
       "3     5.858409   0.607700  0.967562  0.950677  1.310825  1.112580  81.09296   \n",
       "\n",
       "      e1_2450    e2_915   e2_2450  \n",
       "132  67.22015   6.05872   6.66550  \n",
       "93   63.74590  19.41012  11.75590  \n",
       "82   72.51310  14.71736  18.35840  \n",
       "60   76.68500   8.46249  17.36690  \n",
       "44   76.99470   7.86732  12.93390  \n",
       "24   72.13340  17.73146  17.00750  \n",
       "91   65.91400  17.99186  12.14730  \n",
       "97   60.62020  22.62102  11.70850  \n",
       "117  60.10450   9.99038   6.50680  \n",
       "79   58.88850   6.00110   4.48030  \n",
       "157  61.84170   6.39970   5.19170  \n",
       "145  74.89130   6.73920  12.19070  \n",
       "146  72.97530   6.15682  10.04180  \n",
       "69   70.56290   5.48920   8.23705  \n",
       "18   59.78150   6.12536   4.88620  \n",
       "76   62.10010   5.61354   5.00650  \n",
       "70   68.85995   5.33509   7.21160  \n",
       "95   61.71600  21.55308  11.68350  \n",
       "51   69.95025   7.40772   7.82880  \n",
       "3    76.85255   8.24799  16.42495  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação entre variáveis dependentes e independentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = base.iloc[:, 0:10].values\n",
    "\n",
    "#Vamos prever e1_945\n",
    "y = base['e1_915'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(y, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_x = StandardScaler()\n",
    "X = scaler_x.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação entre dados de treinamento e dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X, y,\n",
    "                                                                  test_size = 0.3,\n",
    "                                                                  random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da arquitetura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim = 10, activation = 'softmax'))\n",
    "# Prever todas saídas\n",
    "model.add(Dense(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da taxa de aprendizado (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate =0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da métrica Root Mean Squared Error (RMSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar o backend do tensorflow (eu não sei exatamente porque precisei fazer isso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo do erro, otimizador e métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics= ['mae','mse',rmse])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo de todos hiperparâmetros e arquitetura do modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6)                 66        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 73\n",
      "Trainable params: 73\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:IC_keras.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 1.0314 - mae: 0.8823 - mse: 1.0314 - rmse: 0.8823 - val_loss: 0.9923 - val_mae: 0.8551 - val_mse: 0.9923 - val_rmse: 0.8551\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.9225 - mae: 0.8383 - mse: 0.9225 - rmse: 0.8383 - val_loss: 0.9217 - val_mae: 0.8239 - val_mse: 0.9217 - val_rmse: 0.8239\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.8611 - mae: 0.8067 - mse: 0.8611 - rmse: 0.8067 - val_loss: 0.8635 - val_mae: 0.7979 - val_mse: 0.8635 - val_rmse: 0.7979\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.8096 - mae: 0.7809 - mse: 0.8096 - rmse: 0.7809 - val_loss: 0.8111 - val_mae: 0.7727 - val_mse: 0.8111 - val_rmse: 0.7727\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 0s 101us/step - loss: 0.7613 - mae: 0.7555 - mse: 0.7613 - rmse: 0.7555 - val_loss: 0.7601 - val_mae: 0.7489 - val_mse: 0.7601 - val_rmse: 0.7489\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.7133 - mae: 0.7287 - mse: 0.7133 - rmse: 0.7287 - val_loss: 0.7154 - val_mae: 0.7257 - val_mse: 0.7154 - val_rmse: 0.7257\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 0s 93us/step - loss: 0.6746 - mae: 0.7053 - mse: 0.6746 - rmse: 0.7053 - val_loss: 0.6745 - val_mae: 0.7037 - val_mse: 0.6745 - val_rmse: 0.7037\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 0s 87us/step - loss: 0.6357 - mae: 0.6824 - mse: 0.6357 - rmse: 0.6824 - val_loss: 0.6329 - val_mae: 0.6812 - val_mse: 0.6329 - val_rmse: 0.6812\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 0s 102us/step - loss: 0.6019 - mae: 0.6626 - mse: 0.6019 - rmse: 0.6626 - val_loss: 0.5944 - val_mae: 0.6607 - val_mse: 0.5944 - val_rmse: 0.6607\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 0s 106us/step - loss: 0.5639 - mae: 0.6388 - mse: 0.5639 - rmse: 0.6388 - val_loss: 0.5578 - val_mae: 0.6401 - val_mse: 0.5578 - val_rmse: 0.6401\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.5317 - mae: 0.6184 - mse: 0.5317 - rmse: 0.6184 - val_loss: 0.5281 - val_mae: 0.6223 - val_mse: 0.5281 - val_rmse: 0.6223\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 0s 95us/step - loss: 0.5021 - mae: 0.5977 - mse: 0.5021 - rmse: 0.5977 - val_loss: 0.4936 - val_mae: 0.6031 - val_mse: 0.4936 - val_rmse: 0.6031\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.4753 - mae: 0.5801 - mse: 0.4753 - rmse: 0.5801 - val_loss: 0.4619 - val_mae: 0.5846 - val_mse: 0.4619 - val_rmse: 0.5846\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 0s 108us/step - loss: 0.4420 - mae: 0.5581 - mse: 0.4420 - rmse: 0.5581 - val_loss: 0.4348 - val_mae: 0.5665 - val_mse: 0.4348 - val_rmse: 0.5665\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 0s 100us/step - loss: 0.4122 - mae: 0.5377 - mse: 0.4122 - rmse: 0.5377 - val_loss: 0.4087 - val_mae: 0.5492 - val_mse: 0.4087 - val_rmse: 0.5492\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 0s 80us/step - loss: 0.3837 - mae: 0.5179 - mse: 0.3837 - rmse: 0.5179 - val_loss: 0.3822 - val_mae: 0.5314 - val_mse: 0.3822 - val_rmse: 0.5314\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 0s 99us/step - loss: 0.3592 - mae: 0.5020 - mse: 0.3592 - rmse: 0.5020 - val_loss: 0.3599 - val_mae: 0.5131 - val_mse: 0.3599 - val_rmse: 0.5131\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.3309 - mae: 0.4771 - mse: 0.3309 - rmse: 0.4771 - val_loss: 0.3327 - val_mae: 0.4921 - val_mse: 0.3327 - val_rmse: 0.4921\n",
      "Epoch 19/200\n",
      "89/89 [==============================] - 0s 92us/step - loss: 0.3046 - mae: 0.4565 - mse: 0.3046 - rmse: 0.4565 - val_loss: 0.3066 - val_mae: 0.4719 - val_mse: 0.3066 - val_rmse: 0.4719\n",
      "Epoch 20/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.2808 - mae: 0.4365 - mse: 0.2808 - rmse: 0.4365 - val_loss: 0.2835 - val_mae: 0.4534 - val_mse: 0.2835 - val_rmse: 0.4534\n",
      "Epoch 21/200\n",
      "89/89 [==============================] - 0s 93us/step - loss: 0.2572 - mae: 0.4142 - mse: 0.2572 - rmse: 0.4142 - val_loss: 0.2584 - val_mae: 0.4321 - val_mse: 0.2584 - val_rmse: 0.4321\n",
      "Epoch 22/200\n",
      "89/89 [==============================] - 0s 106us/step - loss: 0.2360 - mae: 0.3931 - mse: 0.2360 - rmse: 0.3931 - val_loss: 0.2364 - val_mae: 0.4123 - val_mse: 0.2364 - val_rmse: 0.4123\n",
      "Epoch 23/200\n",
      "89/89 [==============================] - 0s 91us/step - loss: 0.2150 - mae: 0.3745 - mse: 0.2150 - rmse: 0.3745 - val_loss: 0.2141 - val_mae: 0.3889 - val_mse: 0.2141 - val_rmse: 0.3889\n",
      "Epoch 24/200\n",
      "89/89 [==============================] - 0s 117us/step - loss: 0.1942 - mae: 0.3501 - mse: 0.1942 - rmse: 0.3501 - val_loss: 0.1916 - val_mae: 0.3660 - val_mse: 0.1916 - val_rmse: 0.3660\n",
      "Epoch 25/200\n",
      "89/89 [==============================] - 0s 98us/step - loss: 0.1795 - mae: 0.3346 - mse: 0.1795 - rmse: 0.3346 - val_loss: 0.1740 - val_mae: 0.3494 - val_mse: 0.1740 - val_rmse: 0.3494\n",
      "Epoch 26/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.1617 - mae: 0.3130 - mse: 0.1617 - rmse: 0.3130 - val_loss: 0.1546 - val_mae: 0.3279 - val_mse: 0.1546 - val_rmse: 0.3279\n",
      "Epoch 27/200\n",
      "89/89 [==============================] - 0s 104us/step - loss: 0.1446 - mae: 0.2912 - mse: 0.1446 - rmse: 0.2912 - val_loss: 0.1361 - val_mae: 0.3014 - val_mse: 0.1361 - val_rmse: 0.3014\n",
      "Epoch 28/200\n",
      "89/89 [==============================] - 0s 84us/step - loss: 0.1289 - mae: 0.2735 - mse: 0.1289 - rmse: 0.2735 - val_loss: 0.1196 - val_mae: 0.2781 - val_mse: 0.1196 - val_rmse: 0.2781\n",
      "Epoch 29/200\n",
      "89/89 [==============================] - 0s 111us/step - loss: 0.1162 - mae: 0.2592 - mse: 0.1162 - rmse: 0.2592 - val_loss: 0.1064 - val_mae: 0.2586 - val_mse: 0.1064 - val_rmse: 0.2586\n",
      "Epoch 30/200\n",
      "89/89 [==============================] - 0s 83us/step - loss: 0.1033 - mae: 0.2383 - mse: 0.1033 - rmse: 0.2383 - val_loss: 0.0937 - val_mae: 0.2376 - val_mse: 0.0937 - val_rmse: 0.2376\n",
      "Epoch 31/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0936 - mae: 0.2242 - mse: 0.0936 - rmse: 0.2242 - val_loss: 0.0835 - val_mae: 0.2248 - val_mse: 0.0835 - val_rmse: 0.2248\n",
      "Epoch 32/200\n",
      "89/89 [==============================] - 0s 125us/step - loss: 0.0858 - mae: 0.2142 - mse: 0.0858 - rmse: 0.2142 - val_loss: 0.0748 - val_mae: 0.2083 - val_mse: 0.0748 - val_rmse: 0.2083\n",
      "Epoch 33/200\n",
      "89/89 [==============================] - 0s 99us/step - loss: 0.0768 - mae: 0.2015 - mse: 0.0768 - rmse: 0.2015 - val_loss: 0.0671 - val_mae: 0.1904 - val_mse: 0.0671 - val_rmse: 0.1904\n",
      "Epoch 34/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0691 - mae: 0.1881 - mse: 0.0691 - rmse: 0.1881 - val_loss: 0.0606 - val_mae: 0.1853 - val_mse: 0.0606 - val_rmse: 0.1853\n",
      "Epoch 35/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0638 - mae: 0.1827 - mse: 0.0638 - rmse: 0.1827 - val_loss: 0.0543 - val_mae: 0.1756 - val_mse: 0.0543 - val_rmse: 0.1756\n",
      "Epoch 36/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0590 - mae: 0.1754 - mse: 0.0590 - rmse: 0.1754 - val_loss: 0.0516 - val_mae: 0.1707 - val_mse: 0.0516 - val_rmse: 0.1707\n",
      "Epoch 37/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0549 - mae: 0.1729 - mse: 0.0549 - rmse: 0.1729 - val_loss: 0.0477 - val_mae: 0.1652 - val_mse: 0.0477 - val_rmse: 0.1652\n",
      "Epoch 38/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0509 - mae: 0.1652 - mse: 0.0509 - rmse: 0.1652 - val_loss: 0.0461 - val_mae: 0.1699 - val_mse: 0.0461 - val_rmse: 0.1699\n",
      "Epoch 39/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0476 - mae: 0.1645 - mse: 0.0476 - rmse: 0.1645 - val_loss: 0.0425 - val_mae: 0.1614 - val_mse: 0.0425 - val_rmse: 0.1614\n",
      "Epoch 40/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0457 - mae: 0.1595 - mse: 0.0457 - rmse: 0.1595 - val_loss: 0.0410 - val_mae: 0.1584 - val_mse: 0.0410 - val_rmse: 0.1584\n",
      "Epoch 41/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0441 - mae: 0.1620 - mse: 0.0441 - rmse: 0.1620 - val_loss: 0.0403 - val_mae: 0.1579 - val_mse: 0.0403 - val_rmse: 0.1579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0416 - mae: 0.1566 - mse: 0.0416 - rmse: 0.1566 - val_loss: 0.0388 - val_mae: 0.1609 - val_mse: 0.0388 - val_rmse: 0.1609\n",
      "Epoch 43/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0402 - mae: 0.1540 - mse: 0.0402 - rmse: 0.1540 - val_loss: 0.0366 - val_mae: 0.1575 - val_mse: 0.0366 - val_rmse: 0.1575\n",
      "Epoch 44/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0392 - mae: 0.1541 - mse: 0.0392 - rmse: 0.1541 - val_loss: 0.0351 - val_mae: 0.1558 - val_mse: 0.0351 - val_rmse: 0.1558\n",
      "Epoch 45/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0359 - mae: 0.1444 - mse: 0.0359 - rmse: 0.1444 - val_loss: 0.0341 - val_mae: 0.1564 - val_mse: 0.0341 - val_rmse: 0.1564\n",
      "Epoch 46/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0347 - mae: 0.1430 - mse: 0.0347 - rmse: 0.1430 - val_loss: 0.0363 - val_mae: 0.1586 - val_mse: 0.0363 - val_rmse: 0.1586\n",
      "Epoch 47/200\n",
      "89/89 [==============================] - 0s 55us/step - loss: 0.0348 - mae: 0.1442 - mse: 0.0348 - rmse: 0.1442 - val_loss: 0.0339 - val_mae: 0.1538 - val_mse: 0.0339 - val_rmse: 0.1538\n",
      "Epoch 48/200\n",
      "89/89 [==============================] - 0s 83us/step - loss: 0.0329 - mae: 0.1381 - mse: 0.0329 - rmse: 0.1381 - val_loss: 0.0321 - val_mae: 0.1441 - val_mse: 0.0321 - val_rmse: 0.1441\n",
      "Epoch 49/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0321 - mae: 0.1387 - mse: 0.0321 - rmse: 0.1387 - val_loss: 0.0353 - val_mae: 0.1598 - val_mse: 0.0353 - val_rmse: 0.1598\n",
      "Epoch 50/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0303 - mae: 0.1324 - mse: 0.0303 - rmse: 0.1324 - val_loss: 0.0294 - val_mae: 0.1413 - val_mse: 0.0294 - val_rmse: 0.1413\n",
      "Epoch 51/200\n",
      "89/89 [==============================] - 0s 56us/step - loss: 0.0288 - mae: 0.1315 - mse: 0.0288 - rmse: 0.1315 - val_loss: 0.0294 - val_mae: 0.1403 - val_mse: 0.0294 - val_rmse: 0.1403\n",
      "Epoch 52/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0293 - mae: 0.1362 - mse: 0.0293 - rmse: 0.1362 - val_loss: 0.0323 - val_mae: 0.1514 - val_mse: 0.0323 - val_rmse: 0.1514\n",
      "Epoch 53/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0290 - mae: 0.1337 - mse: 0.0290 - rmse: 0.1337 - val_loss: 0.0310 - val_mae: 0.1474 - val_mse: 0.0310 - val_rmse: 0.1474\n",
      "Epoch 54/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0269 - mae: 0.1248 - mse: 0.0269 - rmse: 0.1248 - val_loss: 0.0261 - val_mae: 0.1353 - val_mse: 0.0261 - val_rmse: 0.1353\n",
      "Epoch 55/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0266 - mae: 0.1235 - mse: 0.0266 - rmse: 0.1235 - val_loss: 0.0278 - val_mae: 0.1393 - val_mse: 0.0278 - val_rmse: 0.1393\n",
      "Epoch 56/200\n",
      "89/89 [==============================] - 0s 58us/step - loss: 0.0252 - mae: 0.1232 - mse: 0.0252 - rmse: 0.1232 - val_loss: 0.0262 - val_mae: 0.1359 - val_mse: 0.0262 - val_rmse: 0.1359\n",
      "Epoch 57/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0274 - mae: 0.1287 - mse: 0.0274 - rmse: 0.1287 - val_loss: 0.0315 - val_mae: 0.1476 - val_mse: 0.0315 - val_rmse: 0.1476\n",
      "Epoch 58/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0252 - mae: 0.1229 - mse: 0.0252 - rmse: 0.1229 - val_loss: 0.0236 - val_mae: 0.1253 - val_mse: 0.0236 - val_rmse: 0.1253\n",
      "Epoch 59/200\n",
      "89/89 [==============================] - 0s 93us/step - loss: 0.0223 - mae: 0.1141 - mse: 0.0223 - rmse: 0.1141 - val_loss: 0.0254 - val_mae: 0.1298 - val_mse: 0.0254 - val_rmse: 0.1298\n",
      "Epoch 60/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0228 - mae: 0.1164 - mse: 0.0228 - rmse: 0.1164 - val_loss: 0.0261 - val_mae: 0.1299 - val_mse: 0.0261 - val_rmse: 0.1299\n",
      "Epoch 61/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.0220 - mae: 0.1144 - mse: 0.0220 - rmse: 0.1144 - val_loss: 0.0239 - val_mae: 0.1230 - val_mse: 0.0239 - val_rmse: 0.1230\n",
      "Epoch 62/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0209 - mae: 0.1093 - mse: 0.0209 - rmse: 0.1093 - val_loss: 0.0241 - val_mae: 0.1314 - val_mse: 0.0241 - val_rmse: 0.1314\n",
      "Epoch 63/200\n",
      "89/89 [==============================] - 0s 84us/step - loss: 0.0219 - mae: 0.1178 - mse: 0.0219 - rmse: 0.1178 - val_loss: 0.0246 - val_mae: 0.1280 - val_mse: 0.0246 - val_rmse: 0.1280\n",
      "Epoch 64/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0221 - mae: 0.1191 - mse: 0.0221 - rmse: 0.1191 - val_loss: 0.0233 - val_mae: 0.1204 - val_mse: 0.0233 - val_rmse: 0.1204\n",
      "Epoch 65/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0203 - mae: 0.1120 - mse: 0.0203 - rmse: 0.1120 - val_loss: 0.0235 - val_mae: 0.1267 - val_mse: 0.0235 - val_rmse: 0.1267\n",
      "Epoch 66/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0197 - mae: 0.1102 - mse: 0.0197 - rmse: 0.1102 - val_loss: 0.0219 - val_mae: 0.1220 - val_mse: 0.0219 - val_rmse: 0.1220\n",
      "Epoch 67/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.0190 - mae: 0.1104 - mse: 0.0190 - rmse: 0.1104 - val_loss: 0.0284 - val_mae: 0.1400 - val_mse: 0.0284 - val_rmse: 0.1400\n",
      "Epoch 68/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0191 - mae: 0.1051 - mse: 0.0191 - rmse: 0.1051 - val_loss: 0.0252 - val_mae: 0.1320 - val_mse: 0.0252 - val_rmse: 0.1320\n",
      "Epoch 69/200\n",
      "89/89 [==============================] - 0s 89us/step - loss: 0.0178 - mae: 0.1065 - mse: 0.0178 - rmse: 0.1065 - val_loss: 0.0235 - val_mae: 0.1263 - val_mse: 0.0235 - val_rmse: 0.1263\n",
      "Epoch 70/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0180 - mae: 0.1046 - mse: 0.0180 - rmse: 0.1046 - val_loss: 0.0223 - val_mae: 0.1208 - val_mse: 0.0223 - val_rmse: 0.1208\n",
      "Epoch 71/200\n",
      "89/89 [==============================] - 0s 117us/step - loss: 0.0175 - mae: 0.1005 - mse: 0.0175 - rmse: 0.1005 - val_loss: 0.0262 - val_mae: 0.1340 - val_mse: 0.0262 - val_rmse: 0.1340\n",
      "Epoch 72/200\n",
      "89/89 [==============================] - 0s 80us/step - loss: 0.0173 - mae: 0.1049 - mse: 0.0173 - rmse: 0.1049 - val_loss: 0.0220 - val_mae: 0.1212 - val_mse: 0.0220 - val_rmse: 0.1212\n",
      "Epoch 73/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0160 - mae: 0.0996 - mse: 0.0160 - rmse: 0.0996 - val_loss: 0.0224 - val_mae: 0.1200 - val_mse: 0.0224 - val_rmse: 0.1200\n",
      "Epoch 74/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0152 - mae: 0.0947 - mse: 0.0152 - rmse: 0.0947 - val_loss: 0.0233 - val_mae: 0.1274 - val_mse: 0.0233 - val_rmse: 0.1274\n",
      "Epoch 75/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0160 - mae: 0.1000 - mse: 0.0160 - rmse: 0.1000 - val_loss: 0.0195 - val_mae: 0.1124 - val_mse: 0.0195 - val_rmse: 0.1124\n",
      "Epoch 76/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0166 - mae: 0.0998 - mse: 0.0166 - rmse: 0.0998 - val_loss: 0.0225 - val_mae: 0.1264 - val_mse: 0.0225 - val_rmse: 0.1264\n",
      "Epoch 77/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0146 - mae: 0.0966 - mse: 0.0146 - rmse: 0.0966 - val_loss: 0.0212 - val_mae: 0.1230 - val_mse: 0.0212 - val_rmse: 0.1230\n",
      "Epoch 78/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0146 - mae: 0.0939 - mse: 0.0146 - rmse: 0.0939 - val_loss: 0.0173 - val_mae: 0.1085 - val_mse: 0.0173 - val_rmse: 0.1085\n",
      "Epoch 79/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0153 - mae: 0.0959 - mse: 0.0153 - rmse: 0.0959 - val_loss: 0.0245 - val_mae: 0.1311 - val_mse: 0.0245 - val_rmse: 0.1311\n",
      "Epoch 80/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0162 - mae: 0.1025 - mse: 0.0162 - rmse: 0.1025 - val_loss: 0.0235 - val_mae: 0.1285 - val_mse: 0.0235 - val_rmse: 0.1285\n",
      "Epoch 81/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0132 - mae: 0.0905 - mse: 0.0132 - rmse: 0.0905 - val_loss: 0.0190 - val_mae: 0.1158 - val_mse: 0.0190 - val_rmse: 0.1158\n",
      "Epoch 82/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0140 - mae: 0.0924 - mse: 0.0140 - rmse: 0.0924 - val_loss: 0.0165 - val_mae: 0.1076 - val_mse: 0.0165 - val_rmse: 0.1076\n",
      "Epoch 83/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0133 - mae: 0.0933 - mse: 0.0133 - rmse: 0.0933 - val_loss: 0.0175 - val_mae: 0.1115 - val_mse: 0.0175 - val_rmse: 0.1115\n",
      "Epoch 84/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0140 - mae: 0.0935 - mse: 0.0140 - rmse: 0.0935 - val_loss: 0.0184 - val_mae: 0.1111 - val_mse: 0.0184 - val_rmse: 0.1111\n",
      "Epoch 85/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0132 - mae: 0.0919 - mse: 0.0132 - rmse: 0.0919 - val_loss: 0.0202 - val_mae: 0.1161 - val_mse: 0.0202 - val_rmse: 0.1161\n",
      "Epoch 86/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0121 - mae: 0.0864 - mse: 0.0121 - rmse: 0.0864 - val_loss: 0.0157 - val_mae: 0.1021 - val_mse: 0.0157 - val_rmse: 0.1021\n",
      "Epoch 87/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0121 - mae: 0.0885 - mse: 0.0121 - rmse: 0.0885 - val_loss: 0.0204 - val_mae: 0.1145 - val_mse: 0.0204 - val_rmse: 0.1145\n",
      "Epoch 88/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0120 - mae: 0.0876 - mse: 0.0120 - rmse: 0.0876 - val_loss: 0.0167 - val_mae: 0.1058 - val_mse: 0.0167 - val_rmse: 0.1058\n",
      "Epoch 89/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0122 - mae: 0.0885 - mse: 0.0122 - rmse: 0.0885 - val_loss: 0.0192 - val_mae: 0.1138 - val_mse: 0.0192 - val_rmse: 0.1138\n",
      "Epoch 90/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0154 - mae: 0.1049 - mse: 0.0154 - rmse: 0.1049 - val_loss: 0.0182 - val_mae: 0.1091 - val_mse: 0.0182 - val_rmse: 0.1091\n",
      "Epoch 91/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0121 - mae: 0.0880 - mse: 0.0121 - rmse: 0.0880 - val_loss: 0.0156 - val_mae: 0.0985 - val_mse: 0.0156 - val_rmse: 0.0985\n",
      "Epoch 92/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0120 - mae: 0.0868 - mse: 0.0120 - rmse: 0.0868 - val_loss: 0.0159 - val_mae: 0.1009 - val_mse: 0.0159 - val_rmse: 0.1009\n",
      "Epoch 93/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0111 - mae: 0.0840 - mse: 0.0111 - rmse: 0.0840 - val_loss: 0.0157 - val_mae: 0.1006 - val_mse: 0.0157 - val_rmse: 0.1006\n",
      "Epoch 94/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0112 - mae: 0.0860 - mse: 0.0112 - rmse: 0.0860 - val_loss: 0.0196 - val_mae: 0.1140 - val_mse: 0.0196 - val_rmse: 0.1140\n",
      "Epoch 95/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0108 - mae: 0.0848 - mse: 0.0108 - rmse: 0.0848 - val_loss: 0.0208 - val_mae: 0.1206 - val_mse: 0.0208 - val_rmse: 0.1206\n",
      "Epoch 96/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0114 - mae: 0.0834 - mse: 0.0114 - rmse: 0.0834 - val_loss: 0.0188 - val_mae: 0.1140 - val_mse: 0.0188 - val_rmse: 0.1140\n",
      "Epoch 97/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0115 - mae: 0.0857 - mse: 0.0115 - rmse: 0.0857 - val_loss: 0.0179 - val_mae: 0.1172 - val_mse: 0.0179 - val_rmse: 0.1172\n",
      "Epoch 98/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0108 - mae: 0.0875 - mse: 0.0108 - rmse: 0.0875 - val_loss: 0.0190 - val_mae: 0.1116 - val_mse: 0.0190 - val_rmse: 0.1116\n",
      "Epoch 99/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0116 - mae: 0.0841 - mse: 0.0116 - rmse: 0.0841 - val_loss: 0.0159 - val_mae: 0.1032 - val_mse: 0.0159 - val_rmse: 0.1032\n",
      "Epoch 100/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0106 - mae: 0.0845 - mse: 0.0106 - rmse: 0.0845 - val_loss: 0.0155 - val_mae: 0.1051 - val_mse: 0.0155 - val_rmse: 0.1051\n",
      "Epoch 101/200\n",
      "89/89 [==============================] - 0s 57us/step - loss: 0.0101 - mae: 0.0825 - mse: 0.0101 - rmse: 0.0825 - val_loss: 0.0200 - val_mae: 0.1161 - val_mse: 0.0200 - val_rmse: 0.1161\n",
      "Epoch 102/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0114 - mae: 0.0881 - mse: 0.0114 - rmse: 0.0881 - val_loss: 0.0154 - val_mae: 0.0950 - val_mse: 0.0154 - val_rmse: 0.0950\n",
      "Epoch 103/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0117 - mae: 0.0887 - mse: 0.0117 - rmse: 0.0887 - val_loss: 0.0148 - val_mae: 0.1009 - val_mse: 0.0148 - val_rmse: 0.1009\n",
      "Epoch 104/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0099 - mae: 0.0792 - mse: 0.0099 - rmse: 0.0792 - val_loss: 0.0168 - val_mae: 0.1090 - val_mse: 0.0168 - val_rmse: 0.1090\n",
      "Epoch 105/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0102 - mae: 0.0818 - mse: 0.0102 - rmse: 0.0818 - val_loss: 0.0177 - val_mae: 0.0996 - val_mse: 0.0177 - val_rmse: 0.0996\n",
      "Epoch 106/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.0114 - mae: 0.0879 - mse: 0.0114 - rmse: 0.0879 - val_loss: 0.0184 - val_mae: 0.1098 - val_mse: 0.0184 - val_rmse: 0.1098\n",
      "Epoch 107/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0091 - mae: 0.0745 - mse: 0.0091 - rmse: 0.0745 - val_loss: 0.0168 - val_mae: 0.1048 - val_mse: 0.0168 - val_rmse: 0.1048\n",
      "Epoch 108/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0089 - mae: 0.0766 - mse: 0.0089 - rmse: 0.0766 - val_loss: 0.0186 - val_mae: 0.1110 - val_mse: 0.0186 - val_rmse: 0.1110\n",
      "Epoch 109/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0111 - mae: 0.0821 - mse: 0.0111 - rmse: 0.0821 - val_loss: 0.0174 - val_mae: 0.1106 - val_mse: 0.0174 - val_rmse: 0.1106\n",
      "Epoch 110/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0083 - mae: 0.0735 - mse: 0.0083 - rmse: 0.0735 - val_loss: 0.0147 - val_mae: 0.0978 - val_mse: 0.0147 - val_rmse: 0.0978\n",
      "Epoch 111/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0097 - mae: 0.0796 - mse: 0.0097 - rmse: 0.0796 - val_loss: 0.0198 - val_mae: 0.1155 - val_mse: 0.0198 - val_rmse: 0.1155\n",
      "Epoch 112/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0116 - mae: 0.0858 - mse: 0.0116 - rmse: 0.0858 - val_loss: 0.0158 - val_mae: 0.1034 - val_mse: 0.0158 - val_rmse: 0.1034\n",
      "Epoch 113/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0092 - mae: 0.0789 - mse: 0.0092 - rmse: 0.0789 - val_loss: 0.0162 - val_mae: 0.1013 - val_mse: 0.0162 - val_rmse: 0.1013\n",
      "Epoch 114/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0097 - mae: 0.0802 - mse: 0.0097 - rmse: 0.0802 - val_loss: 0.0144 - val_mae: 0.1010 - val_mse: 0.0144 - val_rmse: 0.1010\n",
      "Epoch 115/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0089 - mae: 0.0764 - mse: 0.0089 - rmse: 0.0764 - val_loss: 0.0131 - val_mae: 0.0908 - val_mse: 0.0131 - val_rmse: 0.0908\n",
      "Epoch 116/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0081 - mae: 0.0744 - mse: 0.0081 - rmse: 0.0744 - val_loss: 0.0218 - val_mae: 0.1210 - val_mse: 0.0218 - val_rmse: 0.1210\n",
      "Epoch 117/200\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.0118 - mae: 0.0854 - mse: 0.0118 - rmse: 0.08 - 0s 69us/step - loss: 0.0119 - mae: 0.0889 - mse: 0.0119 - rmse: 0.0889 - val_loss: 0.0144 - val_mae: 0.0922 - val_mse: 0.0144 - val_rmse: 0.0922\n",
      "Epoch 118/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0078 - mae: 0.0723 - mse: 0.0078 - rmse: 0.0723 - val_loss: 0.0143 - val_mae: 0.0978 - val_mse: 0.0143 - val_rmse: 0.0978\n",
      "Epoch 119/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0074 - mae: 0.0698 - mse: 0.0074 - rmse: 0.0698 - val_loss: 0.0168 - val_mae: 0.1054 - val_mse: 0.0168 - val_rmse: 0.1054\n",
      "Epoch 120/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0108 - mae: 0.0861 - mse: 0.0108 - rmse: 0.0861 - val_loss: 0.0243 - val_mae: 0.1150 - val_mse: 0.0243 - val_rmse: 0.1150\n",
      "Epoch 121/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0093 - mae: 0.0818 - mse: 0.0093 - rmse: 0.0818 - val_loss: 0.0144 - val_mae: 0.0931 - val_mse: 0.0144 - val_rmse: 0.0931\n",
      "Epoch 122/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0079 - mae: 0.0720 - mse: 0.0079 - rmse: 0.0720 - val_loss: 0.0137 - val_mae: 0.0921 - val_mse: 0.0137 - val_rmse: 0.0921\n",
      "Epoch 123/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0093 - mae: 0.0809 - mse: 0.0093 - rmse: 0.0809 - val_loss: 0.0142 - val_mae: 0.0888 - val_mse: 0.0142 - val_rmse: 0.0888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0079 - mae: 0.0715 - mse: 0.0079 - rmse: 0.0715 - val_loss: 0.0136 - val_mae: 0.0879 - val_mse: 0.0136 - val_rmse: 0.0879\n",
      "Epoch 125/200\n",
      "89/89 [==============================] - 0s 92us/step - loss: 0.0102 - mae: 0.0839 - mse: 0.0102 - rmse: 0.0839 - val_loss: 0.0175 - val_mae: 0.1061 - val_mse: 0.0175 - val_rmse: 0.1061\n",
      "Epoch 126/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0081 - mae: 0.0702 - mse: 0.0081 - rmse: 0.0702 - val_loss: 0.0133 - val_mae: 0.0904 - val_mse: 0.0133 - val_rmse: 0.0904\n",
      "Epoch 127/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0071 - mae: 0.0696 - mse: 0.0071 - rmse: 0.0696 - val_loss: 0.0146 - val_mae: 0.0949 - val_mse: 0.0146 - val_rmse: 0.0949\n",
      "Epoch 128/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0086 - mae: 0.0764 - mse: 0.0086 - rmse: 0.0764 - val_loss: 0.0209 - val_mae: 0.1141 - val_mse: 0.0209 - val_rmse: 0.1141\n",
      "Epoch 129/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0092 - mae: 0.0781 - mse: 0.0092 - rmse: 0.0781 - val_loss: 0.0140 - val_mae: 0.0918 - val_mse: 0.0140 - val_rmse: 0.0918\n",
      "Epoch 130/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0071 - mae: 0.0684 - mse: 0.0071 - rmse: 0.0684 - val_loss: 0.0136 - val_mae: 0.0911 - val_mse: 0.0136 - val_rmse: 0.0911\n",
      "Epoch 131/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0089 - mae: 0.0762 - mse: 0.0089 - rmse: 0.0762 - val_loss: 0.0169 - val_mae: 0.1022 - val_mse: 0.0169 - val_rmse: 0.1022\n",
      "Epoch 132/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.0070 - mae: 0.0683 - mse: 0.0070 - rmse: 0.0683 - val_loss: 0.0180 - val_mae: 0.1036 - val_mse: 0.0180 - val_rmse: 0.1036\n",
      "Epoch 133/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0091 - mae: 0.0747 - mse: 0.0091 - rmse: 0.0747 - val_loss: 0.0164 - val_mae: 0.1026 - val_mse: 0.0164 - val_rmse: 0.1026\n",
      "Epoch 134/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0081 - mae: 0.0745 - mse: 0.0081 - rmse: 0.0745 - val_loss: 0.0128 - val_mae: 0.0848 - val_mse: 0.0128 - val_rmse: 0.0848\n",
      "Epoch 135/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0099 - mae: 0.0801 - mse: 0.0099 - rmse: 0.0801 - val_loss: 0.0141 - val_mae: 0.0906 - val_mse: 0.0141 - val_rmse: 0.0906\n",
      "Epoch 136/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0077 - mae: 0.0726 - mse: 0.0077 - rmse: 0.0726 - val_loss: 0.0151 - val_mae: 0.0949 - val_mse: 0.0151 - val_rmse: 0.0949\n",
      "Epoch 137/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0079 - mae: 0.0749 - mse: 0.0079 - rmse: 0.0749 - val_loss: 0.0139 - val_mae: 0.0912 - val_mse: 0.0139 - val_rmse: 0.0912\n",
      "Epoch 138/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0063 - mae: 0.0656 - mse: 0.0063 - rmse: 0.0656 - val_loss: 0.0147 - val_mae: 0.0962 - val_mse: 0.0147 - val_rmse: 0.0962\n",
      "Epoch 139/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0071 - mae: 0.0682 - mse: 0.0071 - rmse: 0.0682 - val_loss: 0.0165 - val_mae: 0.1005 - val_mse: 0.0165 - val_rmse: 0.1005\n",
      "Epoch 140/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0087 - mae: 0.0732 - mse: 0.0087 - rmse: 0.0732 - val_loss: 0.0138 - val_mae: 0.0886 - val_mse: 0.0138 - val_rmse: 0.0886\n",
      "Epoch 141/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0074 - mae: 0.0710 - mse: 0.0074 - rmse: 0.0710 - val_loss: 0.0123 - val_mae: 0.0836 - val_mse: 0.0123 - val_rmse: 0.0836\n",
      "Epoch 142/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0071 - mae: 0.0711 - mse: 0.0071 - rmse: 0.0711 - val_loss: 0.0133 - val_mae: 0.0891 - val_mse: 0.0133 - val_rmse: 0.0891\n",
      "Epoch 143/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0086 - mae: 0.0754 - mse: 0.0086 - rmse: 0.0754 - val_loss: 0.0165 - val_mae: 0.0961 - val_mse: 0.0165 - val_rmse: 0.0961\n",
      "Epoch 144/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0067 - mae: 0.0665 - mse: 0.0067 - rmse: 0.0665 - val_loss: 0.0120 - val_mae: 0.0827 - val_mse: 0.0120 - val_rmse: 0.0827\n",
      "Epoch 145/200\n",
      "89/89 [==============================] - 0s 57us/step - loss: 0.0055 - mae: 0.0610 - mse: 0.0055 - rmse: 0.0610 - val_loss: 0.0120 - val_mae: 0.0834 - val_mse: 0.0120 - val_rmse: 0.0834\n",
      "Epoch 146/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0069 - mae: 0.0651 - mse: 0.0069 - rmse: 0.0651 - val_loss: 0.0138 - val_mae: 0.0920 - val_mse: 0.0138 - val_rmse: 0.0920\n",
      "Epoch 147/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0074 - mae: 0.0706 - mse: 0.0074 - rmse: 0.0706 - val_loss: 0.0155 - val_mae: 0.0950 - val_mse: 0.0155 - val_rmse: 0.0950\n",
      "Epoch 148/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0066 - mae: 0.0663 - mse: 0.0066 - rmse: 0.0663 - val_loss: 0.0123 - val_mae: 0.0849 - val_mse: 0.0123 - val_rmse: 0.0849\n",
      "Epoch 149/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0066 - mae: 0.0649 - mse: 0.0066 - rmse: 0.0649 - val_loss: 0.0152 - val_mae: 0.0995 - val_mse: 0.0152 - val_rmse: 0.0995\n",
      "Epoch 150/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0072 - mae: 0.0689 - mse: 0.0072 - rmse: 0.0689 - val_loss: 0.0171 - val_mae: 0.1022 - val_mse: 0.0171 - val_rmse: 0.1022\n",
      "Epoch 151/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0072 - mae: 0.0669 - mse: 0.0072 - rmse: 0.0669 - val_loss: 0.0127 - val_mae: 0.0875 - val_mse: 0.0127 - val_rmse: 0.0875\n",
      "Epoch 152/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0074 - mae: 0.0714 - mse: 0.0074 - rmse: 0.0714 - val_loss: 0.0120 - val_mae: 0.0859 - val_mse: 0.0120 - val_rmse: 0.0859\n",
      "Epoch 153/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0071 - mae: 0.0681 - mse: 0.0071 - rmse: 0.0681 - val_loss: 0.0133 - val_mae: 0.0889 - val_mse: 0.0133 - val_rmse: 0.0889\n",
      "Epoch 154/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0070 - mae: 0.0706 - mse: 0.0070 - rmse: 0.0706 - val_loss: 0.0126 - val_mae: 0.0874 - val_mse: 0.0126 - val_rmse: 0.0874\n",
      "Epoch 155/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0062 - mae: 0.0636 - mse: 0.0062 - rmse: 0.0636 - val_loss: 0.0148 - val_mae: 0.0949 - val_mse: 0.0148 - val_rmse: 0.0949\n",
      "Epoch 156/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0067 - mae: 0.0660 - mse: 0.0067 - rmse: 0.0660 - val_loss: 0.0142 - val_mae: 0.0918 - val_mse: 0.0142 - val_rmse: 0.0918\n",
      "Epoch 157/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0073 - mae: 0.0692 - mse: 0.0073 - rmse: 0.0692 - val_loss: 0.0128 - val_mae: 0.0867 - val_mse: 0.0128 - val_rmse: 0.0867\n",
      "Epoch 158/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0063 - mae: 0.0615 - mse: 0.0063 - rmse: 0.0615 - val_loss: 0.0132 - val_mae: 0.0890 - val_mse: 0.0132 - val_rmse: 0.0890\n",
      "Epoch 159/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0063 - mae: 0.0662 - mse: 0.0063 - rmse: 0.0662 - val_loss: 0.0115 - val_mae: 0.0832 - val_mse: 0.0115 - val_rmse: 0.0832\n",
      "Epoch 160/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0065 - mae: 0.0657 - mse: 0.0065 - rmse: 0.0657 - val_loss: 0.0150 - val_mae: 0.0916 - val_mse: 0.0150 - val_rmse: 0.0916\n",
      "Epoch 161/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0069 - mae: 0.0671 - mse: 0.0069 - rmse: 0.0671 - val_loss: 0.0129 - val_mae: 0.0881 - val_mse: 0.0129 - val_rmse: 0.0881\n",
      "Epoch 162/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0072 - mae: 0.0691 - mse: 0.0072 - rmse: 0.0691 - val_loss: 0.0118 - val_mae: 0.0824 - val_mse: 0.0118 - val_rmse: 0.0824\n",
      "Epoch 163/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0067 - mae: 0.0671 - mse: 0.0067 - rmse: 0.0671 - val_loss: 0.0212 - val_mae: 0.1164 - val_mse: 0.0212 - val_rmse: 0.1164\n",
      "Epoch 164/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0094 - mae: 0.0756 - mse: 0.0094 - rmse: 0.0756 - val_loss: 0.0123 - val_mae: 0.0872 - val_mse: 0.0123 - val_rmse: 0.0872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0050 - mae: 0.0569 - mse: 0.0050 - rmse: 0.0569 - val_loss: 0.0127 - val_mae: 0.0862 - val_mse: 0.0127 - val_rmse: 0.0862\n",
      "Epoch 166/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0062 - mae: 0.0647 - mse: 0.0062 - rmse: 0.0647 - val_loss: 0.0217 - val_mae: 0.1168 - val_mse: 0.0217 - val_rmse: 0.1168\n",
      "Epoch 167/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0077 - mae: 0.0688 - mse: 0.0077 - rmse: 0.0688 - val_loss: 0.0114 - val_mae: 0.0828 - val_mse: 0.0114 - val_rmse: 0.0828\n",
      "Epoch 168/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0054 - mae: 0.0589 - mse: 0.0054 - rmse: 0.0589 - val_loss: 0.0115 - val_mae: 0.0807 - val_mse: 0.0115 - val_rmse: 0.0807\n",
      "Epoch 169/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0050 - mae: 0.0591 - mse: 0.0050 - rmse: 0.0591 - val_loss: 0.0173 - val_mae: 0.1012 - val_mse: 0.0173 - val_rmse: 0.1012\n",
      "Epoch 170/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0054 - mae: 0.0598 - mse: 0.0054 - rmse: 0.0598 - val_loss: 0.0113 - val_mae: 0.0813 - val_mse: 0.0113 - val_rmse: 0.0813\n",
      "Epoch 171/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0066 - mae: 0.0671 - mse: 0.0066 - rmse: 0.0671 - val_loss: 0.0118 - val_mae: 0.0834 - val_mse: 0.0118 - val_rmse: 0.0834\n",
      "Epoch 172/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0057 - mae: 0.0611 - mse: 0.0057 - rmse: 0.0611 - val_loss: 0.0153 - val_mae: 0.0997 - val_mse: 0.0153 - val_rmse: 0.0997\n",
      "Epoch 173/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0056 - mae: 0.0597 - mse: 0.0056 - rmse: 0.0597 - val_loss: 0.0134 - val_mae: 0.0894 - val_mse: 0.0134 - val_rmse: 0.0894\n",
      "Epoch 174/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0082 - mae: 0.0741 - mse: 0.0082 - rmse: 0.0741 - val_loss: 0.0131 - val_mae: 0.0848 - val_mse: 0.0131 - val_rmse: 0.0848\n",
      "Epoch 175/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0055 - mae: 0.0593 - mse: 0.0055 - rmse: 0.0593 - val_loss: 0.0133 - val_mae: 0.0923 - val_mse: 0.0133 - val_rmse: 0.0923\n",
      "Epoch 176/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0056 - mae: 0.0608 - mse: 0.0056 - rmse: 0.0608 - val_loss: 0.0131 - val_mae: 0.0835 - val_mse: 0.0131 - val_rmse: 0.0835\n",
      "Epoch 177/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0075 - mae: 0.0703 - mse: 0.0075 - rmse: 0.0703 - val_loss: 0.0136 - val_mae: 0.0942 - val_mse: 0.0136 - val_rmse: 0.0942\n",
      "Epoch 178/200\n",
      "89/89 [==============================] - 0s 91us/step - loss: 0.0062 - mae: 0.0629 - mse: 0.0062 - rmse: 0.0629 - val_loss: 0.0133 - val_mae: 0.0887 - val_mse: 0.0133 - val_rmse: 0.0887\n",
      "Epoch 179/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0049 - mae: 0.0568 - mse: 0.0049 - rmse: 0.0568 - val_loss: 0.0130 - val_mae: 0.0893 - val_mse: 0.0130 - val_rmse: 0.0893\n",
      "Epoch 180/200\n",
      "89/89 [==============================] - 0s 84us/step - loss: 0.0053 - mae: 0.0587 - mse: 0.0053 - rmse: 0.0587 - val_loss: 0.0128 - val_mae: 0.0866 - val_mse: 0.0128 - val_rmse: 0.0866\n",
      "Epoch 181/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0055 - mae: 0.0604 - mse: 0.0055 - rmse: 0.0604 - val_loss: 0.0147 - val_mae: 0.0969 - val_mse: 0.0147 - val_rmse: 0.0969\n",
      "Epoch 182/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.0054 - mae: 0.0612 - mse: 0.0054 - rmse: 0.0612 - val_loss: 0.0126 - val_mae: 0.0863 - val_mse: 0.0126 - val_rmse: 0.0863\n",
      "Epoch 183/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0077 - mae: 0.0697 - mse: 0.0077 - rmse: 0.0697 - val_loss: 0.0164 - val_mae: 0.0998 - val_mse: 0.0164 - val_rmse: 0.0998\n",
      "Epoch 184/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.0065 - mae: 0.0655 - mse: 0.0065 - rmse: 0.0655 - val_loss: 0.0112 - val_mae: 0.0792 - val_mse: 0.0112 - val_rmse: 0.0792\n",
      "Epoch 185/200\n",
      "89/89 [==============================] - 0s 85us/step - loss: 0.0053 - mae: 0.0603 - mse: 0.0053 - rmse: 0.0603 - val_loss: 0.0137 - val_mae: 0.0912 - val_mse: 0.0137 - val_rmse: 0.0912\n",
      "Epoch 186/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0089 - mae: 0.0773 - mse: 0.0089 - rmse: 0.0773 - val_loss: 0.0097 - val_mae: 0.0737 - val_mse: 0.0097 - val_rmse: 0.0737\n",
      "Epoch 187/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0049 - mae: 0.0568 - mse: 0.0049 - rmse: 0.0568 - val_loss: 0.0156 - val_mae: 0.0908 - val_mse: 0.0156 - val_rmse: 0.0908\n",
      "Epoch 188/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0058 - mae: 0.0617 - mse: 0.0058 - rmse: 0.0617 - val_loss: 0.0118 - val_mae: 0.0880 - val_mse: 0.0118 - val_rmse: 0.0880\n",
      "Epoch 189/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0056 - mae: 0.0634 - mse: 0.0056 - rmse: 0.0634 - val_loss: 0.0108 - val_mae: 0.0810 - val_mse: 0.0108 - val_rmse: 0.0810\n",
      "Epoch 190/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0044 - mae: 0.0558 - mse: 0.0044 - rmse: 0.0558 - val_loss: 0.0123 - val_mae: 0.0883 - val_mse: 0.0123 - val_rmse: 0.0883\n",
      "Epoch 191/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0051 - mae: 0.0568 - mse: 0.0051 - rmse: 0.0568 - val_loss: 0.0153 - val_mae: 0.0982 - val_mse: 0.0153 - val_rmse: 0.0982\n",
      "Epoch 192/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0065 - mae: 0.0650 - mse: 0.0065 - rmse: 0.0650 - val_loss: 0.0102 - val_mae: 0.0776 - val_mse: 0.0102 - val_rmse: 0.0776\n",
      "Epoch 193/200\n",
      "89/89 [==============================] - 0s 92us/step - loss: 0.0047 - mae: 0.0565 - mse: 0.0047 - rmse: 0.0565 - val_loss: 0.0118 - val_mae: 0.0809 - val_mse: 0.0118 - val_rmse: 0.0809\n",
      "Epoch 194/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0051 - mae: 0.0607 - mse: 0.0051 - rmse: 0.0607 - val_loss: 0.0125 - val_mae: 0.0841 - val_mse: 0.0125 - val_rmse: 0.0841\n",
      "Epoch 195/200\n",
      "89/89 [==============================] - 0s 58us/step - loss: 0.0044 - mae: 0.0521 - mse: 0.0044 - rmse: 0.0521 - val_loss: 0.0111 - val_mae: 0.0787 - val_mse: 0.0111 - val_rmse: 0.0787\n",
      "Epoch 196/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0075 - mae: 0.0707 - mse: 0.0075 - rmse: 0.0707 - val_loss: 0.0153 - val_mae: 0.1016 - val_mse: 0.0153 - val_rmse: 0.1016\n",
      "Epoch 197/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0066 - mae: 0.0660 - mse: 0.0066 - rmse: 0.0660 - val_loss: 0.0133 - val_mae: 0.0922 - val_mse: 0.0133 - val_rmse: 0.0922\n",
      "Epoch 198/200\n",
      "89/89 [==============================] - 0s 85us/step - loss: 0.0051 - mae: 0.0576 - mse: 0.0051 - rmse: 0.0576 - val_loss: 0.0109 - val_mae: 0.0823 - val_mse: 0.0109 - val_rmse: 0.0823\n",
      "Epoch 199/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0045 - mae: 0.0547 - mse: 0.0045 - rmse: 0.0547 - val_loss: 0.0113 - val_mae: 0.0834 - val_mse: 0.0113 - val_rmse: 0.0834\n",
      "Epoch 200/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0047 - mae: 0.0566 - mse: 0.0047 - rmse: 0.0566 - val_loss: 0.0110 - val_mae: 0.0815 - val_mse: 0.0110 - val_rmse: 0.0815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd75401ccd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 445a5d07d343190d1dcb75b23cfe3f883ec49c2f:main_Predicoes.ipynb
   "source": [
    "model.fit(X_treinamento, y_treinamento, \n",
    "          epochs=200,\n",
    "          batch_size = 30,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = model.predict(X_teste)\n",
    "\n",
    "test_predictions = model.predict(X_teste)\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(scaler_y.inverse_transform(y_teste), scaler_y.inverse_transform(test_predictions))\n",
    "plt.xlabel('True Values ')\n",
    "plt.ylabel('Predictions ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficiente de determinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "acuracia_teste = r2_score(scaler_y.inverse_transform(y_teste),\n",
    "                          scaler_y.inverse_transform(test_predictions))\n",
    "\n",
    "previsao_todas = scaler_y.inverse_transform(model.predict(X_teste))\n",
    "y_teste_real = scaler_y.inverse_transform(y_teste)\n",
    "\n",
    "acuracia_teste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
<<<<<<< HEAD:IC_keras.ipynb
=======
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
>>>>>>> 445a5d07d343190d1dcb75b23cfe3f883ec49c2f:main_Predicoes.ipynb
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
