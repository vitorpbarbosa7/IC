{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão da permissividade dielétrica na frequência de 915 MHz a partir das propriedades físico químicas de 8 diferentes sucos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "###Depois descomentar aqui\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura  e visualização da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_excel('ic.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>Acidez</th>\n",
       "      <th>Aw</th>\n",
       "      <th>Umidade</th>\n",
       "      <th>ST</th>\n",
       "      <th>Brix</th>\n",
       "      <th>Cor - L*</th>\n",
       "      <th>Cor - a*</th>\n",
       "      <th>Cor - b*</th>\n",
       "      <th>sig</th>\n",
       "      <th>w1_915</th>\n",
       "      <th>w1_2450</th>\n",
       "      <th>w2_915</th>\n",
       "      <th>w2_2450</th>\n",
       "      <th>e1_915</th>\n",
       "      <th>e1_2450</th>\n",
       "      <th>e2_915</th>\n",
       "      <th>e2_2450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>90</td>\n",
       "      <td>0.080595</td>\n",
       "      <td>0.993</td>\n",
       "      <td>88.777440</td>\n",
       "      <td>28.001800</td>\n",
       "      <td>11.76</td>\n",
       "      <td>32.700000</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.423333</td>\n",
       "      <td>12.683333</td>\n",
       "      <td>1.005311</td>\n",
       "      <td>1.005091</td>\n",
       "      <td>5.265066</td>\n",
       "      <td>2.351171</td>\n",
       "      <td>61.90826</td>\n",
       "      <td>60.16880</td>\n",
       "      <td>31.32553</td>\n",
       "      <td>14.67355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>90</td>\n",
       "      <td>0.640485</td>\n",
       "      <td>0.953</td>\n",
       "      <td>89.983175</td>\n",
       "      <td>10.016825</td>\n",
       "      <td>12.88</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>12.153171</td>\n",
       "      <td>0.970395</td>\n",
       "      <td>0.951848</td>\n",
       "      <td>4.228839</td>\n",
       "      <td>1.962878</td>\n",
       "      <td>59.01914</td>\n",
       "      <td>57.50490</td>\n",
       "      <td>26.10140</td>\n",
       "      <td>11.98970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>80</td>\n",
       "      <td>1.156098</td>\n",
       "      <td>0.906</td>\n",
       "      <td>90.035023</td>\n",
       "      <td>9.964977</td>\n",
       "      <td>26.08</td>\n",
       "      <td>33.180000</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-1.450000</td>\n",
       "      <td>5.362861</td>\n",
       "      <td>0.964807</td>\n",
       "      <td>1.186616</td>\n",
       "      <td>1.850816</td>\n",
       "      <td>2.263748</td>\n",
       "      <td>61.27884</td>\n",
       "      <td>60.73280</td>\n",
       "      <td>9.80234</td>\n",
       "      <td>6.57210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>70</td>\n",
       "      <td>0.632628</td>\n",
       "      <td>0.954</td>\n",
       "      <td>90.070932</td>\n",
       "      <td>9.929068</td>\n",
       "      <td>12.78</td>\n",
       "      <td>43.140000</td>\n",
       "      <td>-1.420000</td>\n",
       "      <td>13.830000</td>\n",
       "      <td>9.695000</td>\n",
       "      <td>0.972601</td>\n",
       "      <td>0.948325</td>\n",
       "      <td>3.449075</td>\n",
       "      <td>1.646802</td>\n",
       "      <td>64.24496</td>\n",
       "      <td>62.46380</td>\n",
       "      <td>20.97458</td>\n",
       "      <td>11.66050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>50</td>\n",
       "      <td>0.640485</td>\n",
       "      <td>0.953</td>\n",
       "      <td>89.983175</td>\n",
       "      <td>10.016825</td>\n",
       "      <td>12.88</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>7.786388</td>\n",
       "      <td>0.963829</td>\n",
       "      <td>0.937601</td>\n",
       "      <td>2.731786</td>\n",
       "      <td>1.486015</td>\n",
       "      <td>67.99030</td>\n",
       "      <td>65.91400</td>\n",
       "      <td>17.99186</td>\n",
       "      <td>12.14730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>50</td>\n",
       "      <td>0.084709</td>\n",
       "      <td>0.956</td>\n",
       "      <td>88.790259</td>\n",
       "      <td>19.422500</td>\n",
       "      <td>11.56</td>\n",
       "      <td>32.710000</td>\n",
       "      <td>-0.752783</td>\n",
       "      <td>-0.377145</td>\n",
       "      <td>8.008415</td>\n",
       "      <td>0.982527</td>\n",
       "      <td>0.985974</td>\n",
       "      <td>2.993157</td>\n",
       "      <td>1.616580</td>\n",
       "      <td>68.27757</td>\n",
       "      <td>66.06675</td>\n",
       "      <td>21.86165</td>\n",
       "      <td>13.94035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40</td>\n",
       "      <td>0.538279</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.411839</td>\n",
       "      <td>8.588161</td>\n",
       "      <td>8.58</td>\n",
       "      <td>32.593333</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>5.803333</td>\n",
       "      <td>1.159000</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.057319</td>\n",
       "      <td>1.393115</td>\n",
       "      <td>1.166682</td>\n",
       "      <td>71.79364</td>\n",
       "      <td>70.48540</td>\n",
       "      <td>5.85626</td>\n",
       "      <td>8.63710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>30</td>\n",
       "      <td>0.596655</td>\n",
       "      <td>0.947</td>\n",
       "      <td>92.973276</td>\n",
       "      <td>7.026724</td>\n",
       "      <td>29.61</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>22.790000</td>\n",
       "      <td>16.910000</td>\n",
       "      <td>2.750660</td>\n",
       "      <td>0.980062</td>\n",
       "      <td>0.970798</td>\n",
       "      <td>1.220571</td>\n",
       "      <td>1.217685</td>\n",
       "      <td>75.20026</td>\n",
       "      <td>73.61110</td>\n",
       "      <td>6.34590</td>\n",
       "      <td>10.27230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>90</td>\n",
       "      <td>0.557757</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.347034</td>\n",
       "      <td>8.652966</td>\n",
       "      <td>8.68</td>\n",
       "      <td>32.599107</td>\n",
       "      <td>-0.601188</td>\n",
       "      <td>5.858409</td>\n",
       "      <td>2.167000</td>\n",
       "      <td>0.990577</td>\n",
       "      <td>1.265180</td>\n",
       "      <td>1.430307</td>\n",
       "      <td>1.252586</td>\n",
       "      <td>60.59102</td>\n",
       "      <td>59.78150</td>\n",
       "      <td>6.12536</td>\n",
       "      <td>4.88620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>50</td>\n",
       "      <td>0.602444</td>\n",
       "      <td>0.948</td>\n",
       "      <td>92.991176</td>\n",
       "      <td>7.008824</td>\n",
       "      <td>29.63</td>\n",
       "      <td>39.690000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>3.321969</td>\n",
       "      <td>0.986650</td>\n",
       "      <td>0.978802</td>\n",
       "      <td>1.448467</td>\n",
       "      <td>1.386153</td>\n",
       "      <td>69.75121</td>\n",
       "      <td>68.88360</td>\n",
       "      <td>6.02837</td>\n",
       "      <td>7.30225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>0.557757</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.347034</td>\n",
       "      <td>8.652966</td>\n",
       "      <td>8.68</td>\n",
       "      <td>32.599107</td>\n",
       "      <td>-0.601188</td>\n",
       "      <td>5.858409</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.057319</td>\n",
       "      <td>1.393115</td>\n",
       "      <td>1.166682</td>\n",
       "      <td>71.75818</td>\n",
       "      <td>70.43590</td>\n",
       "      <td>5.86250</td>\n",
       "      <td>8.53730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>80</td>\n",
       "      <td>0.084709</td>\n",
       "      <td>0.956</td>\n",
       "      <td>88.790259</td>\n",
       "      <td>19.422500</td>\n",
       "      <td>11.56</td>\n",
       "      <td>32.710000</td>\n",
       "      <td>-0.752783</td>\n",
       "      <td>-0.377145</td>\n",
       "      <td>11.492440</td>\n",
       "      <td>1.005865</td>\n",
       "      <td>1.006491</td>\n",
       "      <td>3.720197</td>\n",
       "      <td>1.936303</td>\n",
       "      <td>62.93572</td>\n",
       "      <td>61.10470</td>\n",
       "      <td>28.76096</td>\n",
       "      <td>14.16600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>0.956</td>\n",
       "      <td>91.539304</td>\n",
       "      <td>32.769800</td>\n",
       "      <td>7.78</td>\n",
       "      <td>65.850000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>0.555067</td>\n",
       "      <td>0.964130</td>\n",
       "      <td>0.943401</td>\n",
       "      <td>1.250194</td>\n",
       "      <td>1.075298</td>\n",
       "      <td>80.23920</td>\n",
       "      <td>76.33970</td>\n",
       "      <td>7.79848</td>\n",
       "      <td>15.68805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>30</td>\n",
       "      <td>1.156098</td>\n",
       "      <td>0.906</td>\n",
       "      <td>90.035023</td>\n",
       "      <td>9.964977</td>\n",
       "      <td>26.08</td>\n",
       "      <td>33.180000</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-1.450000</td>\n",
       "      <td>2.874097</td>\n",
       "      <td>0.951817</td>\n",
       "      <td>0.988299</td>\n",
       "      <td>1.498534</td>\n",
       "      <td>0.856108</td>\n",
       "      <td>73.27514</td>\n",
       "      <td>71.14590</td>\n",
       "      <td>8.57916</td>\n",
       "      <td>11.26380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>20</td>\n",
       "      <td>0.164489</td>\n",
       "      <td>0.986</td>\n",
       "      <td>86.687819</td>\n",
       "      <td>32.041800</td>\n",
       "      <td>6.78</td>\n",
       "      <td>35.553925</td>\n",
       "      <td>-0.752783</td>\n",
       "      <td>-0.377145</td>\n",
       "      <td>1.152911</td>\n",
       "      <td>0.992871</td>\n",
       "      <td>1.011334</td>\n",
       "      <td>1.349007</td>\n",
       "      <td>1.100703</td>\n",
       "      <td>79.49518</td>\n",
       "      <td>77.05370</td>\n",
       "      <td>7.90874</td>\n",
       "      <td>13.07800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>70</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>0.956</td>\n",
       "      <td>91.539304</td>\n",
       "      <td>32.769800</td>\n",
       "      <td>7.78</td>\n",
       "      <td>65.850000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>1.633667</td>\n",
       "      <td>0.988566</td>\n",
       "      <td>1.172366</td>\n",
       "      <td>1.375602</td>\n",
       "      <td>1.139686</td>\n",
       "      <td>65.07668</td>\n",
       "      <td>64.40500</td>\n",
       "      <td>5.38774</td>\n",
       "      <td>5.52770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>70</td>\n",
       "      <td>1.556349</td>\n",
       "      <td>0.955</td>\n",
       "      <td>89.885415</td>\n",
       "      <td>10.114585</td>\n",
       "      <td>26.38</td>\n",
       "      <td>33.130000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-1.590000</td>\n",
       "      <td>8.275347</td>\n",
       "      <td>0.965897</td>\n",
       "      <td>1.145453</td>\n",
       "      <td>1.851697</td>\n",
       "      <td>1.909361</td>\n",
       "      <td>63.15934</td>\n",
       "      <td>62.27990</td>\n",
       "      <td>9.57198</td>\n",
       "      <td>6.92280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>70</td>\n",
       "      <td>0.596655</td>\n",
       "      <td>0.947</td>\n",
       "      <td>92.973276</td>\n",
       "      <td>7.026724</td>\n",
       "      <td>29.61</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>22.790000</td>\n",
       "      <td>16.910000</td>\n",
       "      <td>4.639950</td>\n",
       "      <td>0.997888</td>\n",
       "      <td>0.989731</td>\n",
       "      <td>1.525275</td>\n",
       "      <td>1.588065</td>\n",
       "      <td>65.13674</td>\n",
       "      <td>64.42805</td>\n",
       "      <td>6.28982</td>\n",
       "      <td>5.81900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>80</td>\n",
       "      <td>0.164489</td>\n",
       "      <td>0.986</td>\n",
       "      <td>86.687819</td>\n",
       "      <td>32.041800</td>\n",
       "      <td>6.78</td>\n",
       "      <td>35.553925</td>\n",
       "      <td>-0.752783</td>\n",
       "      <td>-0.377145</td>\n",
       "      <td>2.933722</td>\n",
       "      <td>1.002411</td>\n",
       "      <td>1.258244</td>\n",
       "      <td>1.904247</td>\n",
       "      <td>1.223992</td>\n",
       "      <td>63.20100</td>\n",
       "      <td>62.58710</td>\n",
       "      <td>8.56347</td>\n",
       "      <td>5.99190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>0.538279</td>\n",
       "      <td>0.959</td>\n",
       "      <td>91.411839</td>\n",
       "      <td>8.588161</td>\n",
       "      <td>8.58</td>\n",
       "      <td>32.593333</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>5.803333</td>\n",
       "      <td>1.356333</td>\n",
       "      <td>0.981347</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>1.453888</td>\n",
       "      <td>1.189854</td>\n",
       "      <td>69.55504</td>\n",
       "      <td>68.44280</td>\n",
       "      <td>5.71930</td>\n",
       "      <td>7.37740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T    Acidez     Aw    Umidade         ST   Brix   Cor - L*   Cor - a*  \\\n",
       "38   90  0.080595  0.993  88.777440  28.001800  11.76  32.700000  -0.783333   \n",
       "99   90  0.640485  0.953  89.983175  10.016825  12.88  42.600000  -1.520000   \n",
       "116  80  1.156098  0.906  90.035023   9.964977  26.08  33.180000  -0.260000   \n",
       "94   70  0.632628  0.954  90.070932   9.929068  12.78  43.140000  -1.420000   \n",
       "91   50  0.640485  0.953  89.983175  10.016825  12.88  42.600000  -1.520000   \n",
       "31   50  0.084709  0.956  88.790259  19.422500  11.56  32.710000  -0.752783   \n",
       "8    40  0.538279  0.959  91.411839   8.588161   8.58  32.593333  -0.633333   \n",
       "127  30  0.596655  0.947  92.973276   7.026724  29.61  39.700000  22.790000   \n",
       "19   90  0.557757  0.959  91.347034   8.652966   8.68  32.599107  -0.601188   \n",
       "130  50  0.602444  0.948  92.991176   7.008824  29.63  39.690000  22.800000   \n",
       "9    40  0.557757  0.959  91.347034   8.652966   8.68  32.599107  -0.601188   \n",
       "37   80  0.084709  0.956  88.790259  19.422500  11.56  32.710000  -0.752783   \n",
       "62   10  0.056794  0.956  91.539304  32.769800   7.78  65.850000   3.900000   \n",
       "106  30  1.156098  0.906  90.035023   9.964977  26.08  33.180000  -0.260000   \n",
       "45   20  0.164489  0.986  86.687819  32.041800   6.78  35.553925  -0.752783   \n",
       "74   70  0.056794  0.956  91.539304  32.769800   7.78  65.850000   3.900000   \n",
       "115  70  1.556349  0.955  89.885415  10.114585  26.38  33.130000  -0.250000   \n",
       "135  70  0.596655  0.947  92.973276   7.026724  29.61  39.700000  22.790000   \n",
       "57   80  0.164489  0.986  86.687819  32.041800   6.78  35.553925  -0.752783   \n",
       "10   50  0.538279  0.959  91.411839   8.588161   8.58  32.593333  -0.633333   \n",
       "\n",
       "      Cor - b*        sig    w1_915   w1_2450    w2_915   w2_2450    e1_915  \\\n",
       "38   -0.423333  12.683333  1.005311  1.005091  5.265066  2.351171  61.90826   \n",
       "99   13.030000  12.153171  0.970395  0.951848  4.228839  1.962878  59.01914   \n",
       "116  -1.450000   5.362861  0.964807  1.186616  1.850816  2.263748  61.27884   \n",
       "94   13.830000   9.695000  0.972601  0.948325  3.449075  1.646802  64.24496   \n",
       "91   13.030000   7.786388  0.963829  0.937601  2.731786  1.486015  67.99030   \n",
       "31   -0.377145   8.008415  0.982527  0.985974  2.993157  1.616580  68.27757   \n",
       "8     5.803333   1.159000  0.976608  1.057319  1.393115  1.166682  71.79364   \n",
       "127  16.910000   2.750660  0.980062  0.970798  1.220571  1.217685  75.20026   \n",
       "19    5.858409   2.167000  0.990577  1.265180  1.430307  1.252586  60.59102   \n",
       "130  16.900000   3.321969  0.986650  0.978802  1.448467  1.386153  69.75121   \n",
       "9     5.858409   1.160000  0.976608  1.057319  1.393115  1.166682  71.75818   \n",
       "37   -0.377145  11.492440  1.005865  1.006491  3.720197  1.936303  62.93572   \n",
       "62   32.590000   0.555067  0.964130  0.943401  1.250194  1.075298  80.23920   \n",
       "106  -1.450000   2.874097  0.951817  0.988299  1.498534  0.856108  73.27514   \n",
       "45   -0.377145   1.152911  0.992871  1.011334  1.349007  1.100703  79.49518   \n",
       "74   32.590000   1.633667  0.988566  1.172366  1.375602  1.139686  65.07668   \n",
       "115  -1.590000   8.275347  0.965897  1.145453  1.851697  1.909361  63.15934   \n",
       "135  16.910000   4.639950  0.997888  0.989731  1.525275  1.588065  65.13674   \n",
       "57   -0.377145   2.933722  1.002411  1.258244  1.904247  1.223992  63.20100   \n",
       "10    5.803333   1.356333  0.981347  1.097356  1.453888  1.189854  69.55504   \n",
       "\n",
       "      e1_2450    e2_915   e2_2450  \n",
       "38   60.16880  31.32553  14.67355  \n",
       "99   57.50490  26.10140  11.98970  \n",
       "116  60.73280   9.80234   6.57210  \n",
       "94   62.46380  20.97458  11.66050  \n",
       "91   65.91400  17.99186  12.14730  \n",
       "31   66.06675  21.86165  13.94035  \n",
       "8    70.48540   5.85626   8.63710  \n",
       "127  73.61110   6.34590  10.27230  \n",
       "19   59.78150   6.12536   4.88620  \n",
       "130  68.88360   6.02837   7.30225  \n",
       "9    70.43590   5.86250   8.53730  \n",
       "37   61.10470  28.76096  14.16600  \n",
       "62   76.33970   7.79848  15.68805  \n",
       "106  71.14590   8.57916  11.26380  \n",
       "45   77.05370   7.90874  13.07800  \n",
       "74   64.40500   5.38774   5.52770  \n",
       "115  62.27990   9.57198   6.92280  \n",
       "135  64.42805   6.28982   5.81900  \n",
       "57   62.58710   8.56347   5.99190  \n",
       "10   68.44280   5.71930   7.37740  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação entre variáveis dependentes e independentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = base.iloc[:, 0:10].values\n",
    "\n",
    "#Vamos prever e1_945\n",
    "y = base['e1_915'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(y, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_x = StandardScaler()\n",
    "X = scaler_x.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação entre dados de treinamento e dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X, y,\n",
    "                                                                  test_size = 0.3,\n",
    "                                                                  random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da arquitetura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim = 10, activation = 'softmax'))\n",
    "# Prever todas saídas\n",
    "model.add(Dense(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da taxa de aprendizado (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate =0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da métrica Root Mean Squared Error (RMSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar o backend do tensorflow (eu não sei exatamente porque precisei fazer isso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo do erro, otimizador e métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics= ['mae','mse',rmse])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo de todos hiperparâmetros e arquitetura do modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6)                 66        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 73\n",
      "Trainable params: 73\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 1.0314 - mae: 0.8823 - mse: 1.0314 - rmse: 0.8823 - val_loss: 0.9923 - val_mae: 0.8551 - val_mse: 0.9923 - val_rmse: 0.8551\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.9225 - mae: 0.8383 - mse: 0.9225 - rmse: 0.8383 - val_loss: 0.9217 - val_mae: 0.8239 - val_mse: 0.9217 - val_rmse: 0.8239\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.8611 - mae: 0.8067 - mse: 0.8611 - rmse: 0.8067 - val_loss: 0.8635 - val_mae: 0.7979 - val_mse: 0.8635 - val_rmse: 0.7979\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.8096 - mae: 0.7809 - mse: 0.8096 - rmse: 0.7809 - val_loss: 0.8111 - val_mae: 0.7727 - val_mse: 0.8111 - val_rmse: 0.7727\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 0s 101us/step - loss: 0.7613 - mae: 0.7555 - mse: 0.7613 - rmse: 0.7555 - val_loss: 0.7601 - val_mae: 0.7489 - val_mse: 0.7601 - val_rmse: 0.7489\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.7133 - mae: 0.7287 - mse: 0.7133 - rmse: 0.7287 - val_loss: 0.7154 - val_mae: 0.7257 - val_mse: 0.7154 - val_rmse: 0.7257\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 0s 93us/step - loss: 0.6746 - mae: 0.7053 - mse: 0.6746 - rmse: 0.7053 - val_loss: 0.6745 - val_mae: 0.7037 - val_mse: 0.6745 - val_rmse: 0.7037\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 0s 87us/step - loss: 0.6357 - mae: 0.6824 - mse: 0.6357 - rmse: 0.6824 - val_loss: 0.6329 - val_mae: 0.6812 - val_mse: 0.6329 - val_rmse: 0.6812\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 0s 102us/step - loss: 0.6019 - mae: 0.6626 - mse: 0.6019 - rmse: 0.6626 - val_loss: 0.5944 - val_mae: 0.6607 - val_mse: 0.5944 - val_rmse: 0.6607\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 0s 106us/step - loss: 0.5639 - mae: 0.6388 - mse: 0.5639 - rmse: 0.6388 - val_loss: 0.5578 - val_mae: 0.6401 - val_mse: 0.5578 - val_rmse: 0.6401\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.5317 - mae: 0.6184 - mse: 0.5317 - rmse: 0.6184 - val_loss: 0.5281 - val_mae: 0.6223 - val_mse: 0.5281 - val_rmse: 0.6223\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 0s 95us/step - loss: 0.5021 - mae: 0.5977 - mse: 0.5021 - rmse: 0.5977 - val_loss: 0.4936 - val_mae: 0.6031 - val_mse: 0.4936 - val_rmse: 0.6031\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.4753 - mae: 0.5801 - mse: 0.4753 - rmse: 0.5801 - val_loss: 0.4619 - val_mae: 0.5846 - val_mse: 0.4619 - val_rmse: 0.5846\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 0s 108us/step - loss: 0.4420 - mae: 0.5581 - mse: 0.4420 - rmse: 0.5581 - val_loss: 0.4348 - val_mae: 0.5665 - val_mse: 0.4348 - val_rmse: 0.5665\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 0s 100us/step - loss: 0.4122 - mae: 0.5377 - mse: 0.4122 - rmse: 0.5377 - val_loss: 0.4087 - val_mae: 0.5492 - val_mse: 0.4087 - val_rmse: 0.5492\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 0s 80us/step - loss: 0.3837 - mae: 0.5179 - mse: 0.3837 - rmse: 0.5179 - val_loss: 0.3822 - val_mae: 0.5314 - val_mse: 0.3822 - val_rmse: 0.5314\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 0s 99us/step - loss: 0.3592 - mae: 0.5020 - mse: 0.3592 - rmse: 0.5020 - val_loss: 0.3599 - val_mae: 0.5131 - val_mse: 0.3599 - val_rmse: 0.5131\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.3309 - mae: 0.4771 - mse: 0.3309 - rmse: 0.4771 - val_loss: 0.3327 - val_mae: 0.4921 - val_mse: 0.3327 - val_rmse: 0.4921\n",
      "Epoch 19/200\n",
      "89/89 [==============================] - 0s 92us/step - loss: 0.3046 - mae: 0.4565 - mse: 0.3046 - rmse: 0.4565 - val_loss: 0.3066 - val_mae: 0.4719 - val_mse: 0.3066 - val_rmse: 0.4719\n",
      "Epoch 20/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.2808 - mae: 0.4365 - mse: 0.2808 - rmse: 0.4365 - val_loss: 0.2835 - val_mae: 0.4534 - val_mse: 0.2835 - val_rmse: 0.4534\n",
      "Epoch 21/200\n",
      "89/89 [==============================] - 0s 93us/step - loss: 0.2572 - mae: 0.4142 - mse: 0.2572 - rmse: 0.4142 - val_loss: 0.2584 - val_mae: 0.4321 - val_mse: 0.2584 - val_rmse: 0.4321\n",
      "Epoch 22/200\n",
      "89/89 [==============================] - 0s 106us/step - loss: 0.2360 - mae: 0.3931 - mse: 0.2360 - rmse: 0.3931 - val_loss: 0.2364 - val_mae: 0.4123 - val_mse: 0.2364 - val_rmse: 0.4123\n",
      "Epoch 23/200\n",
      "89/89 [==============================] - 0s 91us/step - loss: 0.2150 - mae: 0.3745 - mse: 0.2150 - rmse: 0.3745 - val_loss: 0.2141 - val_mae: 0.3889 - val_mse: 0.2141 - val_rmse: 0.3889\n",
      "Epoch 24/200\n",
      "89/89 [==============================] - 0s 117us/step - loss: 0.1942 - mae: 0.3501 - mse: 0.1942 - rmse: 0.3501 - val_loss: 0.1916 - val_mae: 0.3660 - val_mse: 0.1916 - val_rmse: 0.3660\n",
      "Epoch 25/200\n",
      "89/89 [==============================] - 0s 98us/step - loss: 0.1795 - mae: 0.3346 - mse: 0.1795 - rmse: 0.3346 - val_loss: 0.1740 - val_mae: 0.3494 - val_mse: 0.1740 - val_rmse: 0.3494\n",
      "Epoch 26/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.1617 - mae: 0.3130 - mse: 0.1617 - rmse: 0.3130 - val_loss: 0.1546 - val_mae: 0.3279 - val_mse: 0.1546 - val_rmse: 0.3279\n",
      "Epoch 27/200\n",
      "89/89 [==============================] - 0s 104us/step - loss: 0.1446 - mae: 0.2912 - mse: 0.1446 - rmse: 0.2912 - val_loss: 0.1361 - val_mae: 0.3014 - val_mse: 0.1361 - val_rmse: 0.3014\n",
      "Epoch 28/200\n",
      "89/89 [==============================] - 0s 84us/step - loss: 0.1289 - mae: 0.2735 - mse: 0.1289 - rmse: 0.2735 - val_loss: 0.1196 - val_mae: 0.2781 - val_mse: 0.1196 - val_rmse: 0.2781\n",
      "Epoch 29/200\n",
      "89/89 [==============================] - 0s 111us/step - loss: 0.1162 - mae: 0.2592 - mse: 0.1162 - rmse: 0.2592 - val_loss: 0.1064 - val_mae: 0.2586 - val_mse: 0.1064 - val_rmse: 0.2586\n",
      "Epoch 30/200\n",
      "89/89 [==============================] - 0s 83us/step - loss: 0.1033 - mae: 0.2383 - mse: 0.1033 - rmse: 0.2383 - val_loss: 0.0937 - val_mae: 0.2376 - val_mse: 0.0937 - val_rmse: 0.2376\n",
      "Epoch 31/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0936 - mae: 0.2242 - mse: 0.0936 - rmse: 0.2242 - val_loss: 0.0835 - val_mae: 0.2248 - val_mse: 0.0835 - val_rmse: 0.2248\n",
      "Epoch 32/200\n",
      "89/89 [==============================] - 0s 125us/step - loss: 0.0858 - mae: 0.2142 - mse: 0.0858 - rmse: 0.2142 - val_loss: 0.0748 - val_mae: 0.2083 - val_mse: 0.0748 - val_rmse: 0.2083\n",
      "Epoch 33/200\n",
      "89/89 [==============================] - 0s 99us/step - loss: 0.0768 - mae: 0.2015 - mse: 0.0768 - rmse: 0.2015 - val_loss: 0.0671 - val_mae: 0.1904 - val_mse: 0.0671 - val_rmse: 0.1904\n",
      "Epoch 34/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0691 - mae: 0.1881 - mse: 0.0691 - rmse: 0.1881 - val_loss: 0.0606 - val_mae: 0.1853 - val_mse: 0.0606 - val_rmse: 0.1853\n",
      "Epoch 35/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0638 - mae: 0.1827 - mse: 0.0638 - rmse: 0.1827 - val_loss: 0.0543 - val_mae: 0.1756 - val_mse: 0.0543 - val_rmse: 0.1756\n",
      "Epoch 36/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0590 - mae: 0.1754 - mse: 0.0590 - rmse: 0.1754 - val_loss: 0.0516 - val_mae: 0.1707 - val_mse: 0.0516 - val_rmse: 0.1707\n",
      "Epoch 37/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0549 - mae: 0.1729 - mse: 0.0549 - rmse: 0.1729 - val_loss: 0.0477 - val_mae: 0.1652 - val_mse: 0.0477 - val_rmse: 0.1652\n",
      "Epoch 38/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0509 - mae: 0.1652 - mse: 0.0509 - rmse: 0.1652 - val_loss: 0.0461 - val_mae: 0.1699 - val_mse: 0.0461 - val_rmse: 0.1699\n",
      "Epoch 39/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0476 - mae: 0.1645 - mse: 0.0476 - rmse: 0.1645 - val_loss: 0.0425 - val_mae: 0.1614 - val_mse: 0.0425 - val_rmse: 0.1614\n",
      "Epoch 40/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0457 - mae: 0.1595 - mse: 0.0457 - rmse: 0.1595 - val_loss: 0.0410 - val_mae: 0.1584 - val_mse: 0.0410 - val_rmse: 0.1584\n",
      "Epoch 41/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0441 - mae: 0.1620 - mse: 0.0441 - rmse: 0.1620 - val_loss: 0.0403 - val_mae: 0.1579 - val_mse: 0.0403 - val_rmse: 0.1579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0416 - mae: 0.1566 - mse: 0.0416 - rmse: 0.1566 - val_loss: 0.0388 - val_mae: 0.1609 - val_mse: 0.0388 - val_rmse: 0.1609\n",
      "Epoch 43/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0402 - mae: 0.1540 - mse: 0.0402 - rmse: 0.1540 - val_loss: 0.0366 - val_mae: 0.1575 - val_mse: 0.0366 - val_rmse: 0.1575\n",
      "Epoch 44/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0392 - mae: 0.1541 - mse: 0.0392 - rmse: 0.1541 - val_loss: 0.0351 - val_mae: 0.1558 - val_mse: 0.0351 - val_rmse: 0.1558\n",
      "Epoch 45/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0359 - mae: 0.1444 - mse: 0.0359 - rmse: 0.1444 - val_loss: 0.0341 - val_mae: 0.1564 - val_mse: 0.0341 - val_rmse: 0.1564\n",
      "Epoch 46/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0347 - mae: 0.1430 - mse: 0.0347 - rmse: 0.1430 - val_loss: 0.0363 - val_mae: 0.1586 - val_mse: 0.0363 - val_rmse: 0.1586\n",
      "Epoch 47/200\n",
      "89/89 [==============================] - 0s 55us/step - loss: 0.0348 - mae: 0.1442 - mse: 0.0348 - rmse: 0.1442 - val_loss: 0.0339 - val_mae: 0.1538 - val_mse: 0.0339 - val_rmse: 0.1538\n",
      "Epoch 48/200\n",
      "89/89 [==============================] - 0s 83us/step - loss: 0.0329 - mae: 0.1381 - mse: 0.0329 - rmse: 0.1381 - val_loss: 0.0321 - val_mae: 0.1441 - val_mse: 0.0321 - val_rmse: 0.1441\n",
      "Epoch 49/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0321 - mae: 0.1387 - mse: 0.0321 - rmse: 0.1387 - val_loss: 0.0353 - val_mae: 0.1598 - val_mse: 0.0353 - val_rmse: 0.1598\n",
      "Epoch 50/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0303 - mae: 0.1324 - mse: 0.0303 - rmse: 0.1324 - val_loss: 0.0294 - val_mae: 0.1413 - val_mse: 0.0294 - val_rmse: 0.1413\n",
      "Epoch 51/200\n",
      "89/89 [==============================] - 0s 56us/step - loss: 0.0288 - mae: 0.1315 - mse: 0.0288 - rmse: 0.1315 - val_loss: 0.0294 - val_mae: 0.1403 - val_mse: 0.0294 - val_rmse: 0.1403\n",
      "Epoch 52/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0293 - mae: 0.1362 - mse: 0.0293 - rmse: 0.1362 - val_loss: 0.0323 - val_mae: 0.1514 - val_mse: 0.0323 - val_rmse: 0.1514\n",
      "Epoch 53/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0290 - mae: 0.1337 - mse: 0.0290 - rmse: 0.1337 - val_loss: 0.0310 - val_mae: 0.1474 - val_mse: 0.0310 - val_rmse: 0.1474\n",
      "Epoch 54/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0269 - mae: 0.1248 - mse: 0.0269 - rmse: 0.1248 - val_loss: 0.0261 - val_mae: 0.1353 - val_mse: 0.0261 - val_rmse: 0.1353\n",
      "Epoch 55/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0266 - mae: 0.1235 - mse: 0.0266 - rmse: 0.1235 - val_loss: 0.0278 - val_mae: 0.1393 - val_mse: 0.0278 - val_rmse: 0.1393\n",
      "Epoch 56/200\n",
      "89/89 [==============================] - 0s 58us/step - loss: 0.0252 - mae: 0.1232 - mse: 0.0252 - rmse: 0.1232 - val_loss: 0.0262 - val_mae: 0.1359 - val_mse: 0.0262 - val_rmse: 0.1359\n",
      "Epoch 57/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0274 - mae: 0.1287 - mse: 0.0274 - rmse: 0.1287 - val_loss: 0.0315 - val_mae: 0.1476 - val_mse: 0.0315 - val_rmse: 0.1476\n",
      "Epoch 58/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0252 - mae: 0.1229 - mse: 0.0252 - rmse: 0.1229 - val_loss: 0.0236 - val_mae: 0.1253 - val_mse: 0.0236 - val_rmse: 0.1253\n",
      "Epoch 59/200\n",
      "89/89 [==============================] - 0s 93us/step - loss: 0.0223 - mae: 0.1141 - mse: 0.0223 - rmse: 0.1141 - val_loss: 0.0254 - val_mae: 0.1298 - val_mse: 0.0254 - val_rmse: 0.1298\n",
      "Epoch 60/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0228 - mae: 0.1164 - mse: 0.0228 - rmse: 0.1164 - val_loss: 0.0261 - val_mae: 0.1299 - val_mse: 0.0261 - val_rmse: 0.1299\n",
      "Epoch 61/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.0220 - mae: 0.1144 - mse: 0.0220 - rmse: 0.1144 - val_loss: 0.0239 - val_mae: 0.1230 - val_mse: 0.0239 - val_rmse: 0.1230\n",
      "Epoch 62/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0209 - mae: 0.1093 - mse: 0.0209 - rmse: 0.1093 - val_loss: 0.0241 - val_mae: 0.1314 - val_mse: 0.0241 - val_rmse: 0.1314\n",
      "Epoch 63/200\n",
      "89/89 [==============================] - 0s 84us/step - loss: 0.0219 - mae: 0.1178 - mse: 0.0219 - rmse: 0.1178 - val_loss: 0.0246 - val_mae: 0.1280 - val_mse: 0.0246 - val_rmse: 0.1280\n",
      "Epoch 64/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0221 - mae: 0.1191 - mse: 0.0221 - rmse: 0.1191 - val_loss: 0.0233 - val_mae: 0.1204 - val_mse: 0.0233 - val_rmse: 0.1204\n",
      "Epoch 65/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0203 - mae: 0.1120 - mse: 0.0203 - rmse: 0.1120 - val_loss: 0.0235 - val_mae: 0.1267 - val_mse: 0.0235 - val_rmse: 0.1267\n",
      "Epoch 66/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0197 - mae: 0.1102 - mse: 0.0197 - rmse: 0.1102 - val_loss: 0.0219 - val_mae: 0.1220 - val_mse: 0.0219 - val_rmse: 0.1220\n",
      "Epoch 67/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.0190 - mae: 0.1104 - mse: 0.0190 - rmse: 0.1104 - val_loss: 0.0284 - val_mae: 0.1400 - val_mse: 0.0284 - val_rmse: 0.1400\n",
      "Epoch 68/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0191 - mae: 0.1051 - mse: 0.0191 - rmse: 0.1051 - val_loss: 0.0252 - val_mae: 0.1320 - val_mse: 0.0252 - val_rmse: 0.1320\n",
      "Epoch 69/200\n",
      "89/89 [==============================] - 0s 89us/step - loss: 0.0178 - mae: 0.1065 - mse: 0.0178 - rmse: 0.1065 - val_loss: 0.0235 - val_mae: 0.1263 - val_mse: 0.0235 - val_rmse: 0.1263\n",
      "Epoch 70/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0180 - mae: 0.1046 - mse: 0.0180 - rmse: 0.1046 - val_loss: 0.0223 - val_mae: 0.1208 - val_mse: 0.0223 - val_rmse: 0.1208\n",
      "Epoch 71/200\n",
      "89/89 [==============================] - 0s 117us/step - loss: 0.0175 - mae: 0.1005 - mse: 0.0175 - rmse: 0.1005 - val_loss: 0.0262 - val_mae: 0.1340 - val_mse: 0.0262 - val_rmse: 0.1340\n",
      "Epoch 72/200\n",
      "89/89 [==============================] - 0s 80us/step - loss: 0.0173 - mae: 0.1049 - mse: 0.0173 - rmse: 0.1049 - val_loss: 0.0220 - val_mae: 0.1212 - val_mse: 0.0220 - val_rmse: 0.1212\n",
      "Epoch 73/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0160 - mae: 0.0996 - mse: 0.0160 - rmse: 0.0996 - val_loss: 0.0224 - val_mae: 0.1200 - val_mse: 0.0224 - val_rmse: 0.1200\n",
      "Epoch 74/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0152 - mae: 0.0947 - mse: 0.0152 - rmse: 0.0947 - val_loss: 0.0233 - val_mae: 0.1274 - val_mse: 0.0233 - val_rmse: 0.1274\n",
      "Epoch 75/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0160 - mae: 0.1000 - mse: 0.0160 - rmse: 0.1000 - val_loss: 0.0195 - val_mae: 0.1124 - val_mse: 0.0195 - val_rmse: 0.1124\n",
      "Epoch 76/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0166 - mae: 0.0998 - mse: 0.0166 - rmse: 0.0998 - val_loss: 0.0225 - val_mae: 0.1264 - val_mse: 0.0225 - val_rmse: 0.1264\n",
      "Epoch 77/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0146 - mae: 0.0966 - mse: 0.0146 - rmse: 0.0966 - val_loss: 0.0212 - val_mae: 0.1230 - val_mse: 0.0212 - val_rmse: 0.1230\n",
      "Epoch 78/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0146 - mae: 0.0939 - mse: 0.0146 - rmse: 0.0939 - val_loss: 0.0173 - val_mae: 0.1085 - val_mse: 0.0173 - val_rmse: 0.1085\n",
      "Epoch 79/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0153 - mae: 0.0959 - mse: 0.0153 - rmse: 0.0959 - val_loss: 0.0245 - val_mae: 0.1311 - val_mse: 0.0245 - val_rmse: 0.1311\n",
      "Epoch 80/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0162 - mae: 0.1025 - mse: 0.0162 - rmse: 0.1025 - val_loss: 0.0235 - val_mae: 0.1285 - val_mse: 0.0235 - val_rmse: 0.1285\n",
      "Epoch 81/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0132 - mae: 0.0905 - mse: 0.0132 - rmse: 0.0905 - val_loss: 0.0190 - val_mae: 0.1158 - val_mse: 0.0190 - val_rmse: 0.1158\n",
      "Epoch 82/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0140 - mae: 0.0924 - mse: 0.0140 - rmse: 0.0924 - val_loss: 0.0165 - val_mae: 0.1076 - val_mse: 0.0165 - val_rmse: 0.1076\n",
      "Epoch 83/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0133 - mae: 0.0933 - mse: 0.0133 - rmse: 0.0933 - val_loss: 0.0175 - val_mae: 0.1115 - val_mse: 0.0175 - val_rmse: 0.1115\n",
      "Epoch 84/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0140 - mae: 0.0935 - mse: 0.0140 - rmse: 0.0935 - val_loss: 0.0184 - val_mae: 0.1111 - val_mse: 0.0184 - val_rmse: 0.1111\n",
      "Epoch 85/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0132 - mae: 0.0919 - mse: 0.0132 - rmse: 0.0919 - val_loss: 0.0202 - val_mae: 0.1161 - val_mse: 0.0202 - val_rmse: 0.1161\n",
      "Epoch 86/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0121 - mae: 0.0864 - mse: 0.0121 - rmse: 0.0864 - val_loss: 0.0157 - val_mae: 0.1021 - val_mse: 0.0157 - val_rmse: 0.1021\n",
      "Epoch 87/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0121 - mae: 0.0885 - mse: 0.0121 - rmse: 0.0885 - val_loss: 0.0204 - val_mae: 0.1145 - val_mse: 0.0204 - val_rmse: 0.1145\n",
      "Epoch 88/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0120 - mae: 0.0876 - mse: 0.0120 - rmse: 0.0876 - val_loss: 0.0167 - val_mae: 0.1058 - val_mse: 0.0167 - val_rmse: 0.1058\n",
      "Epoch 89/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0122 - mae: 0.0885 - mse: 0.0122 - rmse: 0.0885 - val_loss: 0.0192 - val_mae: 0.1138 - val_mse: 0.0192 - val_rmse: 0.1138\n",
      "Epoch 90/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0154 - mae: 0.1049 - mse: 0.0154 - rmse: 0.1049 - val_loss: 0.0182 - val_mae: 0.1091 - val_mse: 0.0182 - val_rmse: 0.1091\n",
      "Epoch 91/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0121 - mae: 0.0880 - mse: 0.0121 - rmse: 0.0880 - val_loss: 0.0156 - val_mae: 0.0985 - val_mse: 0.0156 - val_rmse: 0.0985\n",
      "Epoch 92/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0120 - mae: 0.0868 - mse: 0.0120 - rmse: 0.0868 - val_loss: 0.0159 - val_mae: 0.1009 - val_mse: 0.0159 - val_rmse: 0.1009\n",
      "Epoch 93/200\n",
      "89/89 [==============================] - 0s 79us/step - loss: 0.0111 - mae: 0.0840 - mse: 0.0111 - rmse: 0.0840 - val_loss: 0.0157 - val_mae: 0.1006 - val_mse: 0.0157 - val_rmse: 0.1006\n",
      "Epoch 94/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0112 - mae: 0.0860 - mse: 0.0112 - rmse: 0.0860 - val_loss: 0.0196 - val_mae: 0.1140 - val_mse: 0.0196 - val_rmse: 0.1140\n",
      "Epoch 95/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0108 - mae: 0.0848 - mse: 0.0108 - rmse: 0.0848 - val_loss: 0.0208 - val_mae: 0.1206 - val_mse: 0.0208 - val_rmse: 0.1206\n",
      "Epoch 96/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0114 - mae: 0.0834 - mse: 0.0114 - rmse: 0.0834 - val_loss: 0.0188 - val_mae: 0.1140 - val_mse: 0.0188 - val_rmse: 0.1140\n",
      "Epoch 97/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0115 - mae: 0.0857 - mse: 0.0115 - rmse: 0.0857 - val_loss: 0.0179 - val_mae: 0.1172 - val_mse: 0.0179 - val_rmse: 0.1172\n",
      "Epoch 98/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0108 - mae: 0.0875 - mse: 0.0108 - rmse: 0.0875 - val_loss: 0.0190 - val_mae: 0.1116 - val_mse: 0.0190 - val_rmse: 0.1116\n",
      "Epoch 99/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0116 - mae: 0.0841 - mse: 0.0116 - rmse: 0.0841 - val_loss: 0.0159 - val_mae: 0.1032 - val_mse: 0.0159 - val_rmse: 0.1032\n",
      "Epoch 100/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0106 - mae: 0.0845 - mse: 0.0106 - rmse: 0.0845 - val_loss: 0.0155 - val_mae: 0.1051 - val_mse: 0.0155 - val_rmse: 0.1051\n",
      "Epoch 101/200\n",
      "89/89 [==============================] - 0s 57us/step - loss: 0.0101 - mae: 0.0825 - mse: 0.0101 - rmse: 0.0825 - val_loss: 0.0200 - val_mae: 0.1161 - val_mse: 0.0200 - val_rmse: 0.1161\n",
      "Epoch 102/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0114 - mae: 0.0881 - mse: 0.0114 - rmse: 0.0881 - val_loss: 0.0154 - val_mae: 0.0950 - val_mse: 0.0154 - val_rmse: 0.0950\n",
      "Epoch 103/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0117 - mae: 0.0887 - mse: 0.0117 - rmse: 0.0887 - val_loss: 0.0148 - val_mae: 0.1009 - val_mse: 0.0148 - val_rmse: 0.1009\n",
      "Epoch 104/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0099 - mae: 0.0792 - mse: 0.0099 - rmse: 0.0792 - val_loss: 0.0168 - val_mae: 0.1090 - val_mse: 0.0168 - val_rmse: 0.1090\n",
      "Epoch 105/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0102 - mae: 0.0818 - mse: 0.0102 - rmse: 0.0818 - val_loss: 0.0177 - val_mae: 0.0996 - val_mse: 0.0177 - val_rmse: 0.0996\n",
      "Epoch 106/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.0114 - mae: 0.0879 - mse: 0.0114 - rmse: 0.0879 - val_loss: 0.0184 - val_mae: 0.1098 - val_mse: 0.0184 - val_rmse: 0.1098\n",
      "Epoch 107/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0091 - mae: 0.0745 - mse: 0.0091 - rmse: 0.0745 - val_loss: 0.0168 - val_mae: 0.1048 - val_mse: 0.0168 - val_rmse: 0.1048\n",
      "Epoch 108/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0089 - mae: 0.0766 - mse: 0.0089 - rmse: 0.0766 - val_loss: 0.0186 - val_mae: 0.1110 - val_mse: 0.0186 - val_rmse: 0.1110\n",
      "Epoch 109/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0111 - mae: 0.0821 - mse: 0.0111 - rmse: 0.0821 - val_loss: 0.0174 - val_mae: 0.1106 - val_mse: 0.0174 - val_rmse: 0.1106\n",
      "Epoch 110/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0083 - mae: 0.0735 - mse: 0.0083 - rmse: 0.0735 - val_loss: 0.0147 - val_mae: 0.0978 - val_mse: 0.0147 - val_rmse: 0.0978\n",
      "Epoch 111/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0097 - mae: 0.0796 - mse: 0.0097 - rmse: 0.0796 - val_loss: 0.0198 - val_mae: 0.1155 - val_mse: 0.0198 - val_rmse: 0.1155\n",
      "Epoch 112/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0116 - mae: 0.0858 - mse: 0.0116 - rmse: 0.0858 - val_loss: 0.0158 - val_mae: 0.1034 - val_mse: 0.0158 - val_rmse: 0.1034\n",
      "Epoch 113/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0092 - mae: 0.0789 - mse: 0.0092 - rmse: 0.0789 - val_loss: 0.0162 - val_mae: 0.1013 - val_mse: 0.0162 - val_rmse: 0.1013\n",
      "Epoch 114/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0097 - mae: 0.0802 - mse: 0.0097 - rmse: 0.0802 - val_loss: 0.0144 - val_mae: 0.1010 - val_mse: 0.0144 - val_rmse: 0.1010\n",
      "Epoch 115/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0089 - mae: 0.0764 - mse: 0.0089 - rmse: 0.0764 - val_loss: 0.0131 - val_mae: 0.0908 - val_mse: 0.0131 - val_rmse: 0.0908\n",
      "Epoch 116/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0081 - mae: 0.0744 - mse: 0.0081 - rmse: 0.0744 - val_loss: 0.0218 - val_mae: 0.1210 - val_mse: 0.0218 - val_rmse: 0.1210\n",
      "Epoch 117/200\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.0118 - mae: 0.0854 - mse: 0.0118 - rmse: 0.08 - 0s 69us/step - loss: 0.0119 - mae: 0.0889 - mse: 0.0119 - rmse: 0.0889 - val_loss: 0.0144 - val_mae: 0.0922 - val_mse: 0.0144 - val_rmse: 0.0922\n",
      "Epoch 118/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0078 - mae: 0.0723 - mse: 0.0078 - rmse: 0.0723 - val_loss: 0.0143 - val_mae: 0.0978 - val_mse: 0.0143 - val_rmse: 0.0978\n",
      "Epoch 119/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0074 - mae: 0.0698 - mse: 0.0074 - rmse: 0.0698 - val_loss: 0.0168 - val_mae: 0.1054 - val_mse: 0.0168 - val_rmse: 0.1054\n",
      "Epoch 120/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0108 - mae: 0.0861 - mse: 0.0108 - rmse: 0.0861 - val_loss: 0.0243 - val_mae: 0.1150 - val_mse: 0.0243 - val_rmse: 0.1150\n",
      "Epoch 121/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0093 - mae: 0.0818 - mse: 0.0093 - rmse: 0.0818 - val_loss: 0.0144 - val_mae: 0.0931 - val_mse: 0.0144 - val_rmse: 0.0931\n",
      "Epoch 122/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0079 - mae: 0.0720 - mse: 0.0079 - rmse: 0.0720 - val_loss: 0.0137 - val_mae: 0.0921 - val_mse: 0.0137 - val_rmse: 0.0921\n",
      "Epoch 123/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0093 - mae: 0.0809 - mse: 0.0093 - rmse: 0.0809 - val_loss: 0.0142 - val_mae: 0.0888 - val_mse: 0.0142 - val_rmse: 0.0888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0079 - mae: 0.0715 - mse: 0.0079 - rmse: 0.0715 - val_loss: 0.0136 - val_mae: 0.0879 - val_mse: 0.0136 - val_rmse: 0.0879\n",
      "Epoch 125/200\n",
      "89/89 [==============================] - 0s 92us/step - loss: 0.0102 - mae: 0.0839 - mse: 0.0102 - rmse: 0.0839 - val_loss: 0.0175 - val_mae: 0.1061 - val_mse: 0.0175 - val_rmse: 0.1061\n",
      "Epoch 126/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0081 - mae: 0.0702 - mse: 0.0081 - rmse: 0.0702 - val_loss: 0.0133 - val_mae: 0.0904 - val_mse: 0.0133 - val_rmse: 0.0904\n",
      "Epoch 127/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0071 - mae: 0.0696 - mse: 0.0071 - rmse: 0.0696 - val_loss: 0.0146 - val_mae: 0.0949 - val_mse: 0.0146 - val_rmse: 0.0949\n",
      "Epoch 128/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0086 - mae: 0.0764 - mse: 0.0086 - rmse: 0.0764 - val_loss: 0.0209 - val_mae: 0.1141 - val_mse: 0.0209 - val_rmse: 0.1141\n",
      "Epoch 129/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0092 - mae: 0.0781 - mse: 0.0092 - rmse: 0.0781 - val_loss: 0.0140 - val_mae: 0.0918 - val_mse: 0.0140 - val_rmse: 0.0918\n",
      "Epoch 130/200\n",
      "89/89 [==============================] - 0s 74us/step - loss: 0.0071 - mae: 0.0684 - mse: 0.0071 - rmse: 0.0684 - val_loss: 0.0136 - val_mae: 0.0911 - val_mse: 0.0136 - val_rmse: 0.0911\n",
      "Epoch 131/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0089 - mae: 0.0762 - mse: 0.0089 - rmse: 0.0762 - val_loss: 0.0169 - val_mae: 0.1022 - val_mse: 0.0169 - val_rmse: 0.1022\n",
      "Epoch 132/200\n",
      "89/89 [==============================] - 0s 71us/step - loss: 0.0070 - mae: 0.0683 - mse: 0.0070 - rmse: 0.0683 - val_loss: 0.0180 - val_mae: 0.1036 - val_mse: 0.0180 - val_rmse: 0.1036\n",
      "Epoch 133/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0091 - mae: 0.0747 - mse: 0.0091 - rmse: 0.0747 - val_loss: 0.0164 - val_mae: 0.1026 - val_mse: 0.0164 - val_rmse: 0.1026\n",
      "Epoch 134/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0081 - mae: 0.0745 - mse: 0.0081 - rmse: 0.0745 - val_loss: 0.0128 - val_mae: 0.0848 - val_mse: 0.0128 - val_rmse: 0.0848\n",
      "Epoch 135/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0099 - mae: 0.0801 - mse: 0.0099 - rmse: 0.0801 - val_loss: 0.0141 - val_mae: 0.0906 - val_mse: 0.0141 - val_rmse: 0.0906\n",
      "Epoch 136/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0077 - mae: 0.0726 - mse: 0.0077 - rmse: 0.0726 - val_loss: 0.0151 - val_mae: 0.0949 - val_mse: 0.0151 - val_rmse: 0.0949\n",
      "Epoch 137/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0079 - mae: 0.0749 - mse: 0.0079 - rmse: 0.0749 - val_loss: 0.0139 - val_mae: 0.0912 - val_mse: 0.0139 - val_rmse: 0.0912\n",
      "Epoch 138/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0063 - mae: 0.0656 - mse: 0.0063 - rmse: 0.0656 - val_loss: 0.0147 - val_mae: 0.0962 - val_mse: 0.0147 - val_rmse: 0.0962\n",
      "Epoch 139/200\n",
      "89/89 [==============================] - 0s 70us/step - loss: 0.0071 - mae: 0.0682 - mse: 0.0071 - rmse: 0.0682 - val_loss: 0.0165 - val_mae: 0.1005 - val_mse: 0.0165 - val_rmse: 0.1005\n",
      "Epoch 140/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0087 - mae: 0.0732 - mse: 0.0087 - rmse: 0.0732 - val_loss: 0.0138 - val_mae: 0.0886 - val_mse: 0.0138 - val_rmse: 0.0886\n",
      "Epoch 141/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0074 - mae: 0.0710 - mse: 0.0074 - rmse: 0.0710 - val_loss: 0.0123 - val_mae: 0.0836 - val_mse: 0.0123 - val_rmse: 0.0836\n",
      "Epoch 142/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0071 - mae: 0.0711 - mse: 0.0071 - rmse: 0.0711 - val_loss: 0.0133 - val_mae: 0.0891 - val_mse: 0.0133 - val_rmse: 0.0891\n",
      "Epoch 143/200\n",
      "89/89 [==============================] - 0s 69us/step - loss: 0.0086 - mae: 0.0754 - mse: 0.0086 - rmse: 0.0754 - val_loss: 0.0165 - val_mae: 0.0961 - val_mse: 0.0165 - val_rmse: 0.0961\n",
      "Epoch 144/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0067 - mae: 0.0665 - mse: 0.0067 - rmse: 0.0665 - val_loss: 0.0120 - val_mae: 0.0827 - val_mse: 0.0120 - val_rmse: 0.0827\n",
      "Epoch 145/200\n",
      "89/89 [==============================] - 0s 57us/step - loss: 0.0055 - mae: 0.0610 - mse: 0.0055 - rmse: 0.0610 - val_loss: 0.0120 - val_mae: 0.0834 - val_mse: 0.0120 - val_rmse: 0.0834\n",
      "Epoch 146/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0069 - mae: 0.0651 - mse: 0.0069 - rmse: 0.0651 - val_loss: 0.0138 - val_mae: 0.0920 - val_mse: 0.0138 - val_rmse: 0.0920\n",
      "Epoch 147/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0074 - mae: 0.0706 - mse: 0.0074 - rmse: 0.0706 - val_loss: 0.0155 - val_mae: 0.0950 - val_mse: 0.0155 - val_rmse: 0.0950\n",
      "Epoch 148/200\n",
      "89/89 [==============================] - 0s 73us/step - loss: 0.0066 - mae: 0.0663 - mse: 0.0066 - rmse: 0.0663 - val_loss: 0.0123 - val_mae: 0.0849 - val_mse: 0.0123 - val_rmse: 0.0849\n",
      "Epoch 149/200\n",
      "89/89 [==============================] - 0s 59us/step - loss: 0.0066 - mae: 0.0649 - mse: 0.0066 - rmse: 0.0649 - val_loss: 0.0152 - val_mae: 0.0995 - val_mse: 0.0152 - val_rmse: 0.0995\n",
      "Epoch 150/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0072 - mae: 0.0689 - mse: 0.0072 - rmse: 0.0689 - val_loss: 0.0171 - val_mae: 0.1022 - val_mse: 0.0171 - val_rmse: 0.1022\n",
      "Epoch 151/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0072 - mae: 0.0669 - mse: 0.0072 - rmse: 0.0669 - val_loss: 0.0127 - val_mae: 0.0875 - val_mse: 0.0127 - val_rmse: 0.0875\n",
      "Epoch 152/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0074 - mae: 0.0714 - mse: 0.0074 - rmse: 0.0714 - val_loss: 0.0120 - val_mae: 0.0859 - val_mse: 0.0120 - val_rmse: 0.0859\n",
      "Epoch 153/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0071 - mae: 0.0681 - mse: 0.0071 - rmse: 0.0681 - val_loss: 0.0133 - val_mae: 0.0889 - val_mse: 0.0133 - val_rmse: 0.0889\n",
      "Epoch 154/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0070 - mae: 0.0706 - mse: 0.0070 - rmse: 0.0706 - val_loss: 0.0126 - val_mae: 0.0874 - val_mse: 0.0126 - val_rmse: 0.0874\n",
      "Epoch 155/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0062 - mae: 0.0636 - mse: 0.0062 - rmse: 0.0636 - val_loss: 0.0148 - val_mae: 0.0949 - val_mse: 0.0148 - val_rmse: 0.0949\n",
      "Epoch 156/200\n",
      "89/89 [==============================] - 0s 66us/step - loss: 0.0067 - mae: 0.0660 - mse: 0.0067 - rmse: 0.0660 - val_loss: 0.0142 - val_mae: 0.0918 - val_mse: 0.0142 - val_rmse: 0.0918\n",
      "Epoch 157/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0073 - mae: 0.0692 - mse: 0.0073 - rmse: 0.0692 - val_loss: 0.0128 - val_mae: 0.0867 - val_mse: 0.0128 - val_rmse: 0.0867\n",
      "Epoch 158/200\n",
      "89/89 [==============================] - 0s 78us/step - loss: 0.0063 - mae: 0.0615 - mse: 0.0063 - rmse: 0.0615 - val_loss: 0.0132 - val_mae: 0.0890 - val_mse: 0.0132 - val_rmse: 0.0890\n",
      "Epoch 159/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0063 - mae: 0.0662 - mse: 0.0063 - rmse: 0.0662 - val_loss: 0.0115 - val_mae: 0.0832 - val_mse: 0.0115 - val_rmse: 0.0832\n",
      "Epoch 160/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0065 - mae: 0.0657 - mse: 0.0065 - rmse: 0.0657 - val_loss: 0.0150 - val_mae: 0.0916 - val_mse: 0.0150 - val_rmse: 0.0916\n",
      "Epoch 161/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0069 - mae: 0.0671 - mse: 0.0069 - rmse: 0.0671 - val_loss: 0.0129 - val_mae: 0.0881 - val_mse: 0.0129 - val_rmse: 0.0881\n",
      "Epoch 162/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0072 - mae: 0.0691 - mse: 0.0072 - rmse: 0.0691 - val_loss: 0.0118 - val_mae: 0.0824 - val_mse: 0.0118 - val_rmse: 0.0824\n",
      "Epoch 163/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0067 - mae: 0.0671 - mse: 0.0067 - rmse: 0.0671 - val_loss: 0.0212 - val_mae: 0.1164 - val_mse: 0.0212 - val_rmse: 0.1164\n",
      "Epoch 164/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0094 - mae: 0.0756 - mse: 0.0094 - rmse: 0.0756 - val_loss: 0.0123 - val_mae: 0.0872 - val_mse: 0.0123 - val_rmse: 0.0872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0050 - mae: 0.0569 - mse: 0.0050 - rmse: 0.0569 - val_loss: 0.0127 - val_mae: 0.0862 - val_mse: 0.0127 - val_rmse: 0.0862\n",
      "Epoch 166/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0062 - mae: 0.0647 - mse: 0.0062 - rmse: 0.0647 - val_loss: 0.0217 - val_mae: 0.1168 - val_mse: 0.0217 - val_rmse: 0.1168\n",
      "Epoch 167/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0077 - mae: 0.0688 - mse: 0.0077 - rmse: 0.0688 - val_loss: 0.0114 - val_mae: 0.0828 - val_mse: 0.0114 - val_rmse: 0.0828\n",
      "Epoch 168/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0054 - mae: 0.0589 - mse: 0.0054 - rmse: 0.0589 - val_loss: 0.0115 - val_mae: 0.0807 - val_mse: 0.0115 - val_rmse: 0.0807\n",
      "Epoch 169/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0050 - mae: 0.0591 - mse: 0.0050 - rmse: 0.0591 - val_loss: 0.0173 - val_mae: 0.1012 - val_mse: 0.0173 - val_rmse: 0.1012\n",
      "Epoch 170/200\n",
      "89/89 [==============================] - 0s 60us/step - loss: 0.0054 - mae: 0.0598 - mse: 0.0054 - rmse: 0.0598 - val_loss: 0.0113 - val_mae: 0.0813 - val_mse: 0.0113 - val_rmse: 0.0813\n",
      "Epoch 171/200\n",
      "89/89 [==============================] - 0s 64us/step - loss: 0.0066 - mae: 0.0671 - mse: 0.0066 - rmse: 0.0671 - val_loss: 0.0118 - val_mae: 0.0834 - val_mse: 0.0118 - val_rmse: 0.0834\n",
      "Epoch 172/200\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.0057 - mae: 0.0611 - mse: 0.0057 - rmse: 0.0611 - val_loss: 0.0153 - val_mae: 0.0997 - val_mse: 0.0153 - val_rmse: 0.0997\n",
      "Epoch 173/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0056 - mae: 0.0597 - mse: 0.0056 - rmse: 0.0597 - val_loss: 0.0134 - val_mae: 0.0894 - val_mse: 0.0134 - val_rmse: 0.0894\n",
      "Epoch 174/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0082 - mae: 0.0741 - mse: 0.0082 - rmse: 0.0741 - val_loss: 0.0131 - val_mae: 0.0848 - val_mse: 0.0131 - val_rmse: 0.0848\n",
      "Epoch 175/200\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.0055 - mae: 0.0593 - mse: 0.0055 - rmse: 0.0593 - val_loss: 0.0133 - val_mae: 0.0923 - val_mse: 0.0133 - val_rmse: 0.0923\n",
      "Epoch 176/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0056 - mae: 0.0608 - mse: 0.0056 - rmse: 0.0608 - val_loss: 0.0131 - val_mae: 0.0835 - val_mse: 0.0131 - val_rmse: 0.0835\n",
      "Epoch 177/200\n",
      "89/89 [==============================] - 0s 62us/step - loss: 0.0075 - mae: 0.0703 - mse: 0.0075 - rmse: 0.0703 - val_loss: 0.0136 - val_mae: 0.0942 - val_mse: 0.0136 - val_rmse: 0.0942\n",
      "Epoch 178/200\n",
      "89/89 [==============================] - 0s 91us/step - loss: 0.0062 - mae: 0.0629 - mse: 0.0062 - rmse: 0.0629 - val_loss: 0.0133 - val_mae: 0.0887 - val_mse: 0.0133 - val_rmse: 0.0887\n",
      "Epoch 179/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0049 - mae: 0.0568 - mse: 0.0049 - rmse: 0.0568 - val_loss: 0.0130 - val_mae: 0.0893 - val_mse: 0.0130 - val_rmse: 0.0893\n",
      "Epoch 180/200\n",
      "89/89 [==============================] - 0s 84us/step - loss: 0.0053 - mae: 0.0587 - mse: 0.0053 - rmse: 0.0587 - val_loss: 0.0128 - val_mae: 0.0866 - val_mse: 0.0128 - val_rmse: 0.0866\n",
      "Epoch 181/200\n",
      "89/89 [==============================] - 0s 76us/step - loss: 0.0055 - mae: 0.0604 - mse: 0.0055 - rmse: 0.0604 - val_loss: 0.0147 - val_mae: 0.0969 - val_mse: 0.0147 - val_rmse: 0.0969\n",
      "Epoch 182/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.0054 - mae: 0.0612 - mse: 0.0054 - rmse: 0.0612 - val_loss: 0.0126 - val_mae: 0.0863 - val_mse: 0.0126 - val_rmse: 0.0863\n",
      "Epoch 183/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0077 - mae: 0.0697 - mse: 0.0077 - rmse: 0.0697 - val_loss: 0.0164 - val_mae: 0.0998 - val_mse: 0.0164 - val_rmse: 0.0998\n",
      "Epoch 184/200\n",
      "89/89 [==============================] - 0s 82us/step - loss: 0.0065 - mae: 0.0655 - mse: 0.0065 - rmse: 0.0655 - val_loss: 0.0112 - val_mae: 0.0792 - val_mse: 0.0112 - val_rmse: 0.0792\n",
      "Epoch 185/200\n",
      "89/89 [==============================] - 0s 85us/step - loss: 0.0053 - mae: 0.0603 - mse: 0.0053 - rmse: 0.0603 - val_loss: 0.0137 - val_mae: 0.0912 - val_mse: 0.0137 - val_rmse: 0.0912\n",
      "Epoch 186/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0089 - mae: 0.0773 - mse: 0.0089 - rmse: 0.0773 - val_loss: 0.0097 - val_mae: 0.0737 - val_mse: 0.0097 - val_rmse: 0.0737\n",
      "Epoch 187/200\n",
      "89/89 [==============================] - 0s 75us/step - loss: 0.0049 - mae: 0.0568 - mse: 0.0049 - rmse: 0.0568 - val_loss: 0.0156 - val_mae: 0.0908 - val_mse: 0.0156 - val_rmse: 0.0908\n",
      "Epoch 188/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0058 - mae: 0.0617 - mse: 0.0058 - rmse: 0.0617 - val_loss: 0.0118 - val_mae: 0.0880 - val_mse: 0.0118 - val_rmse: 0.0880\n",
      "Epoch 189/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0056 - mae: 0.0634 - mse: 0.0056 - rmse: 0.0634 - val_loss: 0.0108 - val_mae: 0.0810 - val_mse: 0.0108 - val_rmse: 0.0810\n",
      "Epoch 190/200\n",
      "89/89 [==============================] - 0s 77us/step - loss: 0.0044 - mae: 0.0558 - mse: 0.0044 - rmse: 0.0558 - val_loss: 0.0123 - val_mae: 0.0883 - val_mse: 0.0123 - val_rmse: 0.0883\n",
      "Epoch 191/200\n",
      "89/89 [==============================] - 0s 68us/step - loss: 0.0051 - mae: 0.0568 - mse: 0.0051 - rmse: 0.0568 - val_loss: 0.0153 - val_mae: 0.0982 - val_mse: 0.0153 - val_rmse: 0.0982\n",
      "Epoch 192/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0065 - mae: 0.0650 - mse: 0.0065 - rmse: 0.0650 - val_loss: 0.0102 - val_mae: 0.0776 - val_mse: 0.0102 - val_rmse: 0.0776\n",
      "Epoch 193/200\n",
      "89/89 [==============================] - 0s 92us/step - loss: 0.0047 - mae: 0.0565 - mse: 0.0047 - rmse: 0.0565 - val_loss: 0.0118 - val_mae: 0.0809 - val_mse: 0.0118 - val_rmse: 0.0809\n",
      "Epoch 194/200\n",
      "89/89 [==============================] - 0s 72us/step - loss: 0.0051 - mae: 0.0607 - mse: 0.0051 - rmse: 0.0607 - val_loss: 0.0125 - val_mae: 0.0841 - val_mse: 0.0125 - val_rmse: 0.0841\n",
      "Epoch 195/200\n",
      "89/89 [==============================] - 0s 58us/step - loss: 0.0044 - mae: 0.0521 - mse: 0.0044 - rmse: 0.0521 - val_loss: 0.0111 - val_mae: 0.0787 - val_mse: 0.0111 - val_rmse: 0.0787\n",
      "Epoch 196/200\n",
      "89/89 [==============================] - 0s 81us/step - loss: 0.0075 - mae: 0.0707 - mse: 0.0075 - rmse: 0.0707 - val_loss: 0.0153 - val_mae: 0.1016 - val_mse: 0.0153 - val_rmse: 0.1016\n",
      "Epoch 197/200\n",
      "89/89 [==============================] - 0s 65us/step - loss: 0.0066 - mae: 0.0660 - mse: 0.0066 - rmse: 0.0660 - val_loss: 0.0133 - val_mae: 0.0922 - val_mse: 0.0133 - val_rmse: 0.0922\n",
      "Epoch 198/200\n",
      "89/89 [==============================] - 0s 85us/step - loss: 0.0051 - mae: 0.0576 - mse: 0.0051 - rmse: 0.0576 - val_loss: 0.0109 - val_mae: 0.0823 - val_mse: 0.0109 - val_rmse: 0.0823\n",
      "Epoch 199/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0045 - mae: 0.0547 - mse: 0.0045 - rmse: 0.0547 - val_loss: 0.0113 - val_mae: 0.0834 - val_mse: 0.0113 - val_rmse: 0.0834\n",
      "Epoch 200/200\n",
      "89/89 [==============================] - 0s 63us/step - loss: 0.0047 - mae: 0.0566 - mse: 0.0047 - rmse: 0.0566 - val_loss: 0.0110 - val_mae: 0.0815 - val_mse: 0.0110 - val_rmse: 0.0815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd75401ccd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_treinamento, y_treinamento, \n",
    "          epochs=200,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predictions ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "previsoes = model.predict(X_teste)\n",
    "\n",
    "test_predictions = model.predict(X_teste)\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(scaler_y.inverse_transform(y_teste), scaler_y.inverse_transform(test_predictions))\n",
    "plt.xlabel('True Values ')\n",
    "plt.ylabel('Predictions ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficiente de determinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9914330061583636"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "acuracia_teste = r2_score(scaler_y.inverse_transform(y_teste),\n",
    "                          scaler_y.inverse_transform(test_predictions))\n",
    "\n",
    "previsao_todas = scaler_y.inverse_transform(model.predict(X_teste))\n",
    "y_teste_real = scaler_y.inverse_transform(y_teste)\n",
    "\n",
    "acuracia_teste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
